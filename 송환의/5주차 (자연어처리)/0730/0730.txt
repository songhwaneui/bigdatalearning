바이랭규얼 워드 임베딩
두개의 다른 언어에 대해 임베딩???

비슷한 의미를 가진 언어들은 언어가 다를지라도 비슷한 공간에 넣어둠

---------------------------------------------------
전처리

입력데이터

parallel 한코퍼스 두가지 다른 언어 준비

똑같은 의미인거 같은 선상에  데이터 준비
-------------------------------------------------------
length -ratio shuffle
서로 다른 언어에 있는 문장을 하나의 doc로 합쳐야
학습가능

------------------------------------------------
word2vec학습



input과 legth ratio shuffle과정만 다름

영어 한국 자막 

opus 2016 코퍼스 분석 다 되어있음
==========================================================================================
주재걸 교수 특강

AI
전문가시스템, rulebased  // 머신러닝 외주(테스크,요구사항, 학습데이터) 머신러닝이 알아서 알고리즘 모듈 다만들어서 좋은 아웃풋을줌 // 비지도 딥러닝

입출력간 예제데이터 라벨링 해서 줘야함

기존 머신러닝은 데이터를 넣으면 넣어줄수록 상승선이 한계가있다
반면 딥러닝은 초기 데이터 적을때는 성능이 구리지만 넣으면 넣을수록 완벽에 가까운 알고리즘 상승선이 한계가 없다. 
ex) 디스코드 띠리릭 로봇 체크하는거 라벨링 사용자 대신 시키는경우도 있따 십새끼들



3d 그래픽 렌더링 행렬연산 (코사사코 얼사안코 ) 이런거에 최적화된것 그래픽카드, 슈퍼컴퓨터 // 딥러닝의 대부분은 행렬연산 

Nvidea | 소프트웨어(파이토치,텐서플로우,판다스)플랫폼을 만들어놈 + c++ 코딩 되도록 환경구축 => 쿠다


playground.tensorflow.org
     
background 예측값 
학습값 일치해야 학습이 잘된거

레이어늘리면 학습 시간, 학습데이터 너무 오래걸림

SIGMOID
우리 머리 뉴런 

RELU
SIGMOID 더 개선한거 

TANH 
둘사이

오버피팅(테스트 데이터에는 형편없, 학습데이터에만 잘됨)
- (데이터전처리단), 피쳐를 줄인다. 데이터 augmentation(학습데이터에서 미쳐 커버하지 못하는걸 데이터를 불려줘서 방지하도록 ex 왼쪽만 바라본 고양이 오른쪽 바라본 고양이는고양이 x?
						학습성능수 떨어질수 있지만 모델이  받춰주면(캐패시티, 노드수 ) 테스트 셋이 학습데이터에 근접하게된다. 
						상하반전은 x 실제 일어날법한 (고양이가 물구나무? x 현실에서안일어남) 데이터를 넣는다. 
						+ 가로세로 비율 뚱뚱한 고양이 홀쭉한 고양이
	 
레귤러라이제이션(l1,l2), 드랍아웃, 얼리스타트 => 오버피팅방지


딥러닝 모델 파워풀 데이터만 충분히 맞춰준다면 
있는 정보를 알아서  필요없는정보는 알아서 거름

##################################
최소한 어느정도는 우리가 학습 전에 최적의 값으로 세팅 해줘야한다.(ex 좌우반전, 어그멘테이션 몇도, 등등 )
하이퍼 파라메터는 우리가 설정해줘야한다. => 선험적지식 + 쉅시간에 최적값배웠던거 


딥러닝 노드 몇개, 레이어 몇개 결정이 어렵다.

automl
최적의 세팅까지 자동으로 찾아주는 알고리즘 => Nas 뉴럴 아키텍쳐 서치 알고리즘

좋은모델 나만의 데이터 => 보통 학습 잘안됨 => 어떻게 해야 잘될까? (전문가들이암)


stylegan(딥러닝으로 이세상없는 이미지 만듬)

30 * 60(rgb) => 1800 * 3(rgb) => 0 1 학습을 통해 데이터정보를 축약해 나감


cnn모델이 사람의 인지능력을 다 따라잡음 데이터만 충분하면

사람을 시켜서 퍼포먼스 성능을 잰다음
머신러닝이 사람의 퍼포먼스 성능을 넘는다면 그 task 는 우리가 정복했다!

컴퓨터 비전 인식의 task 는 다 정복했다.

인식task
생성task(입출력만 뒤바꾸는식 고양이고양이고양이 => 실제 고양이 사진 만듬) 



모델의 능력이 딸리면 인풋데이터를 모델이 잘 연산하도록 전처리해서 넣으면 잘나온다.
리니어 모델은 -> 존나멍청 관계를 못찾음

유용한 피쳐를 추출하는과정(라벨링)  -> 머신러닝
			           -> 딥러닝 필요x 이런과정도 스스로 학습해서 알아서 피쳐추출 자동화 ㄹㅇ 아웃소싱처럼됨





머신러닝,딥러닝 태스크의 학습데이터만 있으면 할 수 있는 스코프의 범위가 매우 넓다.
기존 도메인 전문가 시스템으로 만든 알고리즘들이 딥러닝의 등장으로 다 도태됬다.(ex 번역시스템, nlp, 딥러닝)


neuraltalk 
이미지를 보고 텍스트 자동생성
- imagecaptioning rnn모델 텐션모델?

고양이라는 하이레벨개념이
모달리티 소리, 영상, 자연어, 텍스트 다양한 이종데이터들이 딥러닝을 통해서 통합된 형태(벡터스페이스)로 표현이 됨'
ex) 야옹~ -> 고양이


랜덤포레스트,, ~~트리 , 딥러닝, 머신러닝


(지도학습) 머신러닝은 학습결과 레이블에만 한정적으로 집중해서 학습하기떄문에 응용할수가 없다.

(비지도학습)딥러닝은 이전에 만든 학습모델에서 나온 데이터가 파생되서 다른 학습모델에 쓸수가 있다. 응용 해낼수 있는 능력이 중요(딥러닝) -> general knowledge purpose
BERT, GPT2, XLNET



paintschainer.preferred.tech

컨디셔널 모델 사용자가 이조건 저조건 다양하게 줘서 그 결과  => self supervised learning

cf)종설주제 웹툰 자동채색??



==========================================================
I ____  math    : ____에 무엇이 들어가냐?

주어진 문장의 일부만 보여주고 예측하도록 함
입력정보만으로 이용해 학습시킨것을 또 일부만 보여주고 학습시키고 계속해서 스스로 학습

general knowledge purpose -> 엄청난 고수준 요함 ( ~ 어디까지가 뺨이고, 살은 무슨색이고 뜽등등 엄청 배울게 많음)


pre-trained model
transfer model

로우레벨 피쳐들은 범용성을 가진다.



NLU  understaning
LNG  generation




Seq Seq 모델



기존의 SOTA 모델 


QUESTION answer 에 고도화 되게 만들면 



text 는 원래는 lstm이었다가 cnn써서 성능이 올라간 경우도 있다.







====================================
Transformer based on Self-Attention 이 가장 중요하다 요세는!!


