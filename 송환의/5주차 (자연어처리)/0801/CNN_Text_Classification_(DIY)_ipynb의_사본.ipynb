{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Text Classification (DIY).ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKtiON7SWo-W",
        "colab_type": "text"
      },
      "source": [
        "# CNN Text Classification Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEbjAYgOW6WB",
        "colab_type": "text"
      },
      "source": [
        "## Goal\n",
        "본 실습의 목표는 Convolutional Neural Network을 이용하여 문장을 여러 카테고리 중 하나로 분류하는 모델을 만드는 것입니다. 또한, 미리 학습된 단어 벡터를 모델에 적용하는 방법도 배워볼 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROSacgMz6V7r",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "학습 데이터는 Stanford 대학에서 구성한 공손함 데이터를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1YfHPu-7t1f",
        "colab_type": "code",
        "outputId": "ffde05aa-4735-4abc-fe4e-22340066ce42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow import keras\n",
        "\n",
        "nltk.download('punkt') ## 단어 토큰화"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2EW2UKy6elS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"Stanford_politeness_corpus.zip\"):\n",
        "  !wget http://www.cs.cornell.edu/~cristian/Politeness_files/Stanford_politeness_corpus.zip\n",
        "\n",
        "if not os.path.exists(\"Stanford_politeness_corpus/wikipedia.annotated.csv\"):\n",
        "  !unzip Stanford_politeness_corpus.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLxkMzaN9G02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_file): ## 데이터 불러옴 아까 했던거 상위25 하위25 자른거\n",
        "  data = pd.read_csv(data_file)\n",
        "\n",
        "  # Only use the top quartile as polite, and bottom quartile as impolite. Discard the rest.\n",
        "  quantiles = data[\"Normalized Score\"].quantile([0.25, 0.5, 0.75])\n",
        "  print(quantiles)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    score = data.loc[i, \"Normalized Score\"]\n",
        "    if score <= quantiles[0.25]:\n",
        "      # Bottom quartile (impolite).\n",
        "      data.loc[i, \"Normalized Score\"] = 0\n",
        "    elif score >= quantiles[0.75]:\n",
        "      # Top quartile (polite).\n",
        "      data.loc[i, \"Normalized Score\"] = 1\n",
        "    else:\n",
        "      # Neutral.\n",
        "      data.loc[i, \"Normalized Score\"] = 2\n",
        "\n",
        "  data[\"Normalized Score\"] = data[\"Normalized Score\"].astype(int)\n",
        "\n",
        "  # Discard neutral examples.\n",
        "  data = data[data[\"Normalized Score\"] < 2]\n",
        "  data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaI1jGtMWOye",
        "colab_type": "code",
        "outputId": "b6a23d3d-759b-4bae-c3dc-ea88d63f62d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "data = load_data(\"Stanford_politeness_corpus/wikipedia.annotated.csv\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.25   -0.442633\n",
            "0.50    0.052339\n",
            "0.75    0.514399\n",
            "Name: Normalized Score, dtype: float64\n",
            "   Community      Id                                            Request  \\\n",
            "0  Wikipedia  620599  Another admin has already deleted the article....   \n",
            "1  Wikipedia  615680  \"Odder\" in what sense? Also, why are you still...   \n",
            "2  Wikipedia  132503  That's a couple levels of automation beyond my...   \n",
            "3  Wikipedia  620837  CIV, I'll grant (and yes, I shouldn't have). B...   \n",
            "4  Wikipedia  549682  As I wrote above, at first I thought lets keep...   \n",
            "\n",
            "   Score1  Score2  Score3  Score4  Score5         TurkId1         TurkId2  \\\n",
            "0      22      19      21      14      18   ANGX5PAAYGL9P   AO5E3LWBYM72K   \n",
            "1       9       9      15      13       9  A16PRU8T6NZLN5  A1BJTTNDDFZ3ZP   \n",
            "2       9      13      13       9      17  A233ONYNWKDIYF  A2UFD1I8ZO1V4G   \n",
            "3       9       7      13      17      12  A233ONYNWKDIYF  A21753FQKCM5DQ   \n",
            "4      21      17      15      18      17  A1BQCRF5Q76YFY   AO3XB5I5QNNUI   \n",
            "\n",
            "          TurkId3         TurkId4         TurkId5  Normalized Score  \n",
            "0  A2MWGIVIKOXWYM  A2LQ33BQ8G259D   A872FSFU7WV6W                 1  \n",
            "1   AVHARSCRX7M9E   AQWC8BPA9JB7T  A1KC138MX47DXA                 0  \n",
            "2   AYG3MF094634L  A13CO3B2BRXQLZ  A38N58BSMTA0BE                 0  \n",
            "3  A2VMJFBCB921NI   ARYGQ46IMW8AU   AZOAW8JH5HJJK                 0  \n",
            "4  A1ST1WRDA4ALST   A31RP7HI5172B  A3SQ6ORWDN5BGZ                 1  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbrRF2ER6oay",
        "colab_type": "text"
      },
      "source": [
        "다음으로 할 일은 사전을 구성하는 것입니다.\n",
        "\n",
        "\n",
        "신경망의 입력으로 사용하기 위해서는 문장을 숫자로 바꿔야 하는데, 사전의 역할은 단어를 숫자로, 숫자를 단어로 바꿔주는 것입니다.\n",
        "\n",
        "여기서 빠른 계산을 위해 dictionary 자료 구조를 사용하는 것이 일반적입니다.\n",
        "\n",
        "\n",
        "\n",
        "1.   문장들을 소문자로 바꾸고, tokenization (nltk.tokenize 패키지의 word_tokenize  함수 활용)\n",
        "2.   전체 데이터에서 각 토큰들의 등장 빈도 확인 (collections 패키지의 Counter  클래스 활용)\n",
        "3.   가장 등장 빈도가 높은 단어를 vocab_size 만큼 선택 (Counter의 most_common 함수 활용)\n",
        "4.   각각의 단어에 고유한 숫자 부여. 이때, 0번째 토큰은 \"<PAD>\", 1번째 토큰은 \"<OOV>\" 할당\n",
        "5.   토큰 -> 숫자 변환을 위한 dictionary (word_index 변수에 할당)와, 숫자 -> 토큰 변환을 위한 dictionary (word_inverted_index 변수에 할당) 생성\n",
        "  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at89pkxKzTtJ",
        "colab_type": "text"
      },
      "source": [
        "단어 수집 단어에 따라 id만들어주는 사전"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN4C5ewLW9AN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import Text\n",
        "\n",
        "vocab_size = 5000\n",
        "## 가장 등장빈도높은 5천개\n",
        "# we assign the first indices in the vocabulary to special tokens that we use\n",
        "# for padding, and for indicating unknown words\n",
        "pad_id = 0\n",
        "## 패딩토큰에 0번째 아이디\n",
        "oov_id = 1\n",
        "# 아웃오브id 첫번째\n",
        "index_offset = 1\n",
        "## 나머지 아이디는 뒤로밀림\n",
        "\n",
        "def make_vocab(sentences):  ##!! 중요 문장을 입력으로 받아\n",
        "  word_counter = Counter()\n",
        "  \n",
        "  # Your code here\n",
        "  for n in range(0,len(sentences)):\n",
        "    for i in word_tokenize(sentences[n]):\n",
        "      word_counter.update(\"i\")\n",
        "      print(\"=====\")\n",
        "      \n",
        "      print(word_counter)\n",
        "    \n",
        "  most_common = word_counter.most_common() ## 파이썬 컬렉션 객체 카운터 딕셔너리구조  키가 없으면 카운트 0 할당 // 등장빈도 셀때 좋음, \n",
        "                                            ## count 다 끝나면 고빈도 단어 보여줌.\n",
        "  print(\"고빈도 단어:\")\n",
        "  for k, v in most_common[:10]:\n",
        "    print(k, \": \", v)\n",
        "    \n",
        "  vocab = {\n",
        "      '<PAD>': pad_id,\n",
        "      '<OOV>': oov_id\n",
        "  }\n",
        "  vocab[pad_id] = oov_id\n",
        "  \n",
        "  \n",
        "  ## enumerate 인덱스도 같이 포문돌아서 찍어줌 start = index+offset+1\n",
        "  \n",
        "  # Your code here\n",
        "  \n",
        "  return vocab\n",
        "  \n",
        "  \n",
        "sentences = data[\"Request\"].tolist()\n",
        "word_index = make_vocab(sentences)  ## 딕셔너리 텍스트  id\n",
        "word_inverted_index = {} ##딕셔너리/리스트 id 에서 원래 텍스트\n",
        "\n",
        "# Your code here\n",
        "\n",
        "print(\"\\n단어 사전:\")\n",
        "for i in range(0, 10):\n",
        "  print(i, word_inverted_index[i])\n",
        "  \n",
        "print(\"\\n단어 사전 크기: \", len(word_index))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  ##문장이 들어오면 소문자 처리 문장을 토큰단위로 쪼개고 빈도 체크"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myPkQRpj2Yur",
        "colab_type": "code",
        "outputId": "57bffab0-cd51-4648-c1b2-5829c1ac3d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(data[\"Request\"].tolist()[0])\n",
        "# for i in word_tokenize(data[\"Request\"].tolist()):\n",
        "#   print(i)\n",
        "  #word_counter[i] = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['See',\n",
              " '<',\n",
              " 'url',\n",
              " '>',\n",
              " '.',\n",
              " 'I',\n",
              " 'assume',\n",
              " 'you',\n",
              " 'did',\n",
              " \"n't\",\n",
              " 'mean',\n",
              " '<',\n",
              " 'url',\n",
              " '>',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh3O9aHD7HLf",
        "colab_type": "text"
      },
      "source": [
        "사전이 잘 구성되었는지 시험해보겠습니다. \n",
        "\n",
        "사전이 잘 구성되고, 각각의 사전이 word_index 변수와 word_inverted_index 변수에 할당되었다면 문장이 숫자로 변환되었다가 다시 원래 문장으로 돌아오는 것을 확인하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdBgH74TcmNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_to_text(indexes):\n",
        "  return ' '.join([word_inverted_index[i] for i in indexes])\n",
        "  \n",
        "def text_to_index(text):\n",
        "  tokens = tokens = word_tokenize(text.lower())\n",
        "  indexes = []\n",
        "  for tok in tokens:\n",
        "    if tok in word_index:\n",
        "      indexes.append(word_index[tok])\n",
        "    else:\n",
        "      indexes.append(oov_id)\n",
        "      \n",
        "  return indexes\n",
        "\n",
        "print(\"원본: \", sentences[0])\n",
        "ids = text_to_index(sentences[0])\n",
        "print(\"문자 -> 숫자: \", ids)\n",
        "print(\"숫자 -> 문자: \", index_to_text(ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb0Zk0YY7MiD",
        "colab_type": "text"
      },
      "source": [
        "다음으로, 숫자로 바뀐 문장들을 학습 데이터로 사용할 수 있도록 변형하겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "1.   모든 문장들을 동일한 길이가 되도록 padding 처리하거나 자름 (tensorflow.python.keras.preprocessing.sequence 패키지의 pad_sequence 함수 활용)\n",
        "2.   데이터의 일부(10%)를 테스트 데이터로 분리\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQfHyY0GhvBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_variable = # Your code here\n",
        "## 모든 문장을 동일한 길이가 되도록 통일\n",
        "\n",
        "sentence_size = 200\n",
        "x_padded = sequence.pad_sequences(x_variable, ## x_variable 입력데이터, text - to index 배열안 배열\n",
        "                                 maxlen=sentence_size, ## 200단위\n",
        "                                 truncating='post', ##문자 자르거나 패딩할떄 어느부분남기고 자를지 \n",
        "                                 padding='post',# post뒤 앞에서부터 토큰을 가지고있다가 뒷단 을 자르는거  패딩을 뒤에\n",
        "                                 value=pad_id)\n",
        "\n",
        "\n",
        "# 10%데이터를 시험문제로 따로 빼놓고 90퍼만 학습, 10퍼는 test데이터로\n",
        "# Your code here\n",
        "\n",
        "n_test = len(data)  // 10\n",
        "test_inputs = x_padded[:n_test]\n",
        "train_inputs = x_padded[n_test:]\n",
        "\n",
        "\n",
        "ys = np.array(data[\"Normalized Score\"].tolist())  # 정답데이터 불손0 공손1 리스트로 불러와서 케라스쓸라면 넘파이 써야되서 넘파이로 변형\n",
        "## ys도 쪼개서 10% 테스트, => 넘파이 행렬 쉐입이나온다.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "print(\"test_inputs shape: \", test_inputs.shape)\n",
        "print(\"train_inputs shape: \", train_inputs.shape)\n",
        "print(\"test_labels shape: \", test_labels.shape)\n",
        "print(\"train_labels shape: \", train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu1nHsfYCTkD",
        "colab_type": "text"
      },
      "source": [
        "x_padded[0:10]  10: ㄲ끝까지\n",
        "\n",
        "217개 문장이 200개 단어로\n",
        "\n",
        "1961개 문장 200개단어\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ftSUTOB98N_",
        "colab_type": "text"
      },
      "source": [
        "이제 모델을 설계할 차례입니다. \n",
        "\n",
        "keras.Sequential을 이용하여 CNN 모델을 구성해봅시다. Sequential 모델을 사용하려면 동일한 크기의 필터만 사용할 수 있습니다.\n",
        "\n",
        "참고 함수: \n",
        "\n",
        "keras.layers.Embedding\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "keras.layers.Conv1D\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
        "\n",
        "keras.layers.GlobalMaxPool1D\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D\n",
        "\n",
        "keras.layers.Dense\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlsrBpQZmUFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1D 커널이 이동할때 1차원으로 세로만 움직인다 이느낌\n",
        "## 글로벌맥스풀 커널 적용값 중 제이 큰값\n",
        "##\n",
        "\n",
        "# 임베딩 레이어 한국어는 형태소 단위로 임베딩 형태소끼리 결합해서 어절을 이룸 영어처럼 띄어쓰기 하면은 다양한 단어가 너무많음 같은집이여도 집은 집에 집으로 등 단어의 종류가 너무 커져버리므로 \n",
        "# 단어는 고빈도 단위로 항상 자르는데 OUT OF VOCA 처리되는 어절이 너무많음 따라서 형태소 단위로 해야한다.\n",
        "# NLTK 한국어 패키지 형태소분석 패키지 사용해서 => 형태소 임베딩 나온거 -> 그이후 ㄷ동일하게 영어처럼 진행\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,50),\n",
        "    keras.layers.Conv1D(32, 3,padding = \"same\",activation=tf.nn.relu),## 3개단어를 보면서 피쳐맵을 만든다.\n",
        "    keras.layers.GlobalMaxPool1D(),\n",
        "    keras.layers.Dense(2, activation= tf.nn.softmax)\n",
        "   \n",
        "    ## 7*7*64를 쫙펴서 덴스레이어 \n",
        "])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6aCoD9-jNE",
        "colab_type": "text"
      },
      "source": [
        "아래는 학습 결과를 시각화해주고, 성능을 측정하는 함수들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZG1KkAOo_ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(6,5))\n",
        "  val = plt.plot(history.epoch, history.history['val_loss'],\n",
        "                 '--', label='Test')\n",
        "  plt.plot(history.epoch, history.history['loss'], color=val[0].get_color(),\n",
        "           label='Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "  \n",
        "def eval_model(model):\n",
        "  test_loss, test_acc = model.evaluate(test_inputs, test_labels)\n",
        "  print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVGrEUEu-oXv",
        "colab_type": "text"
      },
      "source": [
        "만들어진 모델을 학습시켜보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hPsLLFIot0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "2b8fdbe4-0c9e-4432-c37e-98d3aaffab66"
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_inputs,\n",
        "          train_labels,\n",
        "          epochs=10,\n",
        "          validation_data=(test_inputs, test_labels)\n",
        "         )\n",
        "\n",
        "plot_loss(history)\n",
        "eval_model(model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-fb99f7d5264c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(train_inputs,\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCaixZUv-tjb",
        "colab_type": "text"
      },
      "source": [
        "## Pretrained word vectors\n",
        "\n",
        "이번에는 만들어진 모델에 미리 학습된 단어 벡터를 적용해보겠습니다.\n",
        "\n",
        "단어 벡터는 GloVe 벡터를 사용할 것입니다.\n",
        "\n",
        "벡터 파일을 다운로드 받고 압축을 풀어보겠습니다.\n",
        "\n",
        "파일이 어떻게 구성되어 있는지 볼까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QbowKA4OrZr",
        "colab_type": "text"
      },
      "source": [
        "glove 단어, 벡터 있는애 워드투백과 비슷\n",
        "\n",
        "전체 단어에 대한 임베딩\n",
        "글로브있는단어, 우리가 가지고 있는단어 비교\n",
        "글로브벡터가지고 우리 모델을 학습시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM6dSiL7rXut",
        "colab_type": "code",
        "outputId": "2505a3ca-3ff1-40d9-f9ac-608acc5146d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "source": [
        "if not os.path.exists('glove.6B.zip'):\n",
        "    ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "if not os.path.exists('glove.6B.50d.txt'):\n",
        "    ! unzip glove.6B.zip\n",
        "    \n",
        "! head glove.6B.50d.txt ## 6B 얼마만큼 큰 말뭉치로 학습 시켰다. 50d 이벡터 차원이 50차원"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-01 08:12:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-08-01 08:12:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-08-01 08:12:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  27.9MB/s    in 24s     \n",
            "\n",
            "2019-08-01 08:13:04 (34.7 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
            ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
            ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
            "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
            "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
            "and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n",
            "in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n",
            "a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n",
            "\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n",
            "'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0AvsVnPw--",
        "colab_type": "text"
      },
      "source": [
        "[0]이 단어고 나머지는 벡터값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhv_Sbm4--m9",
        "colab_type": "text"
      },
      "source": [
        "GloVe 벡터를 불러와서 임베딩 행렬을 초기화해보겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "1.   GloVe 파일을 읽고, 각 줄에서 단어(1번째 토큰)와 벡터를 이루는 숫자들(2번째 이후 토큰들)을 분리\n",
        "2.   벡터를 이루는 숫자들을 numpy 행렬로 변환 (numpy의 asarray 함수 활용)\n",
        "3.   단어와 벡터를 연결하는 dictionary 자료구조 구성 (단어 -> 벡터)\n",
        "4.   모든 단어들에 대한 임베딩 행렬을 무작위로 생성 (vocab_size X 50 크기의 numpy 행렬)\n",
        "5.   임베딩 행렬에서, GloVe 벡터가 존재하는 단어들만 해당 GloVe 벡터로 대체\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OWLKUOFr0tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove_embeddings(path):   ##키 : 단어 // value : 인베딩벡터 이걸 로딩 \n",
        "     embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f: ## 텍스트 파일 한줄씩 라인으로\n",
        "            # Your code here\n",
        "            values = linke.strip().split() ## strip 줄바꿈 제거, # split() 띄어쓰기 단위\n",
        "            w = values[0] ## 첫번째깞이 워드\n",
        "            vectors = np.asarray(values[1:], dypte=\"float32\") ## 넘파이 벡터로 만들고\n",
        "            embeddings[w] = vectors\n",
        "                                                                        \n",
        "                                                                        ## 정규분포가 아닌 균등분포\n",
        "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, 50)) ## 무작위로 초기화 전체 단어에 대한 임베딩 행렬 초기화 5000단어, 글로브벡터 50차원에 맟춰서 # 임베딩행렬 제일 첫단 분산 영향 x\n",
        "    num_loaded = 0  #몇개나 불러왔는지 체크\n",
        "    \n",
        "    for w, i in word_index.items(): ## 단어가 있으면 인베딩벡터값으로 바꾼다.\n",
        "        # Your code here\n",
        "        # emvedding[] 이런식하면 키없으면 오류나서\n",
        "        v = embddings.get(w)\n",
        "        if \n",
        "        num_loaded = num_loaded + 1\n",
        "            \n",
        "    print('Successfully loaded pretrained embeddings for '\n",
        "          f'{num_loaded}/{vocab_size} words.')\n",
        "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings('glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c70N63qoAKx7",
        "colab_type": "text"
      },
      "source": [
        "앞서 사용된 모델에서, Embedding layer의 값을 위에서 생성한 임베딩 행렬로 초기화해봅시다.\n",
        "\n",
        "(keras.initializers.Constant 클래스 활용)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjMv1u72r_qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "# 우리가 만든 임베딩 함수로 초기화 한다 => glove_vec\n",
        "\n",
        "# 성능은 구데기 글로브벡터랑 우리가 사용하는거랑 도메인이 안맞는다.\n",
        "\n",
        "# 지금까지는 지도학습방식\n",
        "# 정답사람시켜서찾은건 2000문장\n",
        "\n",
        "# 데이터 구하기 개빢셈\n",
        "\n",
        "# 어떻게 하면 비지도랑 결합해서 지도학습 성능 끌어올릴까?\n",
        "\n",
        "# 다른 모델에서 학습을 시키고 새로운 모델에 다시 학습을 시킴 (트랜스펄러닝)\n",
        "\n",
        "# 비지도 학습으로 1차학습 => 실제 원하는데이터로 추가 학습 => 데이터가 적더라도 높은 성능 보인다.\n",
        "\n",
        "# 글로브 벡터( 60억 토큰 말뭉치 학습) 존나 큰거\n",
        "\n",
        "# 우리가 원하는 모델에 글로브벡터 가져옴 두개 모델간 학습 목적이 맞지않아서 성능이 구데기 \n",
        "\n",
        "# 트랜스퍼 러닝할떄는 첫 태스크 두번쨰 태스크 얼마나 유사성이 있느냐\n",
        "\n",
        "# 텐서플로 허브란\n",
        "\n",
        "# 데이터 많이 확보할수 업스니까 기존 모델 재활용 => 새로운 모델에 활용 이게 중요한 이슈\n",
        "\n",
        "# 예전에는 깃허브 다운이 곧 모델만\n",
        "\n",
        "# 이제는 모델이랑 모델 학습시키는 웨이트 같이 배포\n",
        "\n",
        "# 텐서플로서 유지보수 => 전이학습할수 있게 해논 플랫폼 => 텐서플로 허브\n",
        "\n",
        "# 텍스트 임베딩( 텍스트 -> 벡터)\n",
        "\n",
        "# 학습된 모델, 코드 쭉정리 universal-sentence-encoder-large문장을 잘표현할수 있는목적\n",
        "# 문장, 문단 텍스트를 벡터로 만들수 있따. \n",
        "\n",
        "# cnn모델목적 텍스트 들어왔을떄 벡터를 얻는거\n",
        "\n",
        "# 텐서플로 받은 객체 안에 맨오른쪽 2칸짜리 전까지 그래프 다들어있다.\n",
        "\n",
        "# 문장넣으면 벡터 나옴 덴서 모델 만들어서 분류해서 학습시키면 된다.  ## 덴서말고 svm\n",
        "\n",
        "# 버트 = 순전히 비지도로 학습한거 단어가 주어졌을떄 다음단어 예측 (빈칸채우기) 학습시킬떈\n",
        "\n",
        "# 문장을 주고 모델에 빈칸 단어 뭘까를 학습 비지도학습\n",
        "\n",
        "# 문장간의 상관관계 비지도학습 => 버트 모델 줜나 큼 + DENSE레이어 하면 댐\n",
        "\n",
        "# NER개체마다 벡터가나와야되는데 그것도 버트로 대체\n",
        "\n",
        "# 버트 줜나어려움 프로젝트 사용에 도움되랑 이거야"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGnd9iJqsk3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_inputs,\n",
        "          train_labels,\n",
        "          epochs=10,\n",
        "          validation_data=(test_inputs, test_labels)\n",
        "         )\n",
        "\n",
        "plot_loss(history)\n",
        "eval_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}