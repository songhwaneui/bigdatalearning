{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Text Classification (Full).ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "adzlJRbDyc3A",
        "colab_type": "code",
        "outputId": "8c8186c7-4d28-4c62-df4b-307f7ff21b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "! pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53LJuEPYkGV",
        "colab_type": "text"
      },
      "source": [
        "## 줜나어렵대"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az_pyElrvT6z",
        "colab_type": "code",
        "outputId": "74649f79-85b2-444c-e1d0-6b823cb53c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pickle\n",
        "import bert\n",
        "import os\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "def create_tokenizer_from_hub_module(bert_model_hub):\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    with tf.Graph().as_default():\n",
        "        bert_module = hub.Module(bert_model_hub)\n",
        "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "        with tf.Session() as sess:\n",
        "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                                  tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "        print(\"Using BERT from %s\" %bert_model_hub)\n",
        "        print(\"with vocab size=%d and do_lower_case=%s.\" %(len(vocab_file), str(do_lower_case)))\n",
        "\n",
        "    return bert.tokenization.FullTokenizer(\n",
        "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "\n",
        "def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n",
        "    input_example = dataset.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
        "                                                                             text_a=x[DATA_COLUMN],\n",
        "                                                                             text_b=None,\n",
        "                                                                             label=x[LABEL_COLUMN]), axis=1)\n",
        "    features = bert.run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    return features\n",
        "\n",
        "\n",
        "def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "    bert_module = hub.Module(\n",
        "        bert_model_hub,\n",
        "        trainable=True)\n",
        "    bert_inputs = dict(\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids)\n",
        "    bert_outputs = bert_module(\n",
        "        inputs=bert_inputs,\n",
        "        signature=\"tokens\",\n",
        "        as_dict=True)\n",
        "\n",
        "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Use \"sequence_outputs\" for token-level output.\n",
        "    output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "    with tf.variable_scope(\"output_layer\"):\n",
        "        layer_out = tf.layers.dense(\n",
        "            inputs=output_layer,\n",
        "            units=num_labels,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=tf.initializers.variance_scaling()\n",
        "        )\n",
        "        predicted_labels = tf.squeeze(tf.argmax(layer_out, axis=-1, output_type=tf.int32))\n",
        "\n",
        "        if is_predicting:\n",
        "            return predicted_labels, layer_out\n",
        "        else:\n",
        "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                labels=labels,\n",
        "                logits=layer_out\n",
        "            )\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "            return loss, predicted_labels, layer_out\n",
        "\n",
        "\n",
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "\n",
        "            (loss, predicted_labels, log_probs) = create_model(\n",
        "                bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            train_op = bert.optimization.create_optimizer(\n",
        "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "            # Calculate evaluation metrics.\n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "                f1_score = tf.contrib.metrics.f1_score(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                auc = tf.metrics.auc(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                recall = tf.metrics.recall(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                precision = tf.metrics.precision(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                true_pos = tf.metrics.true_positives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                true_neg = tf.metrics.true_negatives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                false_pos = tf.metrics.false_positives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                false_neg = tf.metrics.false_negatives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                return {\n",
        "                    \"eval_accuracy\": accuracy,\n",
        "                    \"f1_score\": f1_score,\n",
        "                    \"auc\": auc,\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"true_positives\": true_pos,\n",
        "                    \"true_negatives\": true_neg,\n",
        "                    \"false_positives\": false_pos,\n",
        "                    \"false_negatives\": false_neg\n",
        "                }\n",
        "\n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                                  loss=loss,\n",
        "                                                  train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                                  loss=loss,\n",
        "                                                  eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs) = create_model(\n",
        "                bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            predictions = {\n",
        "                'probabilities': log_probs,\n",
        "                'labels': predicted_labels\n",
        "            }\n",
        "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn\n",
        "\n",
        "\n",
        "def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE,\n",
        "                      num_train_steps, num_warmup_steps, BATCH_SIZE):\n",
        "    # Specify outpit directory and number of checkpoint steps to save\n",
        "    run_config = tf.estimator.RunConfig(\n",
        "        model_dir=OUTPUT_DIR,\n",
        "        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "        bert_model_hub=bert_model_hub,\n",
        "        num_labels=len(label_list),\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        num_train_steps=num_train_steps,\n",
        "        num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        params={\"batch_size\": BATCH_SIZE})\n",
        "    return estimator, model_fn, run_config\n",
        "\n",
        "\n",
        "def run_on_dfs(train, test, data_column, label_column,\n",
        "               max_seq_length=128,\n",
        "               batch_size=32,\n",
        "               learning_rate=2e-5,\n",
        "               num_train_epochs=3,\n",
        "               warmup_proportion=0.1,\n",
        "               save_summary_steps=100,\n",
        "               save_checkpoint_steps=10000,\n",
        "               bert_model_hub=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
        "               output_dir=\"output\"):\n",
        "    label_list = train[label_column].unique().tolist()\n",
        "\n",
        "    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n",
        "\n",
        "    train_features = make_features(train, label_list, max_seq_length, tokenizer, data_column, label_column)\n",
        "    test_features = make_features(test, label_list, max_seq_length, tokenizer, data_column, label_column)\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(train_features) / batch_size)\n",
        "\n",
        "    num_train_steps = int(len(train_features) / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "    estimator, model_fn, run_config = estimator_builder(\n",
        "        bert_model_hub,\n",
        "        output_dir,\n",
        "        save_summary_steps,\n",
        "        save_checkpoint_steps,\n",
        "        label_list,\n",
        "        learning_rate,\n",
        "        num_train_steps,\n",
        "        num_warmup_steps,\n",
        "        batch_size)\n",
        "\n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "        features=test_features,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    results = []\n",
        "    for epoch in range(num_train_epochs):\n",
        "        estimator.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
        "\n",
        "        print(\"End of epoch %d.\" %(epoch + 1))\n",
        "\n",
        "        result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "        print(result_dict)\n",
        "        results.append(result_dict)\n",
        "\n",
        "    return results, estimator\n",
        "\n",
        "\n",
        "def pretty_print(result):\n",
        "    df = pd.DataFrame([result]).T\n",
        "    df.columns = [\"values\"]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 08:00:41.508820 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRVzb1Y2y9_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_file):\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Only use the top quartile as polite, and bottom quartile as impolite. Discard the rest.\n",
        "    quantiles = data[\"Normalized Score\"].quantile([0.25, 0.5, 0.75])\n",
        "    # print(quantiles)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        score = data.loc[i, \"Normalized Score\"]\n",
        "        if score <= quantiles[0.25]:\n",
        "            # Bottom quartile (impolite).\n",
        "            data.loc[i, \"Normalized Score\"] = 0\n",
        "        elif score >= quantiles[0.75]:\n",
        "            # Top quartile (polite).\n",
        "            data.loc[i, \"Normalized Score\"] = 1\n",
        "        else:\n",
        "            # Neutral.\n",
        "            data.loc[i, \"Normalized Score\"] = 2\n",
        "\n",
        "    data[\"Normalized Score\"] = data[\"Normalized Score\"].astype(int)\n",
        "\n",
        "    # Discard neutral examples.\n",
        "    data = data[data[\"Normalized Score\"] < 2]\n",
        "    \n",
        "    data.sample(frac=1).reset_index(drop=True)\n",
        "    n_test = len(data) // 10\n",
        "    test_data = data[:n_test]\n",
        "    train_data = data[n_test:]\n",
        "    \n",
        "    print(\"Data loaded successfully. Train=%d, test=%d, total=%d.\" % (len(train_data), len(test_data), len(train_data) + len(test_data)))\n",
        "    print(\"Some train samples:\")\n",
        "    print(train_data.head())\n",
        "    print(\"Some test samples:\")\n",
        "    print(test_data.head())\n",
        "\n",
        "    return train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BxeqnzOyok3",
        "colab_type": "code",
        "outputId": "eadbdb38-9481-41b9-b1de-ec2f163d0383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if not os.path.exists(\"Stanford_politeness_corpus.zip\"):\n",
        "  !wget http://www.cs.cornell.edu/~cristian/Politeness_files/Stanford_politeness_corpus.zip\n",
        "\n",
        "if not os.path.exists(\"Stanford_politeness_corpus/wikipedia.annotated.csv\"):\n",
        "  !unzip Stanford_politeness_corpus.zip\n",
        "\n",
        "train_data, test_data = load_data(\"Stanford_politeness_corpus/wikipedia.annotated.csv\")\n",
        "\n",
        "params = {\n",
        "    \"data_column\": \"Request\",\n",
        "    \"label_column\": \"Normalized Score\",\n",
        "#     \"learning_rate\": 2e-5,\n",
        "    \"batch_size\": 16,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"bert_model_hub\": \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "}\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "result, estimator = run_on_dfs(train_data, test_data, **params)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data loaded successfully. Train=1961, test=217, total=2178.\n",
            "Some train samples:\n",
            "     Community      Id  ...         TurkId5  Normalized Score\n",
            "460  Wikipedia  621480  ...  A1Y3Z92RE62NPS                 1\n",
            "462  Wikipedia  146267  ...  A3IHLWMZNBLUR4                 1\n",
            "463  Wikipedia   84242  ...   AIPK94CUWL45W                 1\n",
            "464  Wikipedia  487517  ...  A1F4D2PZ7NNWTL                 1\n",
            "466  Wikipedia  629492  ...  A2WZQ92N4809N1                 1\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "Some test samples:\n",
            "   Community      Id  ...         TurkId5  Normalized Score\n",
            "0  Wikipedia  629705  ...  A15DM9BMKZZJQ6                 0\n",
            "1  Wikipedia  244336  ...  A3TFQK7QK8X6LM                 1\n",
            "5  Wikipedia  214411  ...  A1Y3Z92RE62NPS                 1\n",
            "8  Wikipedia  177439  ...  A29B522D0BX6HN                 0\n",
            "9  Wikipedia  341534  ...  A28TXBSZPWMEU9                 0\n",
            "\n",
            "[5 rows x 14 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:01:00.208889 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0801 08:01:01.246524 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0801 08:01:01.409889 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0801 08:01:01.411023 140393886611328 run_classifier.py:774] Writing example 0 of 1961\n",
            "I0801 08:01:01.414020 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:01.415405 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:01.416747 140393886611328 run_classifier.py:464] tokens: [CLS] Thanks . As an aside , since this did turn out to be fact ##ual , just very hard to source , do you think the community would count ##enan ##ce an un ##block request from B ##la ##ab ##la if he accepted some strict un ##block conditions ( such as packing in the ' systemic bias ' thing , discussing his edit ##s in a less confrontation ##al manner etc ) ? [SEP]\n",
            "I0801 08:01:01.421781 140393886611328 run_classifier.py:465] input_ids: 101 5749 119 1249 1126 4783 117 1290 1142 1225 1885 1149 1106 1129 1864 4746 117 1198 1304 1662 1106 2674 117 1202 1128 1341 1103 1661 1156 5099 25191 2093 1126 8362 27467 4566 1121 139 1742 6639 1742 1191 1119 3134 1199 9382 8362 27467 2975 113 1216 1112 16360 1107 1103 112 27410 15069 112 1645 117 10751 1117 14609 1116 1107 170 1750 14002 1348 4758 3576 114 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.423512 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.424428 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.425867 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:01.427649 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:01.428924 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:01.430006 140393886611328 run_classifier.py:464] tokens: [CLS] Everything about < u ##rl > looks fantastic , but . . going to | 2 instead of | 30 ##em seems like a major step back . Is there a reason for it ? [SEP]\n",
            "I0801 08:01:01.430993 140393886611328 run_classifier.py:465] input_ids: 101 5268 1164 133 190 17670 135 2736 14820 117 1133 119 119 1280 1106 197 123 1939 1104 197 1476 5521 3093 1176 170 1558 2585 1171 119 2181 1175 170 2255 1111 1122 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.432402 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.433540 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.434749 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:01.436643 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:01.437999 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:01.438913 140393886611328 run_classifier.py:464] tokens: [CLS] I wonder if it would ever be worth doing an article on G & S scholarship ? You know , cover the major discoveries , describe the evolution of the field . . . or is that too likely to hit problems ? [SEP]\n",
            "I0801 08:01:01.439967 140393886611328 run_classifier.py:465] input_ids: 101 146 4608 1191 1122 1156 1518 1129 3869 1833 1126 3342 1113 144 111 156 7084 136 1192 1221 117 2267 1103 1558 17707 117 5594 1103 7243 1104 1103 1768 119 119 119 1137 1110 1115 1315 2620 1106 1855 2645 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.441034 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.442049 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.442898 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:01.444304 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:01.445318 140393886611328 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using BERT from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\n",
            "with vocab size=76 and do_lower_case=False.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:01:01.446197 140393886611328 run_classifier.py:464] tokens: [CLS] Thanks for your help on this , it ' s much appreciated . Should I del ##ete my request for check ##user ? [SEP]\n",
            "I0801 08:01:01.447568 140393886611328 run_classifier.py:465] input_ids: 101 5749 1111 1240 1494 1113 1142 117 1122 112 188 1277 12503 119 9743 146 3687 16618 1139 4566 1111 4031 19399 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.448529 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.449416 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.450258 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:01.451980 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:01.452825 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:01.453804 140393886611328 run_classifier.py:464] tokens: [CLS] Yes please ! B ##uff ##ing up ' ' < u ##rl > ' ' to at least reflect a bit better on current state - of - play - tax ##ono ##mic ##ally would be good : ) Any Re ##lia ##ble Sources call it a < u ##rl > ? [SEP]\n",
            "I0801 08:01:01.454596 140393886611328 run_classifier.py:465] input_ids: 101 2160 4268 106 139 9435 1158 1146 112 112 133 190 17670 135 112 112 1106 1120 1655 7977 170 2113 1618 1113 1954 1352 118 1104 118 1505 118 3641 23038 7257 2716 1156 1129 1363 131 114 6291 11336 4567 2165 22656 1840 1122 170 133 190 17670 135 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.455564 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.456849 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:01.457620 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:02.534701 140393886611328 run_classifier.py:774] Writing example 0 of 217\n",
            "I0801 08:01:02.536086 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:02.537004 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:02.537857 140393886611328 run_classifier.py:464] tokens: [CLS] Where did you learn English ? How come you ' re taking on a third language ? [SEP]\n",
            "I0801 08:01:02.538698 140393886611328 run_classifier.py:465] input_ids: 101 2777 1225 1128 3858 1483 136 1731 1435 1128 112 1231 1781 1113 170 1503 1846 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.539506 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.540429 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.541437 140393886611328 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0801 08:01:02.546620 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:02.548343 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:02.549325 140393886611328 run_classifier.py:464] tokens: [CLS] Thanks very much for your edit to the < u ##rl > article . Would you be interested in ta ##ckling the < u ##rl > of < u ##rl > ? [SEP]\n",
            "I0801 08:01:02.550382 140393886611328 run_classifier.py:465] input_ids: 101 5749 1304 1277 1111 1240 14609 1106 1103 133 190 17670 135 3342 119 5718 1128 1129 3888 1107 27629 27102 1103 133 190 17670 135 1104 133 190 17670 135 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.551501 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.552939 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.558012 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:02.560465 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:02.561475 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:02.562535 140393886611328 run_classifier.py:464] tokens: [CLS] | style = \" vertical - al ##ign : middle ; pad ##ding : 3 ##p ##x ; \" | I ' ve started the Bad ##finger w ##iki am ##d I need help . You seem to know a lot about them , could you please help out ? [SEP]\n",
            "I0801 08:01:02.563718 140393886611328 run_classifier.py:465] input_ids: 101 197 1947 134 107 7391 118 2393 11368 131 2243 132 12921 3408 131 124 1643 1775 132 107 197 146 112 1396 1408 1103 6304 22225 192 12635 1821 1181 146 1444 1494 119 1192 3166 1106 1221 170 1974 1164 1172 117 1180 1128 4268 1494 1149 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.564841 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.565966 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.568307 140393886611328 run_classifier.py:468] label: 1 (id = 0)\n",
            "I0801 08:01:02.572258 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:02.573254 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:02.574437 140393886611328 run_classifier.py:464] tokens: [CLS] These are my numbers : 7 years in Wikipedia , 6 years as an ad ##min , 570 + articles , 4 featured articles , 1 featured list , 21 Good articles , 60 D ##Y ##K ' s - After six years as an ad ##min . I recently made some mistakes and I can understand if I am placed in some type of probation were I am monitored and forbidden to use my tools maybe for a year , but do I really merit the removal of my ad ##mins ##hip ? [SEP]\n",
            "I0801 08:01:02.575376 140393886611328 run_classifier.py:465] input_ids: 101 1636 1132 1139 2849 131 128 1201 1107 18920 117 127 1201 1112 1126 8050 7937 117 28081 116 4237 117 125 2081 4237 117 122 2081 2190 117 1626 2750 4237 117 2539 141 3663 2428 112 188 118 1258 1565 1201 1112 1126 8050 7937 119 146 3055 1189 1199 12572 1105 146 1169 2437 1191 146 1821 1973 1107 1199 2076 1104 23793 1127 146 1821 19232 1105 12031 1106 1329 1139 5537 2654 1111 170 1214 117 1133 1202 146 1541 16008 1103 8116 1104 1139 8050 19296 3157 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.577082 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.577992 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.579243 140393886611328 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0801 08:01:02.581481 140393886611328 run_classifier.py:461] *** Example ***\n",
            "I0801 08:01:02.583432 140393886611328 run_classifier.py:462] guid: None\n",
            "I0801 08:01:02.585998 140393886611328 run_classifier.py:464] tokens: [CLS] I couldn ' t tell you why g ##lam rock was there . Better ? [SEP]\n",
            "I0801 08:01:02.587245 140393886611328 run_classifier.py:465] input_ids: 101 146 1577 112 189 1587 1128 1725 176 7609 2067 1108 1175 119 8529 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.588185 140393886611328 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.589793 140393886611328 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0801 08:01:02.591863 140393886611328 run_classifier.py:468] label: 0 (id = 1)\n",
            "I0801 08:01:02.739287 140393886611328 estimator.py:209] Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faf60087b38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0801 08:01:02.748723 140393886611328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0801 08:01:03.854731 140393886611328 estimator.py:1145] Calling model_fn.\n",
            "I0801 08:01:06.751242 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0801 08:01:06.890378 140393886611328 deprecation.py:323] From <ipython-input-2-46a055aefb92>:65: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0801 08:01:07.404201 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0801 08:01:07.406480 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0801 08:01:07.413540 140393886611328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0801 08:01:07.430923 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0801 08:01:07.639803 140393886611328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0801 08:01:11.287153 140393886611328 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "W0801 08:01:17.672319 140393886611328 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0801 08:01:18.136183 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:01:18.138835 140393886611328 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0801 08:01:21.419868 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "I0801 08:01:27.825024 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:01:28.049707 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:01:37.260469 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 0 into output/model.ckpt.\n",
            "I0801 08:01:50.879715 140393886611328 basic_session_run_hooks.py:262] loss = 1.03264, step = 0\n",
            "I0801 08:02:52.390023 140393886611328 basic_session_run_hooks.py:692] global_step/sec: 1.62572\n",
            "I0801 08:02:52.391474 140393886611328 basic_session_run_hooks.py:260] loss = 0.3857031, step = 100 (61.512 sec)\n",
            "I0801 08:03:02.592556 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 123 into output/model.ckpt.\n",
            "I0801 08:03:09.049999 140393886611328 estimator.py:368] Loss for final step: 0.20567347.\n",
            "I0801 08:03:09.186478 140393886611328 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "End of epoch 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:03:12.120138 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0801 08:03:21.064182 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:03:21.086401 140393886611328 evaluation.py:255] Starting evaluation at 2019-08-01T08:03:21Z\n",
            "I0801 08:03:22.733125 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "W0801 08:03:22.742737 140393886611328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0801 08:03:22.748574 140393886611328 saver.py:1280] Restoring parameters from output/model.ckpt-123\n",
            "I0801 08:03:25.147695 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:03:25.407461 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:03:29.377392 140393886611328 evaluation.py:275] Finished evaluation at 2019-08-01-08:03:29\n",
            "I0801 08:03:29.378694 140393886611328 estimator.py:2039] Saving dict for global step 123: auc = 0.8714286, eval_accuracy = 0.87096775, f1_score = 0.8727272, false_negatives = 16.0, false_positives = 12.0, global_step = 123, loss = 0.36494896, precision = 0.8888889, recall = 0.85714287, true_negatives = 93.0, true_positives = 96.0\n",
            "I0801 08:03:31.643271 140393886611328 estimator.py:2099] Saving 'checkpoint_path' summary for global step 123: output/model.ckpt-123\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'auc': 0.8714286, 'eval_accuracy': 0.87096775, 'f1_score': 0.8727272, 'false_negatives': 16.0, 'false_positives': 12.0, 'loss': 0.36494896, 'precision': 0.8888889, 'recall': 0.85714287, 'true_negatives': 93.0, 'true_positives': 96.0, 'global_step': 123}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:03:32.709995 140393886611328 estimator.py:1145] Calling model_fn.\n",
            "I0801 08:03:35.998855 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0801 08:03:44.996291 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:03:44.999146 140393886611328 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0801 08:03:46.701639 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "I0801 08:03:46.714601 140393886611328 saver.py:1280] Restoring parameters from output/model.ckpt-123\n",
            "W0801 08:03:48.571978 140393886611328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0801 08:03:49.250879 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:03:49.487899 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:03:58.771362 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 123 into output/model.ckpt.\n",
            "I0801 08:04:11.913920 140393886611328 basic_session_run_hooks.py:262] loss = 0.17243105, step = 123\n",
            "I0801 08:05:14.873892 140393886611328 basic_session_run_hooks.py:692] global_step/sec: 1.58829\n",
            "I0801 08:05:14.875294 140393886611328 basic_session_run_hooks.py:260] loss = 0.18582545, step = 223 (62.961 sec)\n",
            "I0801 08:05:24.999996 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 246 into output/model.ckpt.\n",
            "I0801 08:05:31.535321 140393886611328 estimator.py:368] Loss for final step: 0.02106647.\n",
            "I0801 08:05:31.673013 140393886611328 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "End of epoch 2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:05:34.937198 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0801 08:05:43.920698 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:05:43.944471 140393886611328 evaluation.py:255] Starting evaluation at 2019-08-01T08:05:43Z\n",
            "I0801 08:05:45.591808 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "I0801 08:05:45.603909 140393886611328 saver.py:1280] Restoring parameters from output/model.ckpt-246\n",
            "I0801 08:05:48.059813 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:05:48.304451 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:05:52.284791 140393886611328 evaluation.py:275] Finished evaluation at 2019-08-01-08:05:52\n",
            "I0801 08:05:52.286082 140393886611328 estimator.py:2039] Saving dict for global step 246: auc = 0.8732143, eval_accuracy = 0.87096775, f1_score = 0.8653845, false_negatives = 22.0, false_positives = 6.0, global_step = 246, loss = 0.4843882, precision = 0.9375, recall = 0.8035714, true_negatives = 99.0, true_positives = 90.0\n",
            "I0801 08:05:52.290525 140393886611328 estimator.py:2099] Saving 'checkpoint_path' summary for global step 246: output/model.ckpt-246\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'auc': 0.8732143, 'eval_accuracy': 0.87096775, 'f1_score': 0.8653845, 'false_negatives': 22.0, 'false_positives': 6.0, 'loss': 0.4843882, 'precision': 0.9375, 'recall': 0.8035714, 'true_negatives': 99.0, 'true_positives': 90.0, 'global_step': 246}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:05:53.364728 140393886611328 estimator.py:1145] Calling model_fn.\n",
            "I0801 08:05:56.490273 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0801 08:06:05.272269 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:06:05.274793 140393886611328 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0801 08:06:06.929642 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "I0801 08:06:06.941531 140393886611328 saver.py:1280] Restoring parameters from output/model.ckpt-246\n",
            "I0801 08:06:09.405717 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:06:09.649559 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:06:18.595783 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 246 into output/model.ckpt.\n",
            "I0801 08:06:31.548854 140393886611328 basic_session_run_hooks.py:262] loss = 0.012953824, step = 246\n",
            "I0801 08:07:34.270844 140393886611328 basic_session_run_hooks.py:692] global_step/sec: 1.59432\n",
            "I0801 08:07:34.272235 140393886611328 basic_session_run_hooks.py:260] loss = 0.0010671738, step = 346 (62.723 sec)\n",
            "I0801 08:07:44.361317 140393886611328 basic_session_run_hooks.py:606] Saving checkpoints for 369 into output/model.ckpt.\n",
            "I0801 08:07:50.871142 140393886611328 estimator.py:368] Loss for final step: 0.0020271172.\n",
            "I0801 08:07:51.015397 140393886611328 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "End of epoch 3.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0801 08:07:54.260673 140393886611328 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0801 08:08:03.202103 140393886611328 estimator.py:1147] Done calling model_fn.\n",
            "I0801 08:08:03.223937 140393886611328 evaluation.py:255] Starting evaluation at 2019-08-01T08:08:03Z\n",
            "I0801 08:08:04.905133 140393886611328 monitored_session.py:240] Graph was finalized.\n",
            "I0801 08:08:04.918072 140393886611328 saver.py:1280] Restoring parameters from output/model.ckpt-369\n",
            "I0801 08:08:07.409672 140393886611328 session_manager.py:500] Running local_init_op.\n",
            "I0801 08:08:07.660668 140393886611328 session_manager.py:502] Done running local_init_op.\n",
            "I0801 08:08:11.608978 140393886611328 evaluation.py:275] Finished evaluation at 2019-08-01-08:08:11\n",
            "I0801 08:08:11.610624 140393886611328 estimator.py:2039] Saving dict for global step 369: auc = 0.8904762, eval_accuracy = 0.8894009, f1_score = 0.88888884, false_negatives = 16.0, false_positives = 8.0, global_step = 369, loss = 0.59378725, precision = 0.9230769, recall = 0.85714287, true_negatives = 97.0, true_positives = 96.0\n",
            "I0801 08:08:11.627820 140393886611328 estimator.py:2099] Saving 'checkpoint_path' summary for global step 369: output/model.ckpt-369\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'auc': 0.8904762, 'eval_accuracy': 0.8894009, 'f1_score': 0.88888884, 'false_negatives': 16.0, 'false_positives': 8.0, 'loss': 0.59378725, 'precision': 0.9230769, 'recall': 0.85714287, 'true_negatives': 97.0, 'true_positives': 96.0, 'global_step': 369}\n",
            "[{'auc': 0.8714286, 'eval_accuracy': 0.87096775, 'f1_score': 0.8727272, 'false_negatives': 16.0, 'false_positives': 12.0, 'loss': 0.36494896, 'precision': 0.8888889, 'recall': 0.85714287, 'true_negatives': 93.0, 'true_positives': 96.0, 'global_step': 123}, {'auc': 0.8732143, 'eval_accuracy': 0.87096775, 'f1_score': 0.8653845, 'false_negatives': 22.0, 'false_positives': 6.0, 'loss': 0.4843882, 'precision': 0.9375, 'recall': 0.8035714, 'true_negatives': 99.0, 'true_positives': 90.0, 'global_step': 246}, {'auc': 0.8904762, 'eval_accuracy': 0.8894009, 'f1_score': 0.88888884, 'false_negatives': 16.0, 'false_positives': 8.0, 'loss': 0.59378725, 'precision': 0.9230769, 'recall': 0.85714287, 'true_negatives': 97.0, 'true_positives': 96.0, 'global_step': 369}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}