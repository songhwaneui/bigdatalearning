{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTagger.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixmf_AX6AeR5",
        "colab_type": "text"
      },
      "source": [
        "## 1. CoNLL-2003 Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NZW2Ayi8qMT",
        "colab_type": "code",
        "outputId": "1dbfcc46-7fc8-4f1e-c7c2-405a66803839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "source": [
        "!wget -O CoNLL-2003.zip https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
        "!mkdir CoNLL-2003\n",
        "!unzip CoNLL-2003.zip -d CoNLL-2003\n",
        "!rm CoNLL-2003.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-31 06:25:53--  https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip [following]\n",
            "--2019-07-31 06:25:53--  https://www.dropbox.com/s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline/Alt5zsQ5kguUb-PfweUMC_I_5L2g3btpWv2IXyLyAuufKrzUst5XEty5mjsgTYRprub5pQno05Y6oyZyKd2PpYiVcZVZV4PHmlUWEHG_l-sc_w/file# [following]\n",
            "--2019-07-31 06:25:53--  https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline/Alt5zsQ5kguUb-PfweUMC_I_5L2g3btpWv2IXyLyAuufKrzUst5XEty5mjsgTYRprub5pQno05Y6oyZyKd2PpYiVcZVZV4PHmlUWEHG_l-sc_w/file\n",
            "Resolving uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com (uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com (uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AlvADs-WGfPvX1bBNVFR_5fwzPCtPQ1RLlWMtJTvZ_Pi1r3y-u2y5wAvOe7sZGjlhqS0X2Yy3-OYvmCrs2ETcjVOafpsrm2QSTtpTtqbgjJDwsrWlhncCmnHKxBumKI2cJACdeNI_MUqyTJ7Z7o9rg4X6wnzhd-yN80j4f14r6U3-GZej_sd6YqgDeKbyXu8TLzUQNoMtR2hLnV8zitfNBDXi10Kcf6v4U1GUYW-Bfqp4a4Q7TS4W9klo8SmZsxUCZ4Ju1IlBrzq2hQyibX1Th4sdLJuzB44NCUdSUVnfWjkE7MPky2-DKMNMQX7p8QBXWyqp90yDoQEg-2Vqfs165Sa/file [following]\n",
            "--2019-07-31 06:25:53--  https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline2/AlvADs-WGfPvX1bBNVFR_5fwzPCtPQ1RLlWMtJTvZ_Pi1r3y-u2y5wAvOe7sZGjlhqS0X2Yy3-OYvmCrs2ETcjVOafpsrm2QSTtpTtqbgjJDwsrWlhncCmnHKxBumKI2cJACdeNI_MUqyTJ7Z7o9rg4X6wnzhd-yN80j4f14r6U3-GZej_sd6YqgDeKbyXu8TLzUQNoMtR2hLnV8zitfNBDXi10Kcf6v4U1GUYW-Bfqp4a4Q7TS4W9klo8SmZsxUCZ4Ju1IlBrzq2hQyibX1Th4sdLJuzB44NCUdSUVnfWjkE7MPky2-DKMNMQX7p8QBXWyqp90yDoQEg-2Vqfs165Sa/file\n",
            "Reusing existing connection to uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691339 (675K) [application/zip]\n",
            "Saving to: ‘CoNLL-2003.zip’\n",
            "\n",
            "CoNLL-2003.zip      100%[===================>] 675.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-07-31 06:25:53 (17.0 MB/s) - ‘CoNLL-2003.zip’ saved [691339/691339]\n",
            "\n",
            "mkdir: cannot create directory ‘CoNLL-2003’: File exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-80f2f4ff43d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget -O CoNLL-2003.zip https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir CoNLL-2003'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip CoNLL-2003.zip -d CoNLL-2003'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm CoNLL-2003.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91llDkIA95Z",
        "colab_type": "text"
      },
      "source": [
        "## 2. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XstaB1Wa6Obr",
        "colab_type": "code",
        "outputId": "d3146ae8-147b-4cf0-e88b-8c9d55f9f4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import json\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import tqdm\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_4JKAp8BKVA",
        "colab_type": "text"
      },
      "source": [
        "## 3. Logger Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKMt1HGt_Tel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 학습하면서 디버깅을 위해 로그 남겨야 하므로 \n",
        "\n",
        "def init_logger(path:str):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    debug_fh = logging.FileHandler(os.path.join(path, \"debug.log\"))\n",
        "    debug_fh.setLevel(logging.DEBUG)\n",
        "\n",
        "    info_fh = logging.FileHandler(os.path.join(path, \"info.log\"))\n",
        "    info_fh.setLevel(logging.INFO)\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "\n",
        "    info_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')\n",
        "    debug_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s | %(lineno)d:%(funcName)s')\n",
        "\n",
        "    ch.setFormatter(info_formatter)\n",
        "    info_fh.setFormatter(info_formatter)\n",
        "    debug_fh.setFormatter(debug_formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(debug_fh)\n",
        "    logger.addHandler(info_fh)\n",
        "\n",
        "    return logger\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZedCFnABXYA",
        "colab_type": "text"
      },
      "source": [
        "## 4. Hyperparameters Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX0vqhpz-N_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams_dict = {\n",
        "  \"root_dir\": \"out_dirs/KoreaUniv_Data/TEST/\",\n",
        "  \"vocab_size\": 10000,\n",
        "  \"num_epochs\": 10,\n",
        "  \"batch_size\": 16,\n",
        "  \"embedding_dim\": 100,\n",
        "  \"rnn_hidden_dim\": 128,\n",
        "  \"rnn_depth\": 3,\n",
        "  \"dropout_keep_prob\": 1.0\n",
        "}\n",
        "\n",
        "## 0.8 하면 checkpoint  문제 생긴다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQw791j-lDB",
        "colab_type": "code",
        "outputId": "45b9575a-b71a-4d94-8a0e-797b4a25416a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "root_dir = os.path.join(hparams_dict[\"root_dir\"], \"%s/\" % timestamp)\n",
        "logger = init_logger(root_dir)\n",
        "logger.info(\"Hyper-parameters: %s\" %str(hparams_dict))\n",
        "hparams_dict[\"root_dir\"] = root_dir\n",
        "hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "data_dir = \"./CoNLL-2003\"\n",
        "dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n",
        "logger = logging.getLogger(__name__)\n",
        "iterator_initializers = [] ## 한바퀴 에폭 돌때마다 파라메터 재설정"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:30:51,054 | INFO     | Hyper-parameters: {'root_dir': 'out_dirs/KoreaUniv_Data/TEST/', 'vocab_size': 10000, 'num_epochs': 10, 'batch_size': 16, 'embedding_dim': 100, 'rnn_hidden_dim': 128, 'rnn_depth': 3, 'dropout_keep_prob': 1.0}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Pw09XzBl40",
        "colab_type": "text"
      },
      "source": [
        "## 5. Make Vocab Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7151biR-lcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_vocab_table():\n",
        "    \"\"\"\n",
        "    [A]\n",
        "    Vocabulary(단어집) 파일을 로드합니다.\n",
        "    단어 -> id, id -> 단어 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n",
        "        vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "    \n",
        "    print(\"vocab_sample\", vocab[0:10])\n",
        "    print(\"vocab_size\", len(vocab))\n",
        "    \n",
        "    ## 10000번째 넘어가면 그냥 unknown 처리\n",
        "    if len(vocab) > hparams.vocab_size:\n",
        "        vocab = vocab[:hparams.vocab_size]\n",
        "        \n",
        "    print(\"processed vocab\",len(vocab))\n",
        "    \n",
        "    \n",
        "    id2word = vocab\n",
        "    word2id = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "        word2id[word] = i\n",
        "    print([word for idx, word in enumerate(word2id.keys()) if idx < 10])\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [B]\n",
        "    Label(태그 모음) 파일을 로드합니다.\n",
        "    태그 -> id, id -> 태그 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n",
        "      labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "      labels.insert(0, \"PAD\")\n",
        "      id2label = labels ## 리스트\n",
        "      label2id = {} ## 딕셔너리\n",
        "      for i, label in enumerate(labels):\n",
        "          label2id[label] = i\n",
        "    print(label2id)\n",
        "    \n",
        "    return (id2word, word2id), (id2label, label2id)\n",
        "make_vocab_table()\n",
        "\n",
        "## <pad> 어떤 문장은 짧고 어떤문장은 긴데 i like you 3, 그다음 i like you so much 5  이러면 문장길이를 5에 다 맞춰줘야됨 매트릭스 하나에 넣을수 있겠끔 이떄 하는게\n",
        "## 패딩작업"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regd4vxPBnNA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Build Graph (Sequence Tagger Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyr0hDR8_p9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(inputs:tf.Tensor, lengths:tf.Tensor, id2word, id2label):\n",
        "      print(\"Building graph for model: sequence tagger\")\n",
        "\n",
        "      \"\"\"\n",
        "      [C]\n",
        "      단어 임베딩 행렬을 생성합니다.\n",
        "      단어 id를 단어 임베딩 텐서로 변환합니다.\n",
        "      \"\"\"\n",
        "      # Number of possible output categories.\n",
        "      output_dim = len(id2label)\n",
        "      vocab_size = len(id2word) + 1\n",
        "      \n",
        "      \n",
        "      # \n",
        "      embeddings = tf.get_variable(\n",
        "          \"embeddings\",\n",
        "          shape=[vocab_size, hparams.embedding_dim],\n",
        "          initializer=tf.initializers.variance_scaling(\n",
        "              scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n",
        "      )\n",
        "      \n",
        "      # i like you 1, 843, 517 \n",
        "      \n",
        "      # [batch_size, squence_length] : inputs 문장최대길이\n",
        "      embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "      \n",
        "      # shape = [batch_size, sequence_length(time), embed_dim(100)]\n",
        "      layer_out = embedded\n",
        "\n",
        "      \"\"\"\n",
        "      [D]\n",
        "      단어 임베딩을 RNN의 입력으로 사용하기 전,\n",
        "      차원 수를 맞춰주고 성능을 향상시키기 위해\n",
        "      projection layer를 생성하여 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "      # batch, sequence_length, embedding_dim -> batch, sequence_length, rnn_hidden_dim\n",
        "      \n",
        "      ## dense가 rnn_hidden_dim으로 자동으로 만들어줌\n",
        "      layer_out = tf.layers.dense(\n",
        "          inputs=layer_out,\n",
        "          units=hparams.rnn_hidden_dim,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_initializer=tf.initializers.variance_scaling(\n",
        "              scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n",
        "          name=\"input_projection\"\n",
        "      )\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [E]\n",
        "      양방향 RNN을 생성하고, 여기에 텐서를 통과시킵니다.\n",
        "      이렇게 하여, 단어간 의존 관계가 반영된 단어 자질 텐서를 얻습니다.\n",
        "      \"\"\"\n",
        "\n",
        "      with tf.variable_scope(\"bi-RNN\"):\n",
        "          # Build RNN layers\n",
        "          ## GRU or LSTM 둘중 어느것을 쓸거냐! 메모리 작은 GRU\n",
        "          ## lstm 성능 조오타!\n",
        "          \n",
        "          # GRUCell\n",
        "          rnn_cell_forward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "          rnn_cell_backward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "\n",
        "          # Apply dropout to RNN\n",
        "          ## 오버피팅 방지를 위해 학습속도를 느려지나 컨버즈 dropout 데이터를 없애는 그런 정규화 작업\n",
        "          \n",
        "          if hparams.dropout_keep_prob < 1.0:\n",
        "              rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward, output_keep_prob=dropout_keep_prob_ph)\n",
        "              rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward, output_keep_prob=dropout_keep_prob_ph)\n",
        "\n",
        "          # 로컬환경에서 할떄 RNN 레이어 더 쌓는것\n",
        "          # Stack multiple layers of RNN\n",
        "          # rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * hparams.rnn_depth)\n",
        "          # rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * hparams.rnn_depth)\n",
        "\n",
        "          ## 자체적으로 함수를 잘라서\n",
        "          ## I LIKE YOU // I LIKE YOU SO MUCH\n",
        "          ## 레이어 3개  레이어 5개\n",
        "          ## \n",
        "          #  (output_forward, output_backward),((forward_final_cell, forward_final_hidden)) _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "         \n",
        "        (output_forward, output_backward), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "              rnn_cell_forward, rnn_cell_backward,\n",
        "              inputs=layer_out,\n",
        "              sequence_length=lengths,\n",
        "              dtype=tf.float32\n",
        "          )\n",
        "          hiddens = tf.concat([output_forward, output_backward], axis=-1)\n",
        "          # output_forward : [batch, max_sequence_length, renn_hidden_dim]\n",
        "          ## output_backward : [batch, max_sequence_length, rnn_hidden_dim]\n",
        "          \n",
        "          # output_forward_hidden : [batch, rnn_hidden_dim] -> 문장의 제일 마지막 단어 rnn_hidden_state\n",
        "          # output_backward_hidden : [batch, rnn_hidden_dim] - > 문장의 제일 첫번쨰 단어 rnn_hidden_state\n",
        "            # => 이 두개 concat 한게 문장의 전체적인 representation\n",
        "            \n",
        "          \n",
        "          # shape = [batch_size, time, rnn_dim*2]\n",
        "          # lstm 을 통과한 256개  이러게 시\n",
        "                # shape = [batch_size, max_sequence_length, , rnn_hidden_dim*2]\n",
        "\n",
        "      \"\"\"\n",
        "      [F]\n",
        "      마스킹을 적용하여 문장 길이를 통일하기 위해 적용했던 padding을 제거합니다.\n",
        "      \"\"\"\n",
        "      # Donald Trump is the president of the United States 문장길이 10  [1,10,100]\n",
        "      # Barack Obama was the president  문장길이 5   . <pad> <pad> <pad> <pad> [1,10,100]\n",
        "      # i live in paris  <pad> <pad> <pad> <pad> <pad> [1,10,100]\n",
        "      \n",
        "      # [3,10,100] (embedding ) -> [3, 10, 128] (inpput_projection) -> [3,10,128(rnn_forard), [3,10,128](rnn_backward) ->[3,10,256](hiddens)\n",
        "      # [3,10,256] - > [3, 10, 10]\n",
        "      \n",
        "      ## 패딩을 없애는 작업이 마스크 패딩 차원떄문에 학습 잘 안될수도 있다.]\n",
        "      \n",
        "      # [10, 6, 5]\n",
        "      # [[True, ture, True, ture,Truem]~~]\n",
        "      # [True, ~~False~~~]\n",
        "      # [True, ~ False ~~~~~~~~``]]  이런 [3,10] 뱉음\n",
        "      # mask : [3, 10],\n",
        "      # hidden : [3, 10, 256]\n",
        "      \n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, hparams.rnn_hidden_dim * 2])  ## false 인거 뺴고 true 만 가져옴\n",
        "      # bi_lstm_out : [21,256]\n",
        "      layer_out = bi_lstm_out  # shape=[sum of seq length, 2*LSTM hidden layer size]\n",
        "\n",
        "      \"\"\"\n",
        "      [G]\n",
        "      단어 자질 텐서를 바탕으로 단어의 태그를 예측합니다.\n",
        "      이를 위해 fully-connected(dense) layer를 생성하고 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "      # [21. 256] W : [256, 10] , b[10]  -> [21, 10] 로 줄임\n",
        "      \n",
        "      with tf.variable_scope(\"read-out\"):\n",
        "        prev_layer_size = layer_out.get_shape().as_list()[1]\n",
        "        weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim],\n",
        "                                 initializer=tf.initializers.variance_scaling(\n",
        "                                     scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n",
        "                                 ))\n",
        "        bias = tf.get_variable(\"bias\", shape=[output_dim],\n",
        "                               initializer=tf.initializers.zeros())\n",
        "        predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n",
        "\n",
        "        ###########\n",
        "        # 크로스 벡터에 활성함수 이미 쓰이므로 쓸필요 x 쓸거면 또 프로젝션? \n",
        "#         tf.layers.dense(\n",
        "#             inputs =,\n",
        "#             units = ,\n",
        "#             kernel_initializer,\n",
        "#             bias_initializer\n",
        "#         )\n",
        "        \n",
        "        \n",
        "        \n",
        "      return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmF2AfNBnsp",
        "colab_type": "text"
      },
      "source": [
        "## 7. Load Data (tf.data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5TRwTMs-ljq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 단어가 들어왔으면 그 id로 바꿔주는 텐서 오퍼레이션 index_table_from_tensor\n",
        "def load_data(id2word, word2id, id2label, label2id):\n",
        "      \"\"\"\n",
        "      [L]\n",
        "      단어->id 및 태그->id 변환 테이블을 텐서 그래프에 추가합니다.\n",
        "      \"\"\"\n",
        "      word2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "        mapping=tf.constant(id2word),\n",
        "        num_oov_buckets=1,\n",
        "        name=\"word2id\"\n",
        "      )\n",
        "      \n",
        "      #print(word2id) 텐서그래프 찍어보면서 하자\n",
        "      \n",
        "      label2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "        mapping=tf.constant(id2label),\n",
        "        default_value=label2id[\"O\"],\n",
        "        name=\"label2id\"\n",
        "      )\n",
        "     #  print(label2id)\n",
        "      \"\"\"\n",
        "      [M]\n",
        "      입력 데이터 파일을 읽어들여 이를 단어 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "      input_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.inputs\"))\n",
        "      batched_input_dataset = input_dataset.batch(hparams.batch_size) # 배치사이즈 결정\n",
        "      input_iterator = batched_input_dataset.make_initializable_iterator()\n",
        "      batch_input = input_iterator.get_next()\n",
        "      batch_input.set_shape([hparams.batch_size])\n",
        "      words = tf.string_split(batch_input, \" \")\n",
        "      word_ids = word2id.lookup(words)\n",
        "      dense_word_ids = tf.sparse_tensor_to_dense(word_ids)\n",
        "      # shape = [batch_size, time]\n",
        "      \n",
        "      \n",
        "      line_number = word_ids.indices[:, 0]\n",
        "      line_position = word_ids.indices[:, 1]\n",
        "      lengths = tf.segment_max(data=line_position,\n",
        "                               segment_ids=line_number) + 1\n",
        "\n",
        "      \"\"\"\n",
        "      [N]\n",
        "      태그 데이터 파일을 읽어들여 이를 태그 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "\n",
        "      label_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.labels\"))\n",
        "      batched_label_dataset = label_dataset.batch(hparams.batch_size)\n",
        "      label_iterator = batched_label_dataset.make_initializable_iterator()\n",
        "      batch_label_str = label_iterator.get_next()\n",
        "      batch_label = tf.string_split(batch_label_str, \" \")\n",
        "      label_ids = label2id.lookup(batch_label)\n",
        "      dense_label_ids = tf.sparse_tensor_to_dense(label_ids)\n",
        "      # shape = [batch_size, time]\n",
        "\n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      dense_label_ids = tf.boolean_mask(dense_label_ids, mask)\n",
        "\n",
        "      iterator_initializers.append(input_iterator.initializer)\n",
        "      iterator_initializers.append(label_iterator.initializer)\n",
        "\n",
        "      return dense_word_ids, dense_label_ids, lengths\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XTEfi0RBoJT",
        "colab_type": "text"
      },
      "source": [
        "## 8. Train Model (session call)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN0gsvxlGDPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_palceholders():\n",
        "  placeholder_list = []\n",
        "  \n",
        "  return placeholder_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH4rPoA9GDN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjm5_0Bj-lp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def train_model():\n",
        "        sess = tf.Session()\n",
        "        with sess.as_default():\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            \n",
        "            ## vocab 만드는 부분\n",
        "            (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "            \n",
        "            ## data loading -> data process 함수 작성하여 placeholder로 받을수 있게 수정 \n",
        "            inputs, labels, lengths = load_data(id2word, word2id, id2label, label2id)\n",
        "            \n",
        "            \n",
        "      \n",
        "            ## 실제 딥러닝 모델 구현\n",
        "            with tf.variable_scope(\"build_graph\", reuse=False):\n",
        "                logits = build_graph(inputs, lengths, id2word, id2label)\n",
        "\n",
        "            \"\"\"\n",
        "            [O]\n",
        "            모델을 훈련시키기 위해 필요한 오퍼레이션들을 텐서 그래프에 추가합니다.\n",
        "            여기에는 loss, train, accuracy 계산 등이 포함됩니다.\n",
        "            \"\"\"\n",
        "            \n",
        "            \n",
        "            ## 실제 학습이 진행되는 부분\n",
        "            loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels,\n",
        "                                                                     name=\"cross_entropy\")\n",
        "            \n",
        "            \n",
        "            loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n",
        "            train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n",
        "\n",
        "            eval = tf.nn.in_top_k(logits, labels, 1)\n",
        "            correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n",
        "            accuracy = tf.divide(correct_count, tf.shape(labels)[0])\n",
        "\n",
        "            # Initialize iterators, tables, and variables.\n",
        "            local_iterator_initializers = tf.group(*iterator_initializers)\n",
        "            tf.tables_initializer().run()\n",
        "            tf.global_variables_initializer().run()\n",
        "\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            for epochs_completed in range(hparams.num_epochs):\n",
        "                local_iterator_initializers.run()\n",
        "                accuracy_mean, loss_mean, idx_cnt = 0, 0, 0\n",
        "                while True:\n",
        "                    \"\"\"\n",
        "                    [P]\n",
        "                    그래프에 데이터를 입력하여 필요한 계산들을 수행하고,\n",
        "                    Loss에 따라 gradient를 계산하여 파라미터들을 업데이트합니다.\n",
        "                    이러한 과정을 training step이라고 합니다.\n",
        "                    \"\"\"\n",
        "                    try:\n",
        "                      accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n",
        "                          [accuracy, labels, loss_op, global_step, train_op],  ## train_op 콜해주면 학습됨\n",
        "                          feed_dict={dropout_keep_prob_ph: hparams.dropout_keep_prob}\n",
        "                      )\n",
        "                      accuracy_mean += accuracy_val\n",
        "                      loss_mean += loss_val\n",
        "                      idx_cnt += 1\n",
        "                      if global_step_val % 50 == 0:\n",
        "                          accuracy_mean /= idx_cnt\n",
        "                          loss_mean /= idx_cnt\n",
        "                          logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (global_step_val, loss_mean, accuracy_mean * 100))\n",
        "                          accuracy_mean, loss_mean,idx_cnt = 0, 0, 0\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                      # End of epoch.\n",
        "                      break\n",
        "\n",
        "                \"\"\"\n",
        "                [Q]\n",
        "                전체 학습 데이터에 대하여 1회 학습을 완료하였습니다.\n",
        "                이를 1 epoch라고 합니다.\n",
        "                딥러닝 모델의 학습은 일반적으로 수십~수백 epoch 동안 진행됩니다.\n",
        "                \n",
        "                \"\"\"\n",
        "                logger.info(\"End of epoch %d.\" % (epochs_completed+1))\n",
        "                save_path = saver.save(sess, \"saves/model.ckpt\", global_step=global_step_val)\n",
        "                logger.info(\"Model saved at: %s\" % save_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9dVWZEBpRv",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofY5we0ADFW",
        "colab_type": "code",
        "outputId": "fe184040-d708-4afc-b138-1efbc8dd1081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the vanilla Bi-directional LSTM model\n",
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<PAD>', '.', ',', 'the', 'of', 'in', 'to', 'a', '(', ')']\n",
            "{'PAD': 0, 'O': 1, 'B-LOC': 2, 'B-MISC': 3, 'B-ORG': 4, 'B-PER': 5, 'I-LOC': 6, 'I-MISC': 7, 'I-ORG': 8, 'I-PER': 9}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:31:06,931 | WARNING  | \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "2019-07-31 06:31:07,009 | WARNING  | From <ipython-input-11-4fc3abbc958a>:24: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "2019-07-31 06:31:07,037 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:978: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2019-07-31 06:31:07,114 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`normal` is a deprecated alias for `truncated_normal`\n",
            "2019-07-31 06:31:07,116 | WARNING  | From <ipython-input-10-b3945d8e9eee>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building graph for model: sequence tagger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:31:07,498 | WARNING  | From <ipython-input-10-b3945d8e9eee>:47: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "2019-07-31 06:31:07,501 | WARNING  | From <ipython-input-10-b3945d8e9eee>:63: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "2019-07-31 06:31:07,506 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "2019-07-31 06:31:07,606 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2019-07-31 06:31:07,619 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2019-07-31 06:31:15,072 | INFO     | [Step 50] loss: 1.2851, accuracy: 72.58%\n",
            "2019-07-31 06:31:20,304 | INFO     | [Step 100] loss: 0.7863, accuracy: 82.36%\n",
            "2019-07-31 06:31:25,112 | INFO     | [Step 150] loss: 0.7498, accuracy: 80.67%\n",
            "2019-07-31 06:31:29,806 | INFO     | [Step 200] loss: 0.7305, accuracy: 78.67%\n",
            "2019-07-31 06:31:34,344 | INFO     | [Step 250] loss: 0.6354, accuracy: 80.10%\n",
            "2019-07-31 06:31:38,881 | INFO     | [Step 300] loss: 0.5315, accuracy: 84.58%\n",
            "2019-07-31 06:31:41,421 | INFO     | [Step 350] loss: 0.5955, accuracy: 81.32%\n",
            "2019-07-31 06:31:45,269 | INFO     | [Step 400] loss: 0.4487, accuracy: 84.61%\n",
            "2019-07-31 06:31:49,432 | INFO     | [Step 450] loss: 0.4111, accuracy: 86.48%\n",
            "2019-07-31 06:31:54,156 | INFO     | [Step 500] loss: 0.3980, accuracy: 85.28%\n",
            "2019-07-31 06:31:59,035 | INFO     | [Step 550] loss: 0.3644, accuracy: 89.59%\n",
            "2019-07-31 06:32:03,743 | INFO     | [Step 600] loss: 0.3856, accuracy: 86.00%\n",
            "2019-07-31 06:32:09,114 | INFO     | [Step 650] loss: 0.3767, accuracy: 88.39%\n",
            "2019-07-31 06:32:13,523 | INFO     | [Step 700] loss: 0.3023, accuracy: 90.10%\n",
            "2019-07-31 06:32:18,784 | INFO     | [Step 750] loss: 0.3675, accuracy: 87.86%\n",
            "2019-07-31 06:32:24,711 | INFO     | [Step 800] loss: 0.3075, accuracy: 90.76%\n",
            "2019-07-31 06:32:29,393 | INFO     | [Step 850] loss: 0.3064, accuracy: 89.67%\n",
            "2019-07-31 06:32:35,319 | INFO     | [Step 900] loss: 0.2407, accuracy: 92.37%\n",
            "2019-07-31 06:32:39,309 | INFO     | End of epoch 1.\n",
            "2019-07-31 06:32:39,523 | INFO     | Model saved at: saves/model.ckpt-937\n",
            "2019-07-31 06:32:40,963 | INFO     | [Step 950] loss: 0.2918, accuracy: 91.49%\n",
            "2019-07-31 06:32:44,898 | INFO     | [Step 1000] loss: 0.2526, accuracy: 92.08%\n",
            "2019-07-31 06:32:50,474 | INFO     | [Step 1050] loss: 0.2453, accuracy: 92.69%\n",
            "2019-07-31 06:32:54,647 | INFO     | [Step 1100] loss: 0.1598, accuracy: 95.65%\n",
            "2019-07-31 06:33:00,209 | INFO     | [Step 1150] loss: 0.2174, accuracy: 93.34%\n",
            "2019-07-31 06:33:03,842 | INFO     | [Step 1200] loss: 0.2037, accuracy: 94.50%\n",
            "2019-07-31 06:33:08,210 | INFO     | [Step 1250] loss: 0.1754, accuracy: 94.12%\n",
            "2019-07-31 06:33:10,967 | INFO     | [Step 1300] loss: 0.1332, accuracy: 96.15%\n",
            "2019-07-31 06:33:15,704 | INFO     | [Step 1350] loss: 0.1494, accuracy: 95.76%\n",
            "2019-07-31 06:33:19,373 | INFO     | [Step 1400] loss: 0.1596, accuracy: 94.66%\n",
            "2019-07-31 06:33:24,552 | INFO     | [Step 1450] loss: 0.1597, accuracy: 94.84%\n",
            "2019-07-31 06:33:29,121 | INFO     | [Step 1500] loss: 0.1657, accuracy: 95.20%\n",
            "2019-07-31 06:33:34,407 | INFO     | [Step 1550] loss: 0.1371, accuracy: 95.59%\n",
            "2019-07-31 06:33:39,896 | INFO     | [Step 1600] loss: 0.2114, accuracy: 93.62%\n",
            "2019-07-31 06:33:43,822 | INFO     | [Step 1650] loss: 0.1213, accuracy: 96.48%\n",
            "2019-07-31 06:33:49,191 | INFO     | [Step 1700] loss: 0.1729, accuracy: 95.29%\n",
            "2019-07-31 06:33:54,941 | INFO     | [Step 1750] loss: 0.1676, accuracy: 94.99%\n",
            "2019-07-31 06:33:59,567 | INFO     | [Step 1800] loss: 0.0966, accuracy: 97.21%\n",
            "2019-07-31 06:34:05,761 | INFO     | [Step 1850] loss: 0.1106, accuracy: 96.88%\n",
            "2019-07-31 06:34:07,957 | INFO     | End of epoch 2.\n",
            "2019-07-31 06:34:08,108 | INFO     | Model saved at: saves/model.ckpt-1874\n",
            "2019-07-31 06:34:10,914 | INFO     | [Step 1900] loss: 0.1249, accuracy: 96.22%\n",
            "2019-07-31 06:34:14,877 | INFO     | [Step 1950] loss: 0.1103, accuracy: 96.74%\n",
            "2019-07-31 06:34:20,675 | INFO     | [Step 2000] loss: 0.1149, accuracy: 96.74%\n",
            "2019-07-31 06:34:24,615 | INFO     | [Step 2050] loss: 0.0589, accuracy: 98.25%\n",
            "2019-07-31 06:34:30,442 | INFO     | [Step 2100] loss: 0.1144, accuracy: 96.50%\n",
            "2019-07-31 06:34:33,401 | INFO     | [Step 2150] loss: 0.0782, accuracy: 97.94%\n",
            "2019-07-31 06:34:37,698 | INFO     | [Step 2200] loss: 0.0865, accuracy: 97.33%\n",
            "2019-07-31 06:34:40,710 | INFO     | [Step 2250] loss: 0.0607, accuracy: 98.13%\n",
            "2019-07-31 06:34:45,612 | INFO     | [Step 2300] loss: 0.0825, accuracy: 97.76%\n",
            "2019-07-31 06:34:49,267 | INFO     | [Step 2350] loss: 0.0683, accuracy: 98.04%\n",
            "2019-07-31 06:34:54,549 | INFO     | [Step 2400] loss: 0.1167, accuracy: 96.63%\n",
            "2019-07-31 06:34:59,033 | INFO     | [Step 2450] loss: 0.0753, accuracy: 97.82%\n",
            "2019-07-31 06:35:04,400 | INFO     | [Step 2500] loss: 0.1322, accuracy: 95.92%\n",
            "2019-07-31 06:35:09,306 | INFO     | [Step 2550] loss: 0.1130, accuracy: 96.41%\n",
            "2019-07-31 06:35:13,695 | INFO     | [Step 2600] loss: 0.0799, accuracy: 97.77%\n",
            "2019-07-31 06:35:19,748 | INFO     | [Step 2650] loss: 0.1282, accuracy: 96.53%\n",
            "2019-07-31 06:35:25,083 | INFO     | [Step 2700] loss: 0.1145, accuracy: 96.65%\n",
            "2019-07-31 06:35:30,424 | INFO     | [Step 2750] loss: 0.0587, accuracy: 98.30%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-20c114b28e0c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                     accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n\u001b[1;32m     45\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdropout_keep_prob_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     )\n\u001b[1;32m     48\u001b[0m                     \u001b[0maccuracy_mean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWxSlqZd_iou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_predict(saved_file:str):\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "    \"\"\"\n",
        "    [H]\n",
        "    입력 문자열을 단어/문장부호 단위로 쪼개고, 이를 다시 단어 id로 변환합니다.\n",
        "    \"\"\"\n",
        "    sentence = word_tokenize(sentence)\n",
        "    word_ids = []\n",
        "    (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in word2id:\n",
        "            word_ids.append(word2id[word])\n",
        "        else:\n",
        "            word_ids.append(len(word2id))\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        \"\"\"\n",
        "        [I]\n",
        "        태깅을 수행하기 위해 텐서 그래프를 생성합니다.\n",
        "        \"\"\"\n",
        "        dense_word_ids = tf.constant(word_ids)\n",
        "        lengths = tf.constant(len(word_ids))\n",
        "        # Insert batch dimension.\n",
        "        dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n",
        "        lengths = tf.expand_dims(lengths, axis=0)\n",
        "\n",
        "        with tf.variable_scope(\"build_graph\", reuse=tf.AUTO_REUSE):\n",
        "            logits = build_graph(dense_word_ids, lengths, id2word, id2label)\n",
        "        predictions = tf.argmax(logits, axis=1)\n",
        "\n",
        "        \"\"\"\n",
        "        [J]\n",
        "        저장된 모델을 로드하고, 데이터를 입력하여 태깅 결과를 얻습니다.\n",
        "        \"\"\"\n",
        "        print(saved_file)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, saved_file)\n",
        "        pred_val = sess.run(\n",
        "            [predictions]\n",
        "        )[0]\n",
        "\n",
        "    \"\"\"\n",
        "    [K]\n",
        "    태깅 결과를 출력합니다.\n",
        "    \"\"\"\n",
        "    pred_str = [id2label[i] for i in pred_val]\n",
        "    for word, tag in zip(sentence, pred_str):\n",
        "        print(\"%s[%s]\" %(word, tag), end=' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_B0m2rG-Dq",
        "colab_type": "code",
        "outputId": "14d9c831-50f2-46d2-a364-e678fcc565c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "load_and_predict(\"/content/saves/model.ckpt-937\")\n",
        "#load_and_predict(\"/content/saves/model.ckpt-1874\")\n",
        "\n",
        "#Donald Trup is the president of the united states\n",
        "## 트레이닝 코퍼스에 등장하지 않는 단어 => unknown\n",
        "# 캐릭터레벨 \n",
        "#워드피쳐 문자피쳐 합쳐서 새로운 피쳐 만드는 등\n",
        "# 워드에 대한 레프리젠테이션 ="
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a sentence: donal trump is the president of the united states\n",
            "['<PAD>', '.', ',', 'the', 'of', 'in', 'to', 'a', '(', ')']\n",
            "{'PAD': 0, 'O': 1, 'B-LOC': 2, 'B-MISC': 3, 'B-ORG': 4, 'B-PER': 5, 'I-LOC': 6, 'I-MISC': 7, 'I-ORG': 8, 'I-PER': 9}\n",
            "Building graph for model: sequence tagger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:39:39,712 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-07-31 06:39:39,719 | INFO     | Restoring parameters from /content/saves/model.ckpt-937\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/saves/model.ckpt-937\n",
            "donal[O] trump[O] is[O] the[O] president[O] of[O] the[O] united[O] states[O] "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpFWC8IXu4qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}