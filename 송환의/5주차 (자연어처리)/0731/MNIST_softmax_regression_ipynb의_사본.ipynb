{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_softmax_regression.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDxmgu4flT_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJCi_homZTd",
        "colab_type": "code",
        "outputId": "dd871781-8060-44d2-f49b-ff1ddf12f2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "mnist = input_data.read_data_sets(\"./data\", one_hot=True)\n",
        "r = random.randint(0, mnist.train.num_examples - 1)\n",
        "plt.imshow(mnist.train.images[r:r+1].reshape(28, 28),\n",
        "           cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 02:16:28.747725 140693631297408 deprecation.py:323] From <ipython-input-2-7950cd9991ba>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0731 02:16:28.753963 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0731 02:16:28.757788 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0731 02:16:28.853099 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 02:16:29.168582 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0731 02:16:29.173401 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0731 02:16:29.269942 140693631297408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrrCZ4ptmqrx",
        "colab_type": "text"
      },
      "source": [
        "## Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQWy_FHNlvX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PLACEHOLDERS\n",
        "model_inputs = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
        "labels = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
        "global_step = tf.Variable(0, name='global_step', trainable=False) # 배리어블이 변하는지 안변하는지 trainable 지정 => 프리테인드 인베딩 되있는거 그대로 하고 trainable =false로 하면 새로 내가 학습된 데이터만 됨\n",
        "                                                                  # trainable = true 하면 기존 학습 되있는거 다 날리고 내데이터로만 학습\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejya_BrbmupN",
        "colab_type": "text"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0YN-jkxlzTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VARIABLES\n",
        "w = tf.Variable(tf.random_normal(shape=[784, 10]))\n",
        "b = tf.Variable(tf.random_normal(shape=[10]))\n",
        "\n",
        "# tf.summary.histogram(\"weights\", w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGHdO8134zUo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4_HAqzdmLMC",
        "colab_type": "text"
      },
      "source": [
        "## Build a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNQtYEPQl35u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = tf.matmul(model_inputs, w) + b\n",
        "predictions = tf.nn.softmax(logits) ## 애는 0~9까지 확률 뽑아주는게 소프트 맥스 시그모이드는 0과 1로만 분류"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGWopJDLl5bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION\n",
        "compare_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))  ## 0~9 사이 하나 확률값 뱉어남\n",
        "# ACCURACY\n",
        "accuracy = tf.reduce_mean(tf.cast(compare_pred, \"float\"))\n",
        "# COST FUNCTION\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(labels*tf.log(predictions), reduction_indices=1))\n",
        "# OPTIMIZER, TRAIN OPERATION\n",
        "# train_op 모델세이브, ?? 어느때나 다 필요\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss, global_step=global_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 구글 텐서보드 검색 ㄱㄱ\n",
        "# 로컬 일때만 된다 환경세팅 해야한다.\n",
        "\n",
        "# tf.summary.scalar('loss',loss)\n",
        "# tf.summary.scalar('acc', accuracy)\n",
        "# merged = tf.summary.merge_all() ## 지금 서머리 했던거 다 합침"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz2FZMrfnIyK",
        "colab_type": "text"
      },
      "source": [
        "## Train a Model (session call)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glKuQxUhl9Eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04e3cd64-e2a4-4ad7-ed68-3e78953d96fe"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "# \t\t\t\ttimestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# \t\t\t\twriter = tf.summary.FileWriter('./logs/%s' % timestamp, sess.graph)\n",
        "# \t\t\t\tsaver = tf.train.Saver()\n",
        "## 서머리 할때\n",
        "\n",
        "\t\t\t\tsess.run(tf.global_variables_initializer())\n",
        "\t\t\t\n",
        "    ## 모델 간단하면 맣이 학습 시켜야함\n",
        "\t\t\t\tfor i in range(50):\n",
        "\t\t\t\t\t\tavg_loss = 0.\n",
        "\t\t\t\t\t\tfor step in range(10000):\n",
        "\t\t\t\t\t\t\t\tbatch_images, batch_labels = mnist.train.next_batch(100)  ##100*784 짜리 매트릭스가 들어왔다\n",
        "\t\t\t\t\t\t\t\tfeeds_train = {model_inputs: batch_images, labels: batch_labels}\n",
        "\t\t\t\t\t\t\t\t_, loss_val, global_step_val = sess.run([train_op, loss, global_step], feed_dict=feeds_train) # global step val 뒤에 summary_val =, ## sess.run 파라메터 안에 merged  run해줘야 한다\n",
        "\t\t\t\t\t\t\t\tavg_loss += loss_val\n",
        "                ## 로스는 계쏙 진동하면서 위아래로 튈거다.\n",
        "\t\t\t\t\t\t\t\tif (step+1) % 1000 == 0:\n",
        "\t\t\t\t\t\t\t\t\t\tprint (\"step {} | loss : {}\".format(step+1, avg_loss/(step+1)))\n",
        "                  # writer.add_summary(summary_val, global_step=sess.global_stemp_val) 이러면 텐서보드 만들어짐\n",
        "                  # cmd 창에서 텐서보드 지정하면 뜸 , 보드 여러개 띄우고 싶다 => 포트번호 여러개\n",
        "                  \n",
        "                  \n",
        "\t\t\t\t\t\tfeeds_test = {model_inputs: mnist.test.images, labels: mnist.test.labels}\n",
        "\t\t\t\t\t\ttrain_acc = sess.run(accuracy, feed_dict=feeds_train)\n",
        "\t\t\t\t\t\ttest_acc = sess.run(accuracy, feed_dict=feeds_test)\n",
        "\t\t\t\t\t\tprint(\"idx: %02d/50 cost: %.3f train_acc: %.3f test_acc: %.3f\"\n",
        "\t\t\t\t\t\t      % (i+1, avg_loss/(step+1), train_acc, test_acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 1000 | loss : 5.7233760468959805\n",
            "step 2000 | loss : 4.177150306463242\n",
            "step 3000 | loss : 3.417203836917877\n",
            "step 4000 | loss : 2.955724630251527\n",
            "step 5000 | loss : 2.6405041717886926\n",
            "step 6000 | loss : 2.4085545329749585\n",
            "step 7000 | loss : 2.2289431271425317\n",
            "step 8000 | loss : 2.0865234904028473\n",
            "step 9000 | loss : 1.9690194417205122\n",
            "step 10000 | loss : 1.8704817919433117\n",
            "idx: 01/50 cost: 1.870 train_acc: 0.740 test_acc: 0.806\n",
            "step 1000 | loss : 0.940369847804308\n",
            "step 2000 | loss : 0.9247679694592953\n",
            "step 3000 | loss : 0.9102341046333313\n",
            "step 4000 | loss : 0.8961560428664088\n",
            "step 5000 | loss : 0.8819769091308117\n",
            "step 6000 | loss : 0.8709711205412944\n",
            "step 7000 | loss : 0.8590929211335523\n",
            "step 8000 | loss : 0.8488284619208425\n",
            "step 9000 | loss : 0.839022851838006\n",
            "step 10000 | loss : 0.8302378223881125\n",
            "idx: 02/50 cost: 0.830 train_acc: 0.770 test_acc: 0.845\n",
            "step 1000 | loss : 0.7256354969143868\n",
            "step 2000 | loss : 0.7233176212906838\n",
            "step 3000 | loss : 0.7184589892079433\n",
            "step 4000 | loss : 0.7118614677637816\n",
            "step 5000 | loss : 0.7071868876039982\n",
            "step 6000 | loss : 0.703073312856257\n",
            "step 7000 | loss : 0.6981458480464561\n",
            "step 8000 | loss : 0.6932085461905226\n",
            "step 9000 | loss : 0.6890915437266231\n",
            "step 10000 | loss : 0.684287480764836\n",
            "idx: 03/50 cost: 0.684 train_acc: 0.840 test_acc: 0.861\n",
            "step 1000 | loss : 0.6385647979825735\n",
            "step 2000 | loss : 0.6331250184327364\n",
            "step 3000 | loss : 0.6298309166630109\n",
            "step 4000 | loss : 0.6266552301757038\n",
            "step 5000 | loss : 0.6232840178787709\n",
            "step 6000 | loss : 0.6198848645761609\n",
            "step 7000 | loss : 0.6168978127624307\n",
            "step 8000 | loss : 0.6142435552123934\n",
            "step 9000 | loss : 0.6113090363625023\n",
            "step 10000 | loss : 0.6082764777839184\n",
            "idx: 04/50 cost: 0.608 train_acc: 0.900 test_acc: 0.868\n",
            "step 1000 | loss : 0.5765524412989617\n",
            "step 2000 | loss : 0.5768494405150414\n",
            "step 3000 | loss : 0.5721760001977285\n",
            "step 4000 | loss : 0.5715732421651483\n",
            "step 5000 | loss : 0.5691734738349915\n",
            "step 6000 | loss : 0.5669094230557481\n",
            "step 7000 | loss : 0.5647529995228563\n",
            "step 8000 | loss : 0.5630388685185462\n",
            "step 9000 | loss : 0.5606307109263208\n",
            "step 10000 | loss : 0.5585744696006179\n",
            "idx: 05/50 cost: 0.559 train_acc: 0.900 test_acc: 0.874\n",
            "step 1000 | loss : 0.5371397503316403\n",
            "step 2000 | loss : 0.5366137978509069\n",
            "step 3000 | loss : 0.5337752846578757\n",
            "step 4000 | loss : 0.5316298948619514\n",
            "step 5000 | loss : 0.5303914997503162\n",
            "step 6000 | loss : 0.5283773366572956\n",
            "step 7000 | loss : 0.5275810413584113\n",
            "step 8000 | loss : 0.5256810451801867\n",
            "step 9000 | loss : 0.5238079465553165\n",
            "step 10000 | loss : 0.5224389348387718\n",
            "idx: 06/50 cost: 0.522 train_acc: 0.870 test_acc: 0.879\n",
            "step 1000 | loss : 0.5071008618026972\n",
            "step 2000 | loss : 0.5053405371308327\n",
            "step 3000 | loss : 0.5027376903320352\n",
            "step 4000 | loss : 0.5022464672010392\n",
            "step 5000 | loss : 0.5007187593460083\n",
            "step 6000 | loss : 0.49935423682133356\n",
            "step 7000 | loss : 0.49779405075205224\n",
            "step 8000 | loss : 0.49684999461658297\n",
            "step 9000 | loss : 0.49552699558478264\n",
            "step 10000 | loss : 0.49412102116905154\n",
            "idx: 07/50 cost: 0.494 train_acc: 0.810 test_acc: 0.883\n",
            "step 1000 | loss : 0.4852869257852435\n",
            "step 2000 | loss : 0.4822419800832868\n",
            "step 3000 | loss : 0.48074775127073127\n",
            "step 4000 | loss : 0.47909031805582347\n",
            "step 5000 | loss : 0.47806036675423386\n",
            "step 6000 | loss : 0.47682776452352604\n",
            "step 7000 | loss : 0.4757225038036704\n",
            "step 8000 | loss : 0.47491272599902\n",
            "step 9000 | loss : 0.4734677610819538\n",
            "step 10000 | loss : 0.47280504529103634\n",
            "idx: 08/50 cost: 0.473 train_acc: 0.920 test_acc: 0.885\n",
            "step 1000 | loss : 0.4596161195859313\n",
            "step 2000 | loss : 0.45965317120030524\n",
            "step 3000 | loss : 0.45921967095384997\n",
            "step 4000 | loss : 0.4578970001582056\n",
            "step 5000 | loss : 0.4577336419299245\n",
            "step 6000 | loss : 0.45732395578051604\n",
            "step 7000 | loss : 0.4558974910623261\n",
            "step 8000 | loss : 0.4552542330380529\n",
            "step 9000 | loss : 0.45448253876798683\n",
            "step 10000 | loss : 0.45372675497196613\n",
            "idx: 09/50 cost: 0.454 train_acc: 0.870 test_acc: 0.888\n",
            "step 1000 | loss : 0.44217336281388997\n",
            "step 2000 | loss : 0.4439690851531923\n",
            "step 3000 | loss : 0.44284448328614234\n",
            "step 4000 | loss : 0.4424809865988791\n",
            "step 5000 | loss : 0.44228024251312015\n",
            "step 6000 | loss : 0.4407326232058307\n",
            "step 7000 | loss : 0.44064447235422477\n",
            "step 8000 | loss : 0.4400210873140022\n",
            "step 9000 | loss : 0.4389742577349146\n",
            "step 10000 | loss : 0.43828080070875586\n",
            "idx: 10/50 cost: 0.438 train_acc: 0.900 test_acc: 0.890\n",
            "step 1000 | loss : 0.4309890059828758\n",
            "step 2000 | loss : 0.42927659370005133\n",
            "step 3000 | loss : 0.4290558876171708\n",
            "step 4000 | loss : 0.428594490936026\n",
            "step 5000 | loss : 0.4279143914297223\n",
            "step 6000 | loss : 0.4273918096808096\n",
            "step 7000 | loss : 0.42683040292880364\n",
            "step 8000 | loss : 0.42621222804300485\n",
            "step 9000 | loss : 0.4256682392317388\n",
            "step 10000 | loss : 0.42503432615324854\n",
            "idx: 11/50 cost: 0.425 train_acc: 0.830 test_acc: 0.892\n",
            "step 1000 | loss : 0.41888802314549684\n",
            "step 2000 | loss : 0.41720188899338245\n",
            "step 3000 | loss : 0.4173504233211279\n",
            "step 4000 | loss : 0.4167681988365948\n",
            "step 5000 | loss : 0.41609794638603925\n",
            "step 6000 | loss : 0.41554745340223115\n",
            "step 7000 | loss : 0.4153692600865449\n",
            "step 8000 | loss : 0.41429401677288114\n",
            "step 9000 | loss : 0.4139503838155005\n",
            "step 10000 | loss : 0.41343174338936806\n",
            "idx: 12/50 cost: 0.413 train_acc: 0.890 test_acc: 0.894\n",
            "step 1000 | loss : 0.4072342343479395\n",
            "step 2000 | loss : 0.4057541086152196\n",
            "step 3000 | loss : 0.40587731729696197\n",
            "step 4000 | loss : 0.40599026741832495\n",
            "step 5000 | loss : 0.4058249680995941\n",
            "step 6000 | loss : 0.4052096467576921\n",
            "step 7000 | loss : 0.40462191523717983\n",
            "step 8000 | loss : 0.40397565987333656\n",
            "step 9000 | loss : 0.4037242892152733\n",
            "step 10000 | loss : 0.4029615921765566\n",
            "idx: 13/50 cost: 0.403 train_acc: 0.930 test_acc: 0.895\n",
            "step 1000 | loss : 0.398140737619251\n",
            "step 2000 | loss : 0.3984594472739845\n",
            "step 3000 | loss : 0.3984528627581894\n",
            "step 4000 | loss : 0.3967796972440556\n",
            "step 5000 | loss : 0.3961481820516288\n",
            "step 6000 | loss : 0.39637028220482173\n",
            "step 7000 | loss : 0.3957953624310238\n",
            "step 8000 | loss : 0.3952372150197625\n",
            "step 9000 | loss : 0.39458400309955083\n",
            "step 10000 | loss : 0.39408194343037906\n",
            "idx: 14/50 cost: 0.394 train_acc: 0.950 test_acc: 0.897\n",
            "step 1000 | loss : 0.3911987209394574\n",
            "step 2000 | loss : 0.3893784348629415\n",
            "step 3000 | loss : 0.3890816401888927\n",
            "step 4000 | loss : 0.38924316681083293\n",
            "step 5000 | loss : 0.38821542937979103\n",
            "step 6000 | loss : 0.38728178608479596\n",
            "step 7000 | loss : 0.3870667287627501\n",
            "step 8000 | loss : 0.38698985864268615\n",
            "step 9000 | loss : 0.38641429863497617\n",
            "step 10000 | loss : 0.3862193604767323\n",
            "idx: 15/50 cost: 0.386 train_acc: 0.920 test_acc: 0.899\n",
            "step 1000 | loss : 0.3811906676217914\n",
            "step 2000 | loss : 0.38067400084063413\n",
            "step 3000 | loss : 0.38111695439368487\n",
            "step 4000 | loss : 0.380052261935547\n",
            "step 5000 | loss : 0.37977978092581033\n",
            "step 6000 | loss : 0.3800754191055894\n",
            "step 7000 | loss : 0.3796200273303049\n",
            "step 8000 | loss : 0.3787851153127849\n",
            "step 9000 | loss : 0.37857549496160614\n",
            "step 10000 | loss : 0.37829770516902206\n",
            "idx: 16/50 cost: 0.378 train_acc: 0.900 test_acc: 0.899\n",
            "step 1000 | loss : 0.3781251435726881\n",
            "step 2000 | loss : 0.37306614926457404\n",
            "step 3000 | loss : 0.3741214737718304\n",
            "step 4000 | loss : 0.37376423784717916\n",
            "step 5000 | loss : 0.37332558588385584\n",
            "step 6000 | loss : 0.37292742770910264\n",
            "step 7000 | loss : 0.3725409533147301\n",
            "step 8000 | loss : 0.3724449999765493\n",
            "step 9000 | loss : 0.37201808887678717\n",
            "step 10000 | loss : 0.37165974751971664\n",
            "idx: 17/50 cost: 0.372 train_acc: 0.910 test_acc: 0.900\n",
            "step 1000 | loss : 0.36974316779896615\n",
            "step 2000 | loss : 0.3684472310785204\n",
            "step 3000 | loss : 0.36815536390369136\n",
            "step 4000 | loss : 0.36726007315795867\n",
            "step 5000 | loss : 0.3671237877063453\n",
            "step 6000 | loss : 0.3669172795222451\n",
            "step 7000 | loss : 0.36661888891724603\n",
            "step 8000 | loss : 0.3665661177453585\n",
            "step 9000 | loss : 0.3662233400266204\n",
            "step 10000 | loss : 0.3658145735386759\n",
            "idx: 18/50 cost: 0.366 train_acc: 0.920 test_acc: 0.902\n",
            "step 1000 | loss : 0.3602122033238411\n",
            "step 2000 | loss : 0.36164937997795643\n",
            "step 3000 | loss : 0.3610031435576578\n",
            "step 4000 | loss : 0.3616792835490778\n",
            "step 5000 | loss : 0.3608762247361243\n",
            "step 6000 | loss : 0.36081691402879856\n",
            "step 7000 | loss : 0.36065611262246966\n",
            "step 8000 | loss : 0.36037914723856374\n",
            "step 9000 | loss : 0.3604024006852673\n",
            "step 10000 | loss : 0.3600665879871696\n",
            "idx: 19/50 cost: 0.360 train_acc: 0.870 test_acc: 0.903\n",
            "step 1000 | loss : 0.35313317869603633\n",
            "step 2000 | loss : 0.3557322392873466\n",
            "step 3000 | loss : 0.35635809434205296\n",
            "step 4000 | loss : 0.35557188231870535\n",
            "step 5000 | loss : 0.3556907615751028\n",
            "step 6000 | loss : 0.3554456974801918\n",
            "step 7000 | loss : 0.35491532158745187\n",
            "step 8000 | loss : 0.355077430258505\n",
            "step 9000 | loss : 0.35501425642189055\n",
            "step 10000 | loss : 0.3545377582266927\n",
            "idx: 20/50 cost: 0.355 train_acc: 0.920 test_acc: 0.904\n",
            "step 1000 | loss : 0.34993883249536156\n",
            "step 2000 | loss : 0.35211876493133604\n",
            "step 3000 | loss : 0.351707148656249\n",
            "step 4000 | loss : 0.3517782224882394\n",
            "step 5000 | loss : 0.35103192116469145\n",
            "step 6000 | loss : 0.35091474134040374\n",
            "step 7000 | loss : 0.3507029717506043\n",
            "step 8000 | loss : 0.350662950606551\n",
            "step 9000 | loss : 0.3502751892428431\n",
            "step 10000 | loss : 0.3499820783559233\n",
            "idx: 21/50 cost: 0.350 train_acc: 0.920 test_acc: 0.905\n",
            "step 1000 | loss : 0.3473909239768982\n",
            "step 2000 | loss : 0.3476645328439772\n",
            "step 3000 | loss : 0.34695746558656293\n",
            "step 4000 | loss : 0.3473543837890029\n",
            "step 5000 | loss : 0.3467521587342024\n",
            "step 6000 | loss : 0.346433793493857\n",
            "step 7000 | loss : 0.3461944196128419\n",
            "step 8000 | loss : 0.34579195476509633\n",
            "step 9000 | loss : 0.3456185618100895\n",
            "step 10000 | loss : 0.3455910424374044\n",
            "idx: 22/50 cost: 0.346 train_acc: 0.880 test_acc: 0.906\n",
            "step 1000 | loss : 0.34126690431684253\n",
            "step 2000 | loss : 0.34270452602580187\n",
            "step 3000 | loss : 0.3424606363152464\n",
            "step 4000 | loss : 0.3422832577433437\n",
            "step 5000 | loss : 0.3423670834735036\n",
            "step 6000 | loss : 0.3420654838060339\n",
            "step 7000 | loss : 0.3419916735525642\n",
            "step 8000 | loss : 0.34141153057757767\n",
            "step 9000 | loss : 0.3416380320042372\n",
            "step 10000 | loss : 0.3413733504027128\n",
            "idx: 23/50 cost: 0.341 train_acc: 0.890 test_acc: 0.907\n",
            "step 1000 | loss : 0.3390741997435689\n",
            "step 2000 | loss : 0.33849685050919653\n",
            "step 3000 | loss : 0.33901387418061496\n",
            "step 4000 | loss : 0.33861709606461227\n",
            "step 5000 | loss : 0.3385293044343591\n",
            "step 6000 | loss : 0.3380746977707992\n",
            "step 7000 | loss : 0.3382130363493093\n",
            "step 8000 | loss : 0.33817529727192597\n",
            "step 9000 | loss : 0.3377288877968159\n",
            "step 10000 | loss : 0.3372992031764239\n",
            "idx: 24/50 cost: 0.337 train_acc: 0.920 test_acc: 0.907\n",
            "step 1000 | loss : 0.33912625401467084\n",
            "step 2000 | loss : 0.33618736956641077\n",
            "step 3000 | loss : 0.33541661077116924\n",
            "step 4000 | loss : 0.3357442759629339\n",
            "step 5000 | loss : 0.3347409921780229\n",
            "step 6000 | loss : 0.33508174378673233\n",
            "step 7000 | loss : 0.3344738795555064\n",
            "step 8000 | loss : 0.3344916791571304\n",
            "step 9000 | loss : 0.3341968900412321\n",
            "step 10000 | loss : 0.3340086755946279\n",
            "idx: 25/50 cost: 0.334 train_acc: 0.970 test_acc: 0.908\n",
            "step 1000 | loss : 0.334136026032269\n",
            "step 2000 | loss : 0.3316949574127793\n",
            "step 3000 | loss : 0.331666172593832\n",
            "step 4000 | loss : 0.3313094678297639\n",
            "step 5000 | loss : 0.3313235233768821\n",
            "step 6000 | loss : 0.3307625319771469\n",
            "step 7000 | loss : 0.3308987776722227\n",
            "step 8000 | loss : 0.3308888658760116\n",
            "step 9000 | loss : 0.3307661975050966\n",
            "step 10000 | loss : 0.33041116555854677\n",
            "idx: 26/50 cost: 0.330 train_acc: 0.930 test_acc: 0.909\n",
            "step 1000 | loss : 0.32934915299713613\n",
            "step 2000 | loss : 0.3296176353413612\n",
            "step 3000 | loss : 0.327970549279203\n",
            "step 4000 | loss : 0.3282660885537043\n",
            "step 5000 | loss : 0.3283008992053568\n",
            "step 6000 | loss : 0.3277444340356936\n",
            "step 7000 | loss : 0.32795869932749444\n",
            "step 8000 | loss : 0.3275817460557446\n",
            "step 9000 | loss : 0.32739104611840514\n",
            "step 10000 | loss : 0.32729353387355803\n",
            "idx: 27/50 cost: 0.327 train_acc: 0.850 test_acc: 0.909\n",
            "step 1000 | loss : 0.3241447748541832\n",
            "step 2000 | loss : 0.3238930666409433\n",
            "step 3000 | loss : 0.32567513320595026\n",
            "step 4000 | loss : 0.3248500227164477\n",
            "step 5000 | loss : 0.32495074652284384\n",
            "step 6000 | loss : 0.32462832561632\n",
            "step 7000 | loss : 0.3242738217094115\n",
            "step 8000 | loss : 0.32445684665441515\n",
            "step 9000 | loss : 0.32428323695560296\n",
            "step 10000 | loss : 0.3241761238873005\n",
            "idx: 28/50 cost: 0.324 train_acc: 0.890 test_acc: 0.909\n",
            "step 1000 | loss : 0.3206210780292749\n",
            "step 2000 | loss : 0.3222495469227433\n",
            "step 3000 | loss : 0.322908149463435\n",
            "step 4000 | loss : 0.322338984426111\n",
            "step 5000 | loss : 0.322272395645082\n",
            "step 6000 | loss : 0.32195705981428424\n",
            "step 7000 | loss : 0.3222086369640061\n",
            "step 8000 | loss : 0.32171224578656255\n",
            "step 9000 | loss : 0.3214879866010613\n",
            "step 10000 | loss : 0.3216071023017168\n",
            "idx: 29/50 cost: 0.322 train_acc: 0.910 test_acc: 0.909\n",
            "step 1000 | loss : 0.3180610479563475\n",
            "step 2000 | loss : 0.31769813445955514\n",
            "step 3000 | loss : 0.31865893239279586\n",
            "step 4000 | loss : 0.318909876300022\n",
            "step 5000 | loss : 0.3196665898963809\n",
            "step 6000 | loss : 0.3188811667325596\n",
            "step 7000 | loss : 0.3187394329831004\n",
            "step 8000 | loss : 0.3187743948390707\n",
            "step 9000 | loss : 0.318223518093427\n",
            "step 10000 | loss : 0.3185142118401825\n",
            "idx: 30/50 cost: 0.319 train_acc: 0.940 test_acc: 0.910\n",
            "step 1000 | loss : 0.31427927426248786\n",
            "step 2000 | loss : 0.31654178228601815\n",
            "step 3000 | loss : 0.31668383117516835\n",
            "step 4000 | loss : 0.3166951956637204\n",
            "step 5000 | loss : 0.31607300492823126\n",
            "step 6000 | loss : 0.31622455738919475\n",
            "step 7000 | loss : 0.31635548710131217\n",
            "step 8000 | loss : 0.31623899212433026\n",
            "step 9000 | loss : 0.31627977408303154\n",
            "step 10000 | loss : 0.3160450410440564\n",
            "idx: 31/50 cost: 0.316 train_acc: 0.900 test_acc: 0.910\n",
            "step 1000 | loss : 0.3129976449161768\n",
            "step 2000 | loss : 0.31381814409792425\n",
            "step 3000 | loss : 0.3143623893037438\n",
            "step 4000 | loss : 0.3144621148686856\n",
            "step 5000 | loss : 0.31434484702050686\n",
            "step 6000 | loss : 0.3143000205159187\n",
            "step 7000 | loss : 0.3136924349623067\n",
            "step 8000 | loss : 0.31384430987155065\n",
            "step 9000 | loss : 0.3137542363649441\n",
            "step 10000 | loss : 0.3135069458466023\n",
            "idx: 32/50 cost: 0.314 train_acc: 0.900 test_acc: 0.911\n",
            "step 1000 | loss : 0.3115739814564586\n",
            "step 2000 | loss : 0.31341051021963356\n",
            "step 3000 | loss : 0.3125546899785598\n",
            "step 4000 | loss : 0.31236215857230126\n",
            "step 5000 | loss : 0.31225812437534334\n",
            "step 6000 | loss : 0.3118180511891842\n",
            "step 7000 | loss : 0.3118006849406021\n",
            "step 8000 | loss : 0.3114864816153422\n",
            "step 9000 | loss : 0.31178357837183607\n",
            "step 10000 | loss : 0.3114578580535948\n",
            "idx: 33/50 cost: 0.311 train_acc: 0.960 test_acc: 0.911\n",
            "step 1000 | loss : 0.30761253360658886\n",
            "step 2000 | loss : 0.309266063708812\n",
            "step 3000 | loss : 0.310831697533528\n",
            "step 4000 | loss : 0.30962058215681465\n",
            "step 5000 | loss : 0.30962653319835665\n",
            "step 6000 | loss : 0.30972171274448435\n",
            "step 7000 | loss : 0.30952717675800834\n",
            "step 8000 | loss : 0.3092030109902844\n",
            "step 9000 | loss : 0.3091691577873296\n",
            "step 10000 | loss : 0.309362382683903\n",
            "idx: 34/50 cost: 0.309 train_acc: 0.940 test_acc: 0.911\n",
            "step 1000 | loss : 0.3058364065475762\n",
            "step 2000 | loss : 0.30744059398584067\n",
            "step 3000 | loss : 0.3063676602567236\n",
            "step 4000 | loss : 0.307663984335959\n",
            "step 5000 | loss : 0.30703453358113764\n",
            "step 6000 | loss : 0.30708895504412553\n",
            "step 7000 | loss : 0.3072085177483303\n",
            "step 8000 | loss : 0.3069478747304529\n",
            "step 9000 | loss : 0.3068572638730208\n",
            "step 10000 | loss : 0.3068539608083665\n",
            "idx: 35/50 cost: 0.307 train_acc: 0.940 test_acc: 0.912\n",
            "step 1000 | loss : 0.30629953752458094\n",
            "step 2000 | loss : 0.3056991212144494\n",
            "step 3000 | loss : 0.30542960397402447\n",
            "step 4000 | loss : 0.3050309466170147\n",
            "step 5000 | loss : 0.3058137031979859\n",
            "step 6000 | loss : 0.30563669608347116\n",
            "step 7000 | loss : 0.30536148815229536\n",
            "step 8000 | loss : 0.3051923984005116\n",
            "step 9000 | loss : 0.3048068677447736\n",
            "step 10000 | loss : 0.30498361640013755\n",
            "idx: 36/50 cost: 0.305 train_acc: 0.920 test_acc: 0.912\n",
            "step 1000 | loss : 0.30371389135718346\n",
            "step 2000 | loss : 0.30493511981517074\n",
            "step 3000 | loss : 0.30396811102082333\n",
            "step 4000 | loss : 0.30425997541286054\n",
            "step 5000 | loss : 0.3039391027107835\n",
            "step 6000 | loss : 0.3036165923445175\n",
            "step 7000 | loss : 0.30358773040824705\n",
            "step 8000 | loss : 0.30351954109827056\n",
            "step 9000 | loss : 0.3032890279127492\n",
            "step 10000 | loss : 0.3032377455748618\n",
            "idx: 37/50 cost: 0.303 train_acc: 0.920 test_acc: 0.913\n",
            "step 1000 | loss : 0.3043753984719515\n",
            "step 2000 | loss : 0.30161655516549946\n",
            "step 3000 | loss : 0.30180370712031923\n",
            "step 4000 | loss : 0.3016488793194294\n",
            "step 5000 | loss : 0.3013683914452791\n",
            "step 6000 | loss : 0.30188081950197615\n",
            "step 7000 | loss : 0.30164911939576267\n",
            "step 8000 | loss : 0.3011657268390991\n",
            "step 9000 | loss : 0.3012737590550548\n",
            "step 10000 | loss : 0.30124146429263055\n",
            "idx: 38/50 cost: 0.301 train_acc: 0.910 test_acc: 0.913\n",
            "step 1000 | loss : 0.30170738331228497\n",
            "step 2000 | loss : 0.30028732811659575\n",
            "step 3000 | loss : 0.3001843882401784\n",
            "step 4000 | loss : 0.30042201942112295\n",
            "step 5000 | loss : 0.29991088753789663\n",
            "step 6000 | loss : 0.2998363113539914\n",
            "step 7000 | loss : 0.29979163952278237\n",
            "step 8000 | loss : 0.2999827728448436\n",
            "step 9000 | loss : 0.29971276115460527\n",
            "step 10000 | loss : 0.2994662718966603\n",
            "idx: 39/50 cost: 0.299 train_acc: 0.910 test_acc: 0.913\n",
            "step 1000 | loss : 0.3007393763139844\n",
            "step 2000 | loss : 0.2987182178441435\n",
            "step 3000 | loss : 0.29881275607272983\n",
            "step 4000 | loss : 0.29896448106411844\n",
            "step 5000 | loss : 0.2985946815736592\n",
            "step 6000 | loss : 0.29824592275979617\n",
            "step 7000 | loss : 0.29793400096414346\n",
            "step 8000 | loss : 0.29783581459661945\n",
            "step 9000 | loss : 0.2979720427571899\n",
            "step 10000 | loss : 0.29778706106506286\n",
            "idx: 40/50 cost: 0.298 train_acc: 0.910 test_acc: 0.913\n",
            "step 1000 | loss : 0.29744009125977755\n",
            "step 2000 | loss : 0.2964702267628163\n",
            "step 3000 | loss : 0.2973324889453749\n",
            "step 4000 | loss : 0.2972925908975303\n",
            "step 5000 | loss : 0.29675962538719175\n",
            "step 6000 | loss : 0.29692657399984695\n",
            "step 7000 | loss : 0.29665127464224184\n",
            "step 8000 | loss : 0.29668985175108537\n",
            "step 9000 | loss : 0.2967377550109393\n",
            "step 10000 | loss : 0.2962032085984945\n",
            "idx: 41/50 cost: 0.296 train_acc: 0.940 test_acc: 0.913\n",
            "step 1000 | loss : 0.29775264900922777\n",
            "step 2000 | loss : 0.2959462646283209\n",
            "step 3000 | loss : 0.29542105878144503\n",
            "step 4000 | loss : 0.2958737967181951\n",
            "step 5000 | loss : 0.2956407960101962\n",
            "step 6000 | loss : 0.29535472169394295\n",
            "step 7000 | loss : 0.29492559712593047\n",
            "step 8000 | loss : 0.2950030023790896\n",
            "step 9000 | loss : 0.29515362783190274\n",
            "step 10000 | loss : 0.29487607568092644\n",
            "idx: 42/50 cost: 0.295 train_acc: 0.940 test_acc: 0.913\n",
            "step 1000 | loss : 0.29168982668221\n",
            "step 2000 | loss : 0.2936744735427201\n",
            "step 3000 | loss : 0.2934941453995804\n",
            "step 4000 | loss : 0.29358045109082015\n",
            "step 5000 | loss : 0.2939361280851066\n",
            "step 6000 | loss : 0.29337607257502774\n",
            "step 7000 | loss : 0.29344460002066836\n",
            "step 8000 | loss : 0.29322899312665685\n",
            "step 9000 | loss : 0.2932486689045197\n",
            "step 10000 | loss : 0.2932344501953572\n",
            "idx: 43/50 cost: 0.293 train_acc: 0.930 test_acc: 0.913\n",
            "step 1000 | loss : 0.2917270373031497\n",
            "step 2000 | loss : 0.2911856353804469\n",
            "step 3000 | loss : 0.29197692646086215\n",
            "step 4000 | loss : 0.29216193919070066\n",
            "step 5000 | loss : 0.2920150714337826\n",
            "step 6000 | loss : 0.29177536725997927\n",
            "step 7000 | loss : 0.29191263464199646\n",
            "step 8000 | loss : 0.29191614425834267\n",
            "step 9000 | loss : 0.29172515004459354\n",
            "step 10000 | loss : 0.2916634477093816\n",
            "idx: 44/50 cost: 0.292 train_acc: 0.920 test_acc: 0.913\n",
            "step 1000 | loss : 0.2911944927945733\n",
            "step 2000 | loss : 0.2911266794987023\n",
            "step 3000 | loss : 0.2910981360574563\n",
            "step 4000 | loss : 0.2911110383924097\n",
            "step 5000 | loss : 0.29097444911152126\n",
            "step 6000 | loss : 0.29053519035379094\n",
            "step 7000 | loss : 0.29047671610329834\n",
            "step 8000 | loss : 0.2902180751413107\n",
            "step 9000 | loss : 0.290419928021729\n",
            "step 10000 | loss : 0.29042402461990713\n",
            "idx: 45/50 cost: 0.290 train_acc: 0.900 test_acc: 0.913\n",
            "step 1000 | loss : 0.2891585205495357\n",
            "step 2000 | loss : 0.28976027946919203\n",
            "step 3000 | loss : 0.28884841335068145\n",
            "step 4000 | loss : 0.28997254665382205\n",
            "step 5000 | loss : 0.2889183019772172\n",
            "step 6000 | loss : 0.28931546054656304\n",
            "step 7000 | loss : 0.289168138127774\n",
            "step 8000 | loss : 0.288993539578747\n",
            "step 9000 | loss : 0.28879925477628904\n",
            "step 10000 | loss : 0.28921584524698557\n",
            "idx: 46/50 cost: 0.289 train_acc: 0.930 test_acc: 0.913\n",
            "step 1000 | loss : 0.2876316787078977\n",
            "step 2000 | loss : 0.28712737156450746\n",
            "step 3000 | loss : 0.2870828876743714\n",
            "step 4000 | loss : 0.28768056887760757\n",
            "step 5000 | loss : 0.28787930316478016\n",
            "step 6000 | loss : 0.2874123014410337\n",
            "step 7000 | loss : 0.28749637060186695\n",
            "step 8000 | loss : 0.2875761720854789\n",
            "step 9000 | loss : 0.28753790933473244\n",
            "step 10000 | loss : 0.2875240806929767\n",
            "idx: 47/50 cost: 0.288 train_acc: 0.910 test_acc: 0.914\n",
            "step 1000 | loss : 0.2873026791214943\n",
            "step 2000 | loss : 0.28720680809021\n",
            "step 3000 | loss : 0.286919653378427\n",
            "step 4000 | loss : 0.2868304766956717\n",
            "step 5000 | loss : 0.2868938482940197\n",
            "step 6000 | loss : 0.2869979159291834\n",
            "step 7000 | loss : 0.28689013896190696\n",
            "step 8000 | loss : 0.28646420094231145\n",
            "step 9000 | loss : 0.28647244480873146\n",
            "step 10000 | loss : 0.28658379804119466\n",
            "idx: 48/50 cost: 0.287 train_acc: 0.940 test_acc: 0.914\n",
            "step 1000 | loss : 0.28500404758006337\n",
            "step 2000 | loss : 0.28591033378988506\n",
            "step 3000 | loss : 0.28466247203946116\n",
            "step 4000 | loss : 0.28542812885530294\n",
            "step 5000 | loss : 0.2854095865949988\n",
            "step 6000 | loss : 0.2853428672396888\n",
            "step 7000 | loss : 0.28538929554128223\n",
            "step 8000 | loss : 0.28523718550475313\n",
            "step 9000 | loss : 0.2852427716093759\n",
            "step 10000 | loss : 0.28527595041319725\n",
            "idx: 49/50 cost: 0.285 train_acc: 0.940 test_acc: 0.914\n",
            "step 1000 | loss : 0.28190067879110575\n",
            "step 2000 | loss : 0.28500429635867475\n",
            "step 3000 | loss : 0.2841952718322476\n",
            "step 4000 | loss : 0.2841713237185031\n",
            "step 5000 | loss : 0.28436914432942867\n",
            "step 6000 | loss : 0.2841980723602076\n",
            "step 7000 | loss : 0.28408868829905987\n",
            "step 8000 | loss : 0.2840374082908966\n",
            "step 9000 | loss : 0.28424165338070856\n",
            "step 10000 | loss : 0.2840657367225736\n",
            "idx: 50/50 cost: 0.284 train_acc: 0.910 test_acc: 0.914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1t1SOx0265I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 계속 훈련시키면 트레이닝 오버피팅된다 -> test_acc 에 대해서 성능이 떨어지는 순간 => 오버피팅\n",
        "\n",
        "# 보통\n",
        "# 0, 1일때는 시그모이드\n",
        "# 멀티분류는 소프트맥스 쓴다 "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}