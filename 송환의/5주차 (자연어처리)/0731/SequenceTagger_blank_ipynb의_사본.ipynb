{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTagger_blank.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixmf_AX6AeR5",
        "colab_type": "text"
      },
      "source": [
        "## 1. CoNLL-2003 Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NZW2Ayi8qMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "05586fa3-8b37-43c4-e4a2-931a84265553"
      },
      "source": [
        "!wget -O CoNLL-2003.zip https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
        "!mkdir CoNLL-2003\n",
        "!unzip CoNLL-2003.zip -d CoNLL-2003\n",
        "!rm CoNLL-2003.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-31 05:33:03--  https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip [following]\n",
            "--2019-07-31 05:33:04--  https://www.dropbox.com/s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline/AltLpXRVKKytvdmPhr-afmNMf1WXcwANlzu2RdBokn0HgAOViasvMFIz7MaU1CNHQ4DssJld3Uz3Csw6jMvSMveFbnjDfDccLzCZaxe6FEbN1g/file# [following]\n",
            "--2019-07-31 05:33:04--  https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline/AltLpXRVKKytvdmPhr-afmNMf1WXcwANlzu2RdBokn0HgAOViasvMFIz7MaU1CNHQ4DssJld3Uz3Csw6jMvSMveFbnjDfDccLzCZaxe6FEbN1g/file\n",
            "Resolving uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com (uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com (uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AlvV5PP0a9tzEn5xGNmz2lAwtEdu3mE21qPPb9yOgN6PGXzOYvREiRHn3yjYK7AC9h5ug0WZzSY6zmegYFhuUPy2M__FgcUhRUQVnFDCA5OBDXU-aT47niehBZ92Y1R3gA9330b-pf8iF0cgHl3iKLcSEMrNtQVpSa4BAyG-uBGG2-4edQvMuvOZ-aSGZuFiJItGwHRSdBlWaPd_8485I4CwKfYOtmehKbXPEM7lDB8z9acDbKMCy-F5vI6y69qPDMZQ7mWaX7MJAfumx--mrCIPOdeIJ8WQWSkFkPJDssJClo3Z70JcYQaJvexs04vsNn_I1_CjAupX3HE5e8wJ6lzT/file [following]\n",
            "--2019-07-31 05:33:04--  https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline2/AlvV5PP0a9tzEn5xGNmz2lAwtEdu3mE21qPPb9yOgN6PGXzOYvREiRHn3yjYK7AC9h5ug0WZzSY6zmegYFhuUPy2M__FgcUhRUQVnFDCA5OBDXU-aT47niehBZ92Y1R3gA9330b-pf8iF0cgHl3iKLcSEMrNtQVpSa4BAyG-uBGG2-4edQvMuvOZ-aSGZuFiJItGwHRSdBlWaPd_8485I4CwKfYOtmehKbXPEM7lDB8z9acDbKMCy-F5vI6y69qPDMZQ7mWaX7MJAfumx--mrCIPOdeIJ8WQWSkFkPJDssJClo3Z70JcYQaJvexs04vsNn_I1_CjAupX3HE5e8wJ6lzT/file\n",
            "Reusing existing connection to uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691339 (675K) [application/zip]\n",
            "Saving to: ‘CoNLL-2003.zip’\n",
            "\n",
            "CoNLL-2003.zip      100%[===================>] 675.14K  2.29MB/s    in 0.3s    \n",
            "\n",
            "2019-07-31 05:33:05 (2.29 MB/s) - ‘CoNLL-2003.zip’ saved [691339/691339]\n",
            "\n",
            "Archive:  CoNLL-2003.zip\n",
            "  inflating: CoNLL-2003/label.vocab  \n",
            "  inflating: CoNLL-2003/sample.inputs  \n",
            "  inflating: CoNLL-2003/sample.labels  \n",
            "  inflating: CoNLL-2003/sample.vocab  \n",
            "  inflating: CoNLL-2003/test.inputs  \n",
            "  inflating: CoNLL-2003/test.labels  \n",
            "  inflating: CoNLL-2003/train.inputs  \n",
            "  inflating: CoNLL-2003/train.labels  \n",
            "  inflating: CoNLL-2003/train.vocab  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91llDkIA95Z",
        "colab_type": "text"
      },
      "source": [
        "## 2. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XstaB1Wa6Obr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "883d750f-f642-45ea-c743-f746bc34d543"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import tqdm\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_4JKAp8BKVA",
        "colab_type": "text"
      },
      "source": [
        "## 3. Logger Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKMt1HGt_Tel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_logger(path:str):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    debug_fh = logging.FileHandler(os.path.join(path, \"debug.log\"))\n",
        "    debug_fh.setLevel(logging.DEBUG)\n",
        "\n",
        "    info_fh = logging.FileHandler(os.path.join(path, \"info.log\"))\n",
        "    info_fh.setLevel(logging.INFO)\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "\n",
        "    info_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')\n",
        "    debug_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s | %(lineno)d:%(funcName)s')\n",
        "\n",
        "    ch.setFormatter(info_formatter)\n",
        "    info_fh.setFormatter(info_formatter)\n",
        "    debug_fh.setFormatter(debug_formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(debug_fh)\n",
        "    logger.addHandler(info_fh)\n",
        "\n",
        "    return logger\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZedCFnABXYA",
        "colab_type": "text"
      },
      "source": [
        "## 4. Hyperparameters Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX0vqhpz-N_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams_dict = {\n",
        "  \"root_dir\": \"out_dirs/KoreaUniv_Data/TEST/\",\n",
        "  \"vocab_size\": 10000,\n",
        "  \"num_epochs\": 10,\n",
        "  \"batch_size\": 16,\n",
        "  \"embedding_dim\": 100,\n",
        "  \"rnn_hidden_dim\": 128,\n",
        "  \"rnn_depth\": 3,\n",
        "  \"dropout_keep_prob\": 1.0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQw791j-lDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4ec89dd4-565c-465e-da42-97e0e3fe9e18"
      },
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "root_dir = os.path.join(hparams_dict[\"root_dir\"], \"%s/\" % timestamp)\n",
        "logger = init_logger(root_dir)\n",
        "logger.info(\"Hyper-parameters: %s\" %str(hparams_dict))\n",
        "hparams_dict[\"root_dir\"] = root_dir\n",
        "hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "data_dir = \"./CoNLL-2003\"\n",
        "dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n",
        "logger = logging.getLogger(__name__)\n",
        "iterator_initializers = []"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 05:36:19,326 | INFO     | Hyper-parameters: {'root_dir': 'out_dirs/KoreaUniv_Data/TEST/', 'vocab_size': 10000, 'num_epochs': 10, 'batch_size': 16, 'embedding_dim': 100, 'rnn_hidden_dim': 128, 'rnn_depth': 3, 'dropout_keep_prob': 1.0}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Pw09XzBl40",
        "colab_type": "text"
      },
      "source": [
        "## 5. Make Vocab Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7151biR-lcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_vocab_table():\n",
        "    \"\"\"\n",
        "    [A]\n",
        "    Vocabulary(단어집) 파일을 로드합니다.\n",
        "    단어 -> id, id -> 단어 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n",
        "      vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "  \n",
        "    if len(vocab) > hparams.vocab_size:\n",
        "      vocab = vocab[:hparams.vocab_size]\n",
        "\n",
        "    id2word = vocab\n",
        "    word2id = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "      word2id[word] = i\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [B]\n",
        "    Label(태그 모음) 파일을 로드합니다.\n",
        "    태그 -> id, id -> 태그 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n",
        "      labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "      labels.insert(0, \"PAD\")\n",
        "      id2label = labels\n",
        "      label2id = {}\n",
        "      for i, label in enumerate(labels):\n",
        "          label2id[label] = i\n",
        "\n",
        "    return (id2word, word2id), (id2label, label2id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regd4vxPBnNA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Build Graph (Sequence Tagger Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyr0hDR8_p9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(inputs:tf.Tensor, lengths:tf.Tensor, id2word, id2label):\n",
        "      print(\"Building graph for model: sequence tagger\")\n",
        "\n",
        "      \"\"\"\n",
        "      [C]\n",
        "      단어 임베딩 행렬을 생성합니다.\n",
        "      단어 id를 단어 임베딩 텐서로 변환합니다.\n",
        "      \"\"\"\n",
        "      # Number of possible output categories.\n",
        "      output_dim = len(id2label)\n",
        "      vocab_size = len(id2word) + 1\n",
        "      embeddings = tf.get_variable(\n",
        "          \"embeddings\",\n",
        "          shape=[vocab_size, hparams.embedding_dim],\n",
        "          initializer=tf.initializers.variance_scaling(\n",
        "              scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n",
        "      )\n",
        "      embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "      # shape = [batch_size, time, embed_dim]\n",
        "      layer_out = embedded\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [D]\n",
        "      단어 임베딩을 RNN의 입력으로 사용하기 전,\n",
        "      차원 수를 맞춰주고 성능을 향상시키기 위해\n",
        "      projection layer를 생성하여 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "    \n",
        "      with tf.variable_scope(\"read-out\"):\n",
        "        prev_layer_size = layer_out.get_shape().as_list()[1]\n",
        "        weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim],\n",
        "                                 initializer=tf.initializers.variance_scaling(\n",
        "                                     scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n",
        "                                 ))\n",
        "        bias = tf.get_variable(\"bias\", shape=[output_dim],\n",
        "                               initializer=tf.initializers.zeros())\n",
        "        predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n",
        "\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [E]\n",
        "      양방향 RNN을 생성하고, 여기에 텐서를 통과시킵니다.\n",
        "      이렇게 하여, 단어간 의존 관계가 반영된 단어 자질 텐서를 얻습니다.\n",
        "      \"\"\"\n",
        "\n",
        "      with tf.variable_scope(\"bi-RNN\"):\n",
        "          # Build RNN layers\n",
        "          rnn_cell_forward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "          rnn_cell_backward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "\n",
        "          # Apply dropout to RNN\n",
        "          if hparams.dropout_keep_prob < 1.0:\n",
        "              rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward, output_keep_prob=dropout_keep_prob_ph)\n",
        "              rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward, output_keep_prob=dropout_keep_prob_ph)\n",
        "\n",
        "          # Stack multiple layers of RNN\n",
        "          # rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * hparams.rnn_depth)\n",
        "          # rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * hparams.rnn_depth)\n",
        "\n",
        "          (output_forward, output_backward), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "              rnn_cell_forward, rnn_cell_backward,\n",
        "              inputs=layer_out,\n",
        "              sequence_length=lengths,\n",
        "              dtype=tf.float32\n",
        "          )\n",
        "          hiddens = tf.concat([output_forward, output_backward], axis=-1)\n",
        "          # shape = [batch_size, time, rnn_dim*2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [F]\n",
        "      마스킹을 적용하여 문장 길이를 통일하기 위해 적용했던 padding을 제거합니다.\n",
        "      \"\"\"\n",
        "\n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, hparams.rnn_hidden_dim * 2])\n",
        "      layer_out = bi_lstm_out  # shape=[sum of seq length, 2*LSTM hidden layer size]\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [G]\n",
        "      단어 자질 텐서를 바탕으로 단어의 태그를 예측합니다.\n",
        "      이를 위해 fully-connected(dense) layer를 생성하고 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "\n",
        "      layer_out = tf.layers.dense(\n",
        "      inputs=layer_out,\n",
        "      units=hparams.rnn_hidden_dim,\n",
        "      activation=tf.nn.relu,\n",
        "      kernel_initializer=tf.initializers.variance_scaling(\n",
        "      scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n",
        "      name=\"input_projection\"\n",
        "      )\n",
        "\n",
        "      return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmF2AfNBnsp",
        "colab_type": "text"
      },
      "source": [
        "## 7. Load Data (tf.data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5TRwTMs-ljq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(id2word, word2id, id2label, label2id):\n",
        "      \"\"\"\n",
        "      [L]\n",
        "      단어->id 및 태그->id 변환 테이블을 텐서 그래프에 추가합니다.\n",
        "      \"\"\"\n",
        "  #\n",
        "      word2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "      mapping=tf.constant(id2word),\n",
        "      num_oov_buckets=1,\n",
        "      name=\"word2id\"\n",
        "      )\n",
        "\n",
        "      label2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "      mapping=tf.constant(id2label),\n",
        "      default_value=label2id[\"O\"],\n",
        "      name=\"label2id\"\n",
        "      )\n",
        "\n",
        "      \"\"\"\n",
        "      [M]\n",
        "      입력 데이터 파일을 읽어들여 이를 단어 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "      sentence = word_tokenize(sentence)\n",
        "      word_ids = []\n",
        "      (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "\n",
        "      for word in sentence:\n",
        "        \n",
        "        if word in word2id:\n",
        "            word_ids.append(word2id[word])\n",
        "            el\n",
        "        else:\n",
        "             word_ids.append(len(word2id))\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [N]\n",
        "      태그 데이터 파일을 읽어들여 이를 태그 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "      label_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.labels\"))\n",
        "      batched_label_dataset = label_dataset.batch(hparams.batch_size)\n",
        "      label_iterator = batched_label_dataset.make_initializable_iterator()\n",
        "      batch_label_str = label_iterator.get_next()\n",
        "      batch_label = tf.string_split(batch_label_str, \" \")\n",
        "      label_ids = label2id.lookup(batch_label)\n",
        "      dense_label_ids = tf.sparse_tensor_to_dense(label_ids)\n",
        "    # shape = [batch_size, time]\n",
        "\n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      dense_label_ids = tf.boolean_mask(dense_label_ids, mask)\n",
        "\n",
        "      iterator_initializers.append(input_iterator.initializer)\n",
        "      iterator_initializers.append(label_iterator.initializer)\n",
        "\n",
        "      return dense_word_ids, dense_label_ids, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XTEfi0RBoJT",
        "colab_type": "text"
      },
      "source": [
        "## 8. Train Model (session call)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjm5_0Bj-lp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cae8b6db-e1e6-422b-d646-d939a9314f85"
      },
      "source": [
        "  def train_model():\n",
        "        sess = tf.Session()\n",
        "        with sess.as_default():\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "            inputs, labels, lengths = load_data(id2word, word2id, id2label, label2id)\n",
        "\n",
        "            with tf.variable_scope(\"build_graph\", reuse=False):\n",
        "                logits = build_graph(inputs, lengths, id2word, id2label)\n",
        "\n",
        "            \"\"\"\n",
        "            [O]\n",
        "            모델을 훈련시키기 위해 필요한 오퍼레이션들을 텐서 그래프에 추가합니다.\n",
        "            여기에는 loss, train, accuracy 계산 등이 포함됩니다.\n",
        "            \"\"\"\n",
        "              try:\n",
        "    accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n",
        "        [accuracy, labels, loss_op, global_step, train_op],\n",
        "        feed_dict={dropout_keep_prob_ph: hparams.dropout_keep_prob}\n",
        "    )\n",
        "    accuracy_mean += accuracy_val\n",
        "    loss_mean += loss_val\n",
        "    idx_cnt += 1\n",
        "    if global_step_val % 50 == 0:\n",
        "        accuracy_mean /= idx_cnt\n",
        "        loss_mean /= idx_cnt\n",
        "        logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (global_step_val, loss_mean, accuracy_mean * 100))\n",
        "        accuracy_mean, loss_mean,idx_cnt = 0, 0, 0\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    # End of epoch.\n",
        "    break\n",
        "\n",
        "            # Initialize iterators, tables, and variables.\n",
        "            local_iterator_initializers = tf.group(*iterator_initializers)\n",
        "            tf.tables_initializer().run()\n",
        "            tf.global_variables_initializer().run()\n",
        "\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            for epochs_completed in range(hparams.num_epochs):\n",
        "                local_iterator_initializers.run()\n",
        "                accuracy_mean, loss_mean, idx_cnt = 0, 0, 0\n",
        "                while True:\n",
        "                    \"\"\"\n",
        "                    [P]\n",
        "                    그래프에 데이터를 입력하여 필요한 계산들을 수행하고,\n",
        "                    Loss에 따라 gradient를 계산하여 파라미터들을 업데이트합니다.\n",
        "                    이러한 과정을 training step이라고 합니다.\n",
        "                    \"\"\"\n",
        "\n",
        "                loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels,\n",
        "                                                                         name=\"cross_entropy\")\n",
        "                loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n",
        "                train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n",
        "\n",
        "                eval = tf.nn.in_top_k(logits, labels, 1)\n",
        "                correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n",
        "                accuracy = tf.divide(correct_count, tf.shape(labels)[0])\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                \"\"\"\n",
        "                [Q]\n",
        "                전체 학습 데이터에 대하여 1회 학습을 완료하였습니다.\n",
        "                이를 1 epoch라고 합니다.\n",
        "                딥러닝 모델의 학습은 일반적으로 수십~수백 epoch 동안 진행됩니다.\n",
        "                \n",
        "                \"\"\"\n",
        "                logger.info(\"End of epoch %d.\" % (epochs_completed+1))\n",
        "                save_path = saver.save(sess, \"saves/model.ckpt\", global_step=global_step_val)\n",
        "                logger.info(\"Model saved at: %s\" % save_path)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-2a565296baa8>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    try:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9dVWZEBpRv",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofY5we0ADFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the vanilla Bi-directional LSTM model\n",
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz85PSZnNS2B",
        "colab_type": "text"
      },
      "source": [
        "## Model Load and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWxSlqZd_iou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_predict(saved_file:str):\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "    \"\"\"\n",
        "    [H]\n",
        "    입력 문자열을 단어/문장부호 단위로 쪼개고, 이를 다시 단어 id로 변환합니다.\n",
        "    \"\"\"\n",
        "    ##\n",
        "  input_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.inputs\"))\n",
        "  batched_input_dataset = input_dataset.batch(hparams.batch_size)\n",
        "  input_iterator = batched_input_dataset.make_initializable_iterator()\n",
        "  batch_input = input_iterator.get_next()\n",
        "  batch_input.set_shape([hparams.batch_size])\n",
        "  words = tf.string_split(batch_input, \" \")\n",
        "  word_ids = word2id.lookup(words)\n",
        "  dense_word_ids = tf.sparse_tensor_to_dense(word_ids)\n",
        "  # shape = [batch_size, time]\n",
        "\n",
        "\n",
        "  line_number = word_ids.indices[:, 0]\n",
        "  line_position = word_ids.indices[:, 1]\n",
        "  lengths = tf.segment_max(data=line_position,\n",
        "                           segment_ids=line_number) + 1\n",
        "\n",
        "    \n",
        "   \n",
        "     \n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        \"\"\"\n",
        "        [I]\n",
        "        태깅을 수행하기 위해 텐서 그래프를 생성합니다.\n",
        "        \"\"\"\n",
        "    dense_word_ids = tf.constant(word_ids)\n",
        "    lengths = tf.constant(len(word_ids))\n",
        "    # Insert batch dimension.\n",
        "    dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n",
        "    lengths = tf.expand_dims(lengths, axis=0)\n",
        "\n",
        "    with tf.variable_scope(\"build_graph\", reuse=tf.AUTO_REUSE):\n",
        "        logits = build_graph(dense_word_ids, lengths, id2word, id2label)\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "        \"\"\"\n",
        "        [J]\n",
        "        저장된 모델을 로드하고, 데이터를 입력하여 태깅 결과를 얻습니다.\n",
        "        \"\"\"\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, saved_file)\n",
        "        pred_val = sess.run(\n",
        "            [predictions]\n",
        "        )[0]\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [K]\n",
        "    태깅 결과를 출력합니다.\n",
        "    \"\"\"\n",
        "    \n",
        "    pred_str = [id2label[i] for i in pred_val]\n",
        "    for word, tag in zip(sentence, pred_str):\n",
        "    print(\"%s[%s]\" %(word, tag), end=' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_B0m2rG-Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_and_predict(\"/content/saves/model.ckpt-1874\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}