{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split  ## 데이터 트레이닝 셋, 테스트셋 쪼갠다.\n",
    "\n",
    "import pandas as pd\n",
    "import missingno as mino\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## 검증 이슈가 중요. 내가 만든 시스템에 대한 검증 왜이렇게 나왔냐?, 모델이 얼마나 잘나왔냐\n",
    "## iris 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR) ## two wrong data points  폴드아웃 (트라이닝 테스트 벨리디이션 셋) 검증을 하기위해서 트레이닝 셋(모델만들), 테스트 셋(검증)분리\n",
    "                    ## 잘못된 데이터라도 현재 수집된 데이터는 다 정답이라는 가정을 하고 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_target = pd.DataFrame(data.target, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.concat([iris ,iris_target ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = iris.sample(frac=0.75)  ##75퍼 랜덤하게 뽑는다, 샘플링 방법이 부족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.pop() # 뮤터블 자신도 바뀌고 결과값 반환\n",
    "           # 75% 제외 나머지 \n",
    "            # 텐서플로우 공식 홈페이지\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tes_split => (x_train, X_test, ytrain y_test ) 4개 원자 언패킹해서 반환해줌 y 타켓 x 잘라서 집어넣고 test_size(33 퍼 테스트 하겠다)\n",
    "# 일정한 순서로 랜덤 돌아감 randon 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train , y_test \\\n",
    "= train_test_split(iris.iloc[:,:-1], iris.iloc[:,-1])  \n",
    "\n",
    "## 66% vs 33으로 데이터를 쪼개줌\n",
    "# data 줫도 없는데 트레인 테스트 셋 나눔 => 데이타 리퀴즈 문제(트레이닝 데이터 없는데 테스트 데이터에 나온다) data leakage\n",
    "\n",
    "## 데이터 적으면 정규분포 x 왜곡문제 => 대표성 x\n",
    "## 빅데이터 10만건이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "112                6.8               3.0                5.5               2.1\n",
       "49                 5.0               3.3                1.4               0.2\n",
       "67                 5.8               2.7                4.1               1.0\n",
       "124                6.7               3.3                5.7               2.1\n",
       "..                 ...               ...                ...               ...\n",
       "50                 7.0               3.2                4.7               1.4\n",
       "43                 5.0               3.5                1.6               0.6\n",
       "103                6.3               2.9                5.6               1.8\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "58                 6.6               2.9                4.6               1.3\n",
       "\n",
       "[112 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train\n",
    "## random state 고정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # 코딩타입이 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.0\n",
      "2 0.9821428571428571\n",
      "3 0.9642857142857143\n",
      "4 0.9642857142857143\n",
      "5 0.9642857142857143\n",
      "6 0.9464285714285714\n",
      "7 0.9642857142857143\n",
      "8 0.9642857142857143\n",
      "9 0.9821428571428571\n",
      "10 0.9732142857142857\n",
      "11 0.9732142857142857\n",
      "12 0.9553571428571429\n",
      "13 0.9821428571428571\n",
      "14 0.9642857142857143\n",
      "15 0.9732142857142857\n",
      "16 0.9642857142857143\n",
      "17 0.9732142857142857\n",
      "18 0.9642857142857143\n",
      "19 0.9732142857142857\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    knn =  KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(i, knn.score(X_train,y_train) )\n",
    "    \n",
    "    ## 2일때 성능이 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV ## cross validation 기법 사용\n",
    "\n",
    "param_grid = {'n_neighbors':range(1,100), 'leaf_size' : [10,20,30,40,50]}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid) # estimator 알고리즘 인스턴스 시킨것\n",
    "                    ## 그리드 서치 브루트 포스 하나씪 노가다 탐색\n",
    "                    ## 하이퍼파라메터 사이즈 다 찾아줌 싸이킷 쓰는 이유\n",
    "                    ## 싸이킷 파이프라이닝 알고리즘 찾아줌 모든 알고리즘 그리드 미리 모듈 만들어 놓고 import + decroator 텔레그램 \n",
    "                    ## 자동화 프로그램 실리콘밸리 , 스페인 월급루팡\n",
    "                    \n",
    "                    ## 전처리를 어느정도 해주냐 나머지는 다 자동화 cf) Auto Ai? \n",
    "                    \n",
    "\n",
    "                    ## 우리가 실제 데이터는 전처리가 큰 이슈 vs 더미랑 해도 성능이 안좋음\n",
    "                    ## 웹에서 수집하면 데이터 문자 => 숫자로 변경 (도메인별로 데이터(문자)를 숫자화(vectorization, encoding))\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': range(1, 100), 'leaf_size': [10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit( iris.iloc[:,:-1], iris.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leaf_size': 10, 'n_neighbors': 5}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.00232903</td>\n",
       "      <td>0.00232609</td>\n",
       "      <td>0.0033226</td>\n",
       "      <td>0.00365861</td>\n",
       "      <td>0.00166321</td>\n",
       "      <td>0.00166623</td>\n",
       "      <td>0.00166202</td>\n",
       "      <td>0.00166178</td>\n",
       "      <td>0.00132823</td>\n",
       "      <td>0.00199461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0019803</td>\n",
       "      <td>0.00198619</td>\n",
       "      <td>0.00197546</td>\n",
       "      <td>0.0020051</td>\n",
       "      <td>0.00199374</td>\n",
       "      <td>0.00132895</td>\n",
       "      <td>0.00164104</td>\n",
       "      <td>0.00167346</td>\n",
       "      <td>0.00199231</td>\n",
       "      <td>0.00199366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.00046873</td>\n",
       "      <td>0.000469461</td>\n",
       "      <td>0.000469516</td>\n",
       "      <td>0.000943022</td>\n",
       "      <td>0.000469854</td>\n",
       "      <td>0.00047319</td>\n",
       "      <td>0.000470359</td>\n",
       "      <td>0.00047019</td>\n",
       "      <td>0.000470699</td>\n",
       "      <td>1.36268e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.45312e-05</td>\n",
       "      <td>0.000823854</td>\n",
       "      <td>0.000836395</td>\n",
       "      <td>1.50298e-05</td>\n",
       "      <td>1.57348e-06</td>\n",
       "      <td>0.00047019</td>\n",
       "      <td>0.000457172</td>\n",
       "      <td>0.000479651</td>\n",
       "      <td>3.22152e-05</td>\n",
       "      <td>3.31004e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.00166273</td>\n",
       "      <td>0.000986814</td>\n",
       "      <td>0.00199525</td>\n",
       "      <td>0.00232697</td>\n",
       "      <td>0.00166273</td>\n",
       "      <td>0.00132608</td>\n",
       "      <td>0.00132982</td>\n",
       "      <td>0.00133006</td>\n",
       "      <td>0.00166233</td>\n",
       "      <td>0.000998656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00202107</td>\n",
       "      <td>0.00200764</td>\n",
       "      <td>0.00164962</td>\n",
       "      <td>0.0020171</td>\n",
       "      <td>0.00165669</td>\n",
       "      <td>0.00198325</td>\n",
       "      <td>0.00134158</td>\n",
       "      <td>0.00196481</td>\n",
       "      <td>0.00164978</td>\n",
       "      <td>0.00199493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000471708</td>\n",
       "      <td>1.40337e-05</td>\n",
       "      <td>8.99133e-07</td>\n",
       "      <td>0.00123974</td>\n",
       "      <td>0.000470528</td>\n",
       "      <td>0.000472573</td>\n",
       "      <td>0.000469909</td>\n",
       "      <td>0.000470584</td>\n",
       "      <td>0.000469909</td>\n",
       "      <td>1.51207e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.82085e-05</td>\n",
       "      <td>1.93637e-05</td>\n",
       "      <td>0.000461531</td>\n",
       "      <td>1.56176e-05</td>\n",
       "      <td>0.000465138</td>\n",
       "      <td>1.74218e-05</td>\n",
       "      <td>0.000484858</td>\n",
       "      <td>2.43677e-05</td>\n",
       "      <td>0.000459608</td>\n",
       "      <td>3.08733e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_leaf_size</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 1}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 2}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 3}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 4}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 5}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 6}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 7}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 8}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 9}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 10}</td>\n",
       "      <td>...</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 90}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 91}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 92}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 93}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 94}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 95}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 96}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 97}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 98}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 99}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.54902</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0.00878204</td>\n",
       "      <td>0.0159247</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00914659</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.000571662</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0191656</td>\n",
       "      <td>0.0191656</td>\n",
       "      <td>0.00990148</td>\n",
       "      <td>0.0162094</td>\n",
       "      <td>0.0244949</td>\n",
       "      <td>0.0235702</td>\n",
       "      <td>0.0235702</td>\n",
       "      <td>0.0231788</td>\n",
       "      <td>0.0226439</td>\n",
       "      <td>0.126337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>46</td>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>456</td>\n",
       "      <td>461</td>\n",
       "      <td>466</td>\n",
       "      <td>471</td>\n",
       "      <td>471</td>\n",
       "      <td>481</td>\n",
       "      <td>486</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.95098</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.568627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.963557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643296</td>\n",
       "      <td>0.639929</td>\n",
       "      <td>0.633294</td>\n",
       "      <td>0.629927</td>\n",
       "      <td>0.620024</td>\n",
       "      <td>0.606556</td>\n",
       "      <td>0.599921</td>\n",
       "      <td>0.583284</td>\n",
       "      <td>0.566647</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00932036</td>\n",
       "      <td>0.0187064</td>\n",
       "      <td>0.0125044</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.0125256</td>\n",
       "      <td>0.00776735</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00490771</td>\n",
       "      <td>0.00966642</td>\n",
       "      <td>0.00498503</td>\n",
       "      <td>0.00518181</td>\n",
       "      <td>0.00445303</td>\n",
       "      <td>0.0146477</td>\n",
       "      <td>0.00997006</td>\n",
       "      <td>0.0209138</td>\n",
       "      <td>0.0165542</td>\n",
       "      <td>0.110919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0    \\\n",
       "mean_fit_time                                0.00232903   \n",
       "std_fit_time                                 0.00046873   \n",
       "mean_score_time                              0.00166273   \n",
       "std_score_time                              0.000471708   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     1   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 1}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.921569   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.966667   \n",
       "std_test_score                                0.0333333   \n",
       "rank_test_score                                      46   \n",
       "split0_train_score                                    1   \n",
       "split1_train_score                                    1   \n",
       "split2_train_score                                    1   \n",
       "mean_train_score                                      1   \n",
       "std_train_score                                       0   \n",
       "\n",
       "                                                    1    \\\n",
       "mean_fit_time                                0.00232609   \n",
       "std_fit_time                                0.000469461   \n",
       "mean_score_time                             0.000986814   \n",
       "std_score_time                              1.40337e-05   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     2   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 2}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.941176   \n",
       "split2_test_score                              0.958333   \n",
       "mean_test_score                                0.953333   \n",
       "std_test_score                               0.00878204   \n",
       "rank_test_score                                     101   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.976728   \n",
       "std_train_score                              0.00932036   \n",
       "\n",
       "                                                    2    \\\n",
       "mean_fit_time                                 0.0033226   \n",
       "std_fit_time                                0.000469516   \n",
       "mean_score_time                              0.00199525   \n",
       "std_score_time                              8.99133e-07   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     3   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 3}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                                0.0159247   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                              0.95098   \n",
       "mean_train_score                               0.963458   \n",
       "std_train_score                               0.0187064   \n",
       "\n",
       "                                                    3    \\\n",
       "mean_fit_time                                0.00365861   \n",
       "std_fit_time                                0.000943022   \n",
       "mean_score_time                              0.00232697   \n",
       "std_score_time                               0.00123974   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     4   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 4}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.963359   \n",
       "std_train_score                               0.0125044   \n",
       "\n",
       "                                                    4    \\\n",
       "mean_fit_time                                0.00166321   \n",
       "std_fit_time                                0.000469854   \n",
       "mean_score_time                              0.00166273   \n",
       "std_score_time                              0.000470528   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     5   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 5}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.986667   \n",
       "std_test_score                               0.00914659   \n",
       "rank_test_score                                       1   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                    5    \\\n",
       "mean_fit_time                                0.00166623   \n",
       "std_fit_time                                 0.00047319   \n",
       "mean_score_time                              0.00132608   \n",
       "std_score_time                              0.000472573   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     6   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 6}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                    6    \\\n",
       "mean_fit_time                                0.00166202   \n",
       "std_fit_time                                0.000470359   \n",
       "mean_score_time                              0.00132982   \n",
       "std_score_time                              0.000469909   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     7   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 7}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.973361   \n",
       "std_train_score                               0.0125256   \n",
       "\n",
       "                                                    7    \\\n",
       "mean_fit_time                                0.00166178   \n",
       "std_fit_time                                 0.00047019   \n",
       "mean_score_time                              0.00133006   \n",
       "std_score_time                              0.000470584   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     8   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 8}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                              0.000571662   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.970093   \n",
       "std_train_score                              0.00776735   \n",
       "\n",
       "                                                    8    \\\n",
       "mean_fit_time                                0.00132823   \n",
       "std_fit_time                                0.000470699   \n",
       "mean_score_time                              0.00166233   \n",
       "std_score_time                              0.000469909   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     9   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 9}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     9    ...  \\\n",
       "mean_fit_time                                 0.00199461  ...   \n",
       "std_fit_time                                 1.36268e-06  ...   \n",
       "mean_score_time                              0.000998656  ...   \n",
       "std_score_time                               1.51207e-06  ...   \n",
       "param_leaf_size                                       10  ...   \n",
       "param_n_neighbors                                     10  ...   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 10}  ...   \n",
       "split0_test_score                               0.960784  ...   \n",
       "split1_test_score                               0.980392  ...   \n",
       "split2_test_score                               0.979167  ...   \n",
       "mean_test_score                                 0.973333  ...   \n",
       "std_test_score                                0.00902067  ...   \n",
       "rank_test_score                                       16  ...   \n",
       "split0_train_score                              0.969697  ...   \n",
       "split1_train_score                              0.979798  ...   \n",
       "split2_train_score                              0.941176  ...   \n",
       "mean_train_score                                0.963557  ...   \n",
       "std_train_score                                 0.016354  ...   \n",
       "\n",
       "                                                     485  \\\n",
       "mean_fit_time                                  0.0019803   \n",
       "std_fit_time                                 1.45312e-05   \n",
       "mean_score_time                               0.00202107   \n",
       "std_score_time                               1.82085e-05   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     90   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 90}   \n",
       "split0_test_score                               0.627451   \n",
       "split1_test_score                               0.666667   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                     0.64   \n",
       "std_test_score                                 0.0191656   \n",
       "rank_test_score                                      426   \n",
       "split0_train_score                              0.646465   \n",
       "split1_train_score                              0.636364   \n",
       "split2_train_score                              0.647059   \n",
       "mean_train_score                                0.643296   \n",
       "std_train_score                               0.00490771   \n",
       "\n",
       "                                                     486  \\\n",
       "mean_fit_time                                 0.00198619   \n",
       "std_fit_time                                 0.000823854   \n",
       "mean_score_time                               0.00200764   \n",
       "std_score_time                               1.93637e-05   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     91   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 91}   \n",
       "split0_test_score                               0.627451   \n",
       "split1_test_score                               0.666667   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                     0.64   \n",
       "std_test_score                                 0.0191656   \n",
       "rank_test_score                                      426   \n",
       "split0_train_score                              0.646465   \n",
       "split1_train_score                              0.626263   \n",
       "split2_train_score                              0.647059   \n",
       "mean_train_score                                0.639929   \n",
       "std_train_score                               0.00966642   \n",
       "\n",
       "                                                     487  \\\n",
       "mean_fit_time                                 0.00197546   \n",
       "std_fit_time                                 0.000836395   \n",
       "mean_score_time                               0.00164962   \n",
       "std_score_time                               0.000461531   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     92   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 92}   \n",
       "split0_test_score                               0.627451   \n",
       "split1_test_score                               0.647059   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                 0.633333   \n",
       "std_test_score                                0.00990148   \n",
       "rank_test_score                                      456   \n",
       "split0_train_score                              0.636364   \n",
       "split1_train_score                              0.626263   \n",
       "split2_train_score                              0.637255   \n",
       "mean_train_score                                0.633294   \n",
       "std_train_score                               0.00498503   \n",
       "\n",
       "                                                     488  \\\n",
       "mean_fit_time                                  0.0020051   \n",
       "std_fit_time                                 1.50298e-05   \n",
       "mean_score_time                                0.0020171   \n",
       "std_score_time                               1.56176e-05   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     93   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 93}   \n",
       "split0_test_score                               0.607843   \n",
       "split1_test_score                               0.647059   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                 0.626667   \n",
       "std_test_score                                 0.0162094   \n",
       "rank_test_score                                      461   \n",
       "split0_train_score                              0.626263   \n",
       "split1_train_score                              0.626263   \n",
       "split2_train_score                              0.637255   \n",
       "mean_train_score                                0.629927   \n",
       "std_train_score                               0.00518181   \n",
       "\n",
       "                                                     489  \\\n",
       "mean_fit_time                                 0.00199374   \n",
       "std_fit_time                                 1.57348e-06   \n",
       "mean_score_time                               0.00165669   \n",
       "std_score_time                               0.000465138   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     94   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 94}   \n",
       "split0_test_score                               0.588235   \n",
       "split1_test_score                               0.647059   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                     0.62   \n",
       "std_test_score                                 0.0244949   \n",
       "rank_test_score                                      466   \n",
       "split0_train_score                              0.616162   \n",
       "split1_train_score                              0.626263   \n",
       "split2_train_score                              0.617647   \n",
       "mean_train_score                                0.620024   \n",
       "std_train_score                               0.00445303   \n",
       "\n",
       "                                                     490  \\\n",
       "mean_fit_time                                 0.00132895   \n",
       "std_fit_time                                  0.00047019   \n",
       "mean_score_time                               0.00198325   \n",
       "std_score_time                               1.74218e-05   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     95   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 95}   \n",
       "split0_test_score                               0.568627   \n",
       "split1_test_score                               0.607843   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                      0.6   \n",
       "std_test_score                                 0.0235702   \n",
       "rank_test_score                                      471   \n",
       "split0_train_score                              0.585859   \n",
       "split1_train_score                              0.616162   \n",
       "split2_train_score                              0.617647   \n",
       "mean_train_score                                0.606556   \n",
       "std_train_score                                0.0146477   \n",
       "\n",
       "                                                     491  \\\n",
       "mean_fit_time                                 0.00164104   \n",
       "std_fit_time                                 0.000457172   \n",
       "mean_score_time                               0.00134158   \n",
       "std_score_time                               0.000484858   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     96   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 96}   \n",
       "split0_test_score                               0.568627   \n",
       "split1_test_score                               0.607843   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                      0.6   \n",
       "std_test_score                                 0.0235702   \n",
       "rank_test_score                                      471   \n",
       "split0_train_score                              0.585859   \n",
       "split1_train_score                              0.606061   \n",
       "split2_train_score                              0.607843   \n",
       "mean_train_score                                0.599921   \n",
       "std_train_score                               0.00997006   \n",
       "\n",
       "                                                     492  \\\n",
       "mean_fit_time                                 0.00167346   \n",
       "std_fit_time                                 0.000479651   \n",
       "mean_score_time                               0.00196481   \n",
       "std_score_time                               2.43677e-05   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     97   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 97}   \n",
       "split0_test_score                               0.568627   \n",
       "split1_test_score                               0.588235   \n",
       "split2_test_score                                  0.625   \n",
       "mean_test_score                                 0.593333   \n",
       "std_test_score                                 0.0231788   \n",
       "rank_test_score                                      481   \n",
       "split0_train_score                              0.555556   \n",
       "split1_train_score                              0.606061   \n",
       "split2_train_score                              0.588235   \n",
       "mean_train_score                                0.583284   \n",
       "std_train_score                                0.0209138   \n",
       "\n",
       "                                                     493  \\\n",
       "mean_fit_time                                 0.00199231   \n",
       "std_fit_time                                 3.22152e-05   \n",
       "mean_score_time                               0.00164978   \n",
       "std_score_time                               0.000459608   \n",
       "param_leaf_size                                       50   \n",
       "param_n_neighbors                                     98   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 98}   \n",
       "split0_test_score                                0.54902   \n",
       "split1_test_score                               0.568627   \n",
       "split2_test_score                               0.604167   \n",
       "mean_test_score                                 0.573333   \n",
       "std_test_score                                 0.0226439   \n",
       "rank_test_score                                      486   \n",
       "split0_train_score                              0.545455   \n",
       "split1_train_score                              0.585859   \n",
       "split2_train_score                              0.568627   \n",
       "mean_train_score                                0.566647   \n",
       "std_train_score                                0.0165542   \n",
       "\n",
       "                                                     494  \n",
       "mean_fit_time                                 0.00199366  \n",
       "std_fit_time                                 3.31004e-05  \n",
       "mean_score_time                               0.00199493  \n",
       "std_score_time                               3.08733e-05  \n",
       "param_leaf_size                                       50  \n",
       "param_n_neighbors                                     99  \n",
       "params              {'leaf_size': 50, 'n_neighbors': 99}  \n",
       "split0_test_score                               0.333333  \n",
       "split1_test_score                               0.333333  \n",
       "split2_test_score                               0.604167  \n",
       "mean_test_score                                     0.42  \n",
       "std_test_score                                  0.126337  \n",
       "rank_test_score                                      491  \n",
       "split0_train_score                              0.333333  \n",
       "split1_train_score                              0.333333  \n",
       "split2_train_score                              0.568627  \n",
       "mean_train_score                                0.411765  \n",
       "std_train_score                                 0.110919  \n",
       "\n",
       "[18 rows x 495 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_fit_time 학습 평균ㅅ간, 편차, 어떤 파라미터 사이즈 ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for문 노가다 말고 최적 파라메터 찾아주는 패키지\n",
    "# cross validation => 데이터가 작을때 성능 예측 방법,\n",
    "# 모든 데이터를 트레이닝할때 쓰므로 data leackage 안생김, 대충 성능 체크정도 5개 모델을 평균으로 만들어 최종 모델 활용 어려움?\n",
    "# 최종모델 x 모델의 정확성 측정 , 오버피팅 여부 체크\n",
    "# 내모델이 이모델보다 과하게 성능좋음 -> 오버피팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인코딩 -> (라벨인코딩, 원핫인코딩)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris ## 문자로 된 데이터 숫자로 변경\n",
    "    ## 판다스 apply map 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'species_value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-070ed2f348b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecies_value_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5180\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'species_value_counts'"
     ]
    }
   ],
   "source": [
    "iris.species_value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    1\n",
       "146    1\n",
       "147    1\n",
       "148    1\n",
       "149    1\n",
       "Name: species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.species.map({\"setosa\":0, 'virginica':1, 'versicolor':2}) ## 문자를 숫자로 라벨 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x에 있는 학습할 데이터를 숫자로 바꾸면 이걸 종류가 아니라 크기로 인식  조심하거나 주로 타겟데이터를 라벨인콛이한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LabelEncoder' object has no attribute 'fit_tranform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-c955f2a0b529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_tranform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 라벨 인코더를\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 1. 카테고리컬 데이터를 쓸떄는 라벨 인코딩을 한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 2. 원 핫 인코딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'fit_tranform'"
     ]
    }
   ],
   "source": [
    "le.fit_tranform() \n",
    "# 라벨 인코더를 \n",
    "\n",
    "# 1. 카테고리컬 데이터를 쓸떄는 라벨 인코딩을 한다.\n",
    "# 2. 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "0         1           0          0\n",
       "1         1           0          0\n",
       "2         1           0          0\n",
       "3         1           0          0\n",
       "4         1           0          0\n",
       "..      ...         ...        ...\n",
       "145       0           0          1\n",
       "146       0           0          1\n",
       "147       0           0          1\n",
       "148       0           0          1\n",
       "149       0           0          1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(iris.species) ## 더미변수 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(iris[['species']]).toarray() ## species 1차원 => 팬시인덱싱 2차원으로해야 쓸수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa']], dtype=object)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 사이킷 역변환 지원, 판다스는 x\n",
    "\n",
    "ohe.inverse_transform([[1.,0.,0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원핫인코드 \n",
    "## 벡터크기 항상 1, 대신 차원이 커진다. 차원의저주 : 데이터 별로 없는데 더미변수 쓰면 망한다.\n",
    "## 학습, 단어 -> 숫자로 다바꿔야함\n",
    "\n",
    "## 전처리 수많은 고통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 딥러닝 기반 nomalization 해야한다. 싸이킷에있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling 하면 피쳐 그대로 성능 좋아짐\n",
    "## min max 범위안으로 규모 줄이는 스케일링\n",
    "## 스탠다드 스캐일러 z-m / 표준편차 (표준화) 편차 영향 안받아서 모델이 잘만들어짐\n",
    "## 아웃라이커 로버스트 스케일링 사용 min max (X)\n",
    "## 아웃라이어 있을떄 로버스트 스케일링 사용 예측 특성 안변함, 회귀분석시 전체값 안한다 타겟값은 그대로, 학습데이터만 스케일링 하니까\n",
    "\n",
    "# 스케일링 전 후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15f137f86d8>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris.boxplot() # 웨이트 0~9사이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웨이트 0~5000 이런경우 -> 모델 성능을 낮춰버림 ->스케일링 확 낮춰버려서 크기적 영향이줄어듬\n",
    "\n",
    "## 중요한 애를 스케일링하면 성능 망가짐 \n",
    "## 도메인지식없이 아무떄나 스케일링하면 성능 떨어짐\n",
    "\n",
    "## 데이터 수집하고나서 각각 도메인에 대한 어트리뷰트별 조사를 해야한다.\n",
    "## 각각 데이터성질에 따라 스케일링 주의!\n",
    "\n",
    "## 문자데이터 중요한 값이면 웨이팅을 더 크게줘야한다!. ## 판다스에서는 맘대로 웨이팅 조절해서 쓸수 있다. iris.apply   iris.sepal_length.map(lambda x:x+1) + 예측할때도 같이 변환?\n",
    "\n",
    "## 딥러닝 초기 어떤값에 주느냐 수렴 여부 ( 노가다 포문돌린다. 최적의 초기값) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## empirical 경험적 해봤더니 ~~ 됬따! 성능좋누! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습어느정도 됬냐를 보고 필요한 학습 데이터량 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-evaluation\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/25/1e4e6462aeffb185ada97c9ff5a7b7ca59544e418541e3babd59f7171c0b/sklearn-evaluation-0.5.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\legen\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\legen\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (2.2.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\legen\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (2.10.1)\n",
      "Collecting tabulate (from sklearn-evaluation)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
      "Requirement already satisfied: mistune in c:\\users\\legen\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (0.8.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn-evaluation) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn-evaluation) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (2.8.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (2019.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (1.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\legen\\anaconda3\\lib\\site-packages (from jinja2->sklearn-evaluation) (1.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\legen\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->sklearn-evaluation) (41.0.1)\n",
      "Building wheels for collected packages: sklearn-evaluation, tabulate\n",
      "  Building wheel for sklearn-evaluation (setup.py): started\n",
      "  Building wheel for sklearn-evaluation (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\legen\\AppData\\Local\\pip\\Cache\\wheels\\a8\\38\\0d\\9103d63a0189c6e3b4ecc6f03e40c1b6762b1ff55612a7313c\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\legen\\AppData\\Local\\pip\\Cache\\wheels\\2b\\67\\89\\414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
      "Successfully built sklearn-evaluation tabulate\n",
      "Installing collected packages: tabulate, sklearn-evaluation\n",
      "Successfully installed sklearn-evaluation-0.5 tabulate-0.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_evaluation_selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'validation_curve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-85973ea06075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn_evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvalidation_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'validation_curve'"
     ]
    }
   ],
   "source": [
    "import sklearn_evaluation\n",
    "import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassifierEvaluator',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'compute',\n",
       " 'evaluator',\n",
       " 'metrics',\n",
       " 'plot',\n",
       " 'preprocessing',\n",
       " 'report',\n",
       " 'util',\n",
       " 'validate']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_size, train_score, test_score \\\n",
    " = learning_curve(KNeighborsClassifier(),  iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15f147184e0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_evaluation.plot.learning_curve(train_score, test_score,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 초록색 그래프가 수렴하지 않는다면 더 많은 데이터)\n",
    "## 계속 낮은 쪾에서 머무르면 전처리가 안되거나 아예 안되는 데이터\n",
    "## 초록색(cross-validation)가 올라가고 데이터가 더 필요하다\n",
    "## 데이터 충분히 학습되있는지 보여주는 그래프 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score, test_score = validation_curve(KNeighborsClassifier(),iris.iloc[:,:-1],iris.iloc[:,-1],'n_neighbors',[3,4,5,6,7,8,9,10],cv=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-fbce355edb95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msklearn_evaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n_neighbors'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn_evaluation\\plot\\validation_curve.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[1;34m(train_scores, test_scores, param_range, param_name, semilogx, ax)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scores_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training score\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     ax.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n\u001b[0;32m     57\u001b[0m             color=\"g\")\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1527\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 242\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (8,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_evaluation.plot.validation_curve(train_score,test_score,'n_neighbors',[3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x트레이닝 스코어는 가면갈수록 바이어스 낮아짐 좋아짐 => 학습이 어느정도 많이됨\n",
    "##  베스트 제너랄레이션 이후테스트에러 발생 보버피팅 발생( )\n",
    "\n",
    "## 어느정도 구간까지 학습해야되는지 확인하는그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0, 11,  0],\n",
       "       [ 0,  1, 16]], dtype=int64)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15f148e6f98>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_evaluation.plot.confusion_matrix(y_test, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       1.00      0.94      0.97        17\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        38\n",
      "   macro avg       0.97      0.98      0.98        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, knn.predict(X_test)))  ## 안예쁘면 print accuracy 만 가지고 모델 정확도 바보짓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score ## estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666668"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.mean(cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-1))#알고리즘을 인스턴스화한애\n",
    "# 실무 10 fold\n",
    "\n",
    "# 10등분 모델을 10개 만듬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어떤건 1 어떤건 낮음 => 데이타 리퀴지\n",
    "\n",
    "## 애 모델이 70 인데 내 모델이 85 정도 나오면 오버피팅\n",
    "## 80 82 이정도는 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9800000000000001"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "%time np.mean(cross_val_score(SVC(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-1))#알고리즘을 인스턴스화한애\n",
    "\n",
    "## svm 학습속도, 속도도 느림,  실무 사용 x / 수학으로 만들어짐 수학적 아름다움\n",
    "\n",
    "## 커널 트릭 유일하게 차원 확대 시키는 알고리즘 // 나머지는 차원 축소\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range()\n",
    "knn =  KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 0, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 0, 2, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 0, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 0, 2, 1, 0, 0, 0,\n",
       "       2, 1, 0, 1, 0, 1, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values  ## 판다스값 .values 하면 넘파이로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn.predict(X_test) == y_test.values ) #내가 하나하나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test,y_test)  ## 자체적 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 선형경우 X\n",
    "# but 선형 경향을 보여줘 설명하기 좋다 \n",
    "# 로지스틱 리그레션\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 코딩타입이 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()  ## 데이터가 선형알고리즘을 따르는지 먼저 판별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)  ## 파일 맨앞 버전 주석 달기, # 버전 바껴서 함수 파라메터 설정값이 바껴서 좆댄다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test,y_test)  ## 자체적 성능평가  ## 트레이닝셋이 적어서 성능이 들쭉날쭉 하다 => 모델 제대로 안만들어짐 => 데이터 새로 추가 수집\n",
    "                        # 성능이 계속 바뀜 => 언더피팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습할때마다 성능이 항상 변한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 비선형알고리즘, 선형알고리즘 성능이 비슷하다 => 선형 알고리즘이다. \n",
    "# 비선형 성능이 뛰어나다 = 비선형 알고리즘이다.\n",
    "# 현실은 비선형 // 설명가능여부 떄문에 선형을 하지 잘안한다.\n",
    "\n",
    "# 1차방정식 2차방정식 ... 알아야 할 x변수 많아진다. => 비선형 요구되는 데이터 많다. 적으면 성능 X(뉴럴네트워크)\n",
    "\n",
    "# X 차원, Y 타겟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE => 비선형중 설명 가능한 모델\n",
    "# 각각의 가지는 하나의 대응되는 룰로 바꿈 \n",
    "# IF A > 0    A < 0 => 시스템 화 시키기 쉬움, 오버피팅 잘남\n",
    "\n",
    "# 트리 모델 기반으로 앙상블 => 랜덤 포레스트 (성능 좋다, 오버피팅 잘안난다(여러개를 합쳐서),  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier() ## 싸이킷에서 트리모델 만드는거 별로 옵션이 별로 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.decision_path(X_test) ## sparse matrix => toarray() 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.decision_path(X_test).toarray() ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 어떤 알고리즘이 어떤 상황에 잘되냐\n",
    "## 싸이킷 공식 홈페이지 -> 알고리즘 선택 상황 가이드라인 제시\n",
    "## xd부스팅이 성능이 좋다 오버핏 안난다. 싸이킷말고 다른패키지에서\n",
    "\n",
    "## 데이터 분석 80퍼 전처리 => 정답데이터를 만든다.\n",
    "## 분석자가 동시에 수집 -> 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 랜던부스팅, 디시젼 트리 => 피쳐 임폴턴스(중요도 체크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03767183, 0.04137228, 0.40374965, 0.51720623])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_  ## 학습이 늦을때 피쳐 중요한 애들만 위주로 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  ##싸이킷에서 안쓴다 케라스 있는거 쓴다 사이킷이랑 케라스 잘붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB ## 나이브베이즈 확률기반 알고리즘 (가오시안, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용방법 다 똑같음 핏 \n",
    "## 전처리 엄청나게해야 그나마 성능 30퍼 넘는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "## 사람이 하는 행동처럼 분류하는애?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier() ## 수집한 데이터 알고리즘 vs 더미 알고리즘보다 성능이 못나오면 오직 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최적모델, 최적파라메터 => 찾는과정 노가다. 싸이킷 과정 최소화 기능 제공"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
