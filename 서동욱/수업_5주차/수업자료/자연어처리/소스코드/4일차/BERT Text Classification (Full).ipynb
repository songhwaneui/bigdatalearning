{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT Text Classification (Full).ipynb","version":"0.3.2","provenance":[{"file_id":"1le2ydSu4GUrWKkUeQMa_1_ZbE37xviNP","timestamp":1564649591143}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"adzlJRbDyc3A","colab_type":"code","outputId":"5f0f088c-79c3-466c-d418-187d2ab215e2","executionInfo":{"status":"ok","timestamp":1564649653685,"user_tz":-540,"elapsed":3346,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":75}},"source":["! pip install bert-tensorflow"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az_pyElrvT6z","colab_type":"code","outputId":"fbde9894-7ba1-495e-8f49-afe225e89e49","executionInfo":{"status":"ok","timestamp":1564649941697,"user_tz":-540,"elapsed":2182,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":93}},"source":["import math\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import pickle\n","import bert\n","import os\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","\n","def create_tokenizer_from_hub_module(bert_model_hub):\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    with tf.Graph().as_default():\n","        bert_module = hub.Module(bert_model_hub)\n","        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","        with tf.Session() as sess:\n","            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                                  tokenization_info[\"do_lower_case\"]])\n","\n","        print(\"Using BERT from %s\" %bert_model_hub)\n","        print(\"with vocab size=%d and do_lower_case=%s.\" %(len(vocab_file), str(do_lower_case)))\n","\n","    return bert.tokenization.FullTokenizer(\n","        vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","\n","def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n","    input_example = dataset.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n","                                                                             text_a=x[DATA_COLUMN],\n","                                                                             text_b=None,\n","                                                                             label=x[LABEL_COLUMN]), axis=1)\n","    features = bert.run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    return features\n","\n","\n","def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    bert_module = hub.Module(\n","        bert_model_hub,\n","        trainable=True)\n","    bert_inputs = dict(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids)\n","    bert_outputs = bert_module(\n","        inputs=bert_inputs,\n","        signature=\"tokens\",\n","        as_dict=True)\n","\n","    # Use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"pooled_output\"]\n","\n","    with tf.variable_scope(\"output_layer\"):\n","        layer_out = tf.layers.dense(\n","            inputs=output_layer,\n","            units=num_labels,\n","            use_bias=False,\n","            kernel_initializer=tf.initializers.variance_scaling()\n","        )\n","        predicted_labels = tf.squeeze(tf.argmax(layer_out, axis=-1, output_type=tf.int32))\n","\n","        if is_predicting:\n","            return predicted_labels, layer_out\n","        else:\n","            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","                labels=labels,\n","                logits=layer_out\n","            )\n","            loss = tf.reduce_mean(loss)\n","\n","            return loss, predicted_labels, layer_out\n","\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            (loss, predicted_labels, log_probs) = create_model(\n","                bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            train_op = bert.optimization.create_optimizer(\n","                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics.\n","            def metric_fn(label_ids, predicted_labels):\n","                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","                f1_score = tf.contrib.metrics.f1_score(\n","                    label_ids,\n","                    predicted_labels)\n","                auc = tf.metrics.auc(\n","                    label_ids,\n","                    predicted_labels)\n","                recall = tf.metrics.recall(\n","                    label_ids,\n","                    predicted_labels)\n","                precision = tf.metrics.precision(\n","                    label_ids,\n","                    predicted_labels)\n","                true_pos = tf.metrics.true_positives(\n","                    label_ids,\n","                    predicted_labels)\n","                true_neg = tf.metrics.true_negatives(\n","                    label_ids,\n","                    predicted_labels)\n","                false_pos = tf.metrics.false_positives(\n","                    label_ids,\n","                    predicted_labels)\n","                false_neg = tf.metrics.false_negatives(\n","                    label_ids,\n","                    predicted_labels)\n","                return {\n","                    \"eval_accuracy\": accuracy,\n","                    \"f1_score\": f1_score,\n","                    \"auc\": auc,\n","                    \"precision\": precision,\n","                    \"recall\": recall,\n","                    \"true_positives\": true_pos,\n","                    \"true_negatives\": true_neg,\n","                    \"false_positives\": false_pos,\n","                    \"false_negatives\": false_neg\n","                }\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  train_op=train_op)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                                                  loss=loss,\n","                                                  eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","                bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","                'probabilities': log_probs,\n","                'labels': predicted_labels\n","            }\n","            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn\n","\n","\n","def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE,\n","                      num_train_steps, num_warmup_steps, BATCH_SIZE):\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","        bert_model_hub=bert_model_hub,\n","        num_labels=len(label_list),\n","        learning_rate=LEARNING_RATE,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        config=run_config,\n","        params={\"batch_size\": BATCH_SIZE})\n","    return estimator, model_fn, run_config\n","\n","\n","def run_on_dfs(train, test, data_column, label_column,\n","               max_seq_length=128,\n","               batch_size=32,\n","               learning_rate=2e-5,\n","               num_train_epochs=3,\n","               warmup_proportion=0.1,\n","               save_summary_steps=100,\n","               save_checkpoint_steps=10000,\n","               bert_model_hub=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n","               output_dir=\"output\"):\n","    label_list = train[label_column].unique().tolist()\n","\n","    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n","\n","    train_features = make_features(train, label_list, max_seq_length, tokenizer, data_column, label_column)\n","    test_features = make_features(test, label_list, max_seq_length, tokenizer, data_column, label_column)\n","\n","    steps_per_epoch = math.ceil(len(train_features) / batch_size)\n","\n","    num_train_steps = int(len(train_features) / batch_size * num_train_epochs)\n","    num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","    estimator, model_fn, run_config = estimator_builder(\n","        bert_model_hub,\n","        output_dir,\n","        save_summary_steps,\n","        save_checkpoint_steps,\n","        label_list,\n","        learning_rate,\n","        num_train_steps,\n","        num_warmup_steps,\n","        batch_size)\n","\n","    train_input_fn = bert.run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=max_seq_length,\n","        is_training=True,\n","        drop_remainder=False)\n","\n","    test_input_fn = run_classifier.input_fn_builder(\n","        features=test_features,\n","        seq_length=max_seq_length,\n","        is_training=False,\n","        drop_remainder=False)\n","\n","    results = []\n","    for epoch in range(num_train_epochs):\n","        estimator.train(input_fn=train_input_fn, steps=steps_per_epoch)\n","\n","        print(\"End of epoch %d.\" %(epoch + 1))\n","\n","        result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n","        print(result_dict)\n","        results.append(result_dict)\n","\n","    return results, estimator\n","\n","\n","def pretty_print(result):\n","    df = pd.DataFrame([result]).T\n","    df.columns = [\"values\"]\n","    return df"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0801 08:58:59.291864 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IRVzb1Y2y9_m","colab_type":"code","colab":{}},"source":["def load_data(data_file):\n","    data = pd.read_csv(data_file)\n","\n","    # Only use the top quartile as polite, and bottom quartile as impolite. Discard the rest.\n","    quantiles = data[\"Normalized Score\"].quantile([0.25, 0.5, 0.75])\n","    # print(quantiles)\n","\n","    for i in range(len(data)):\n","        score = data.loc[i, \"Normalized Score\"]\n","        if score <= quantiles[0.25]:\n","            # Bottom quartile (impolite).\n","            data.loc[i, \"Normalized Score\"] = 0\n","        elif score >= quantiles[0.75]:\n","            # Top quartile (polite).\n","            data.loc[i, \"Normalized Score\"] = 1\n","        else:\n","            # Neutral.\n","            data.loc[i, \"Normalized Score\"] = 2\n","\n","    data[\"Normalized Score\"] = data[\"Normalized Score\"].astype(int)\n","\n","    # Discard neutral examples.\n","    data = data[data[\"Normalized Score\"] < 2]\n","    \n","    data.sample(frac=1).reset_index(drop=True)\n","    n_test = len(data) // 10\n","    test_data = data[:n_test]\n","    train_data = data[n_test:]\n","    \n","    print(\"Data loaded successfully. Train=%d, test=%d, total=%d.\" % (len(train_data), len(test_data), len(train_data) + len(test_data)))\n","    print(\"Some train samples:\")\n","    print(train_data.head())\n","    print(\"Some test samples:\")\n","    print(test_data.head())\n","\n","    return train_data, test_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BxeqnzOyok3","colab_type":"code","outputId":"e42af242-04ae-402a-bae4-daec13a859af","executionInfo":{"status":"error","timestamp":1564650022284,"user_tz":-540,"elapsed":70513,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if not os.path.exists(\"Stanford_politeness_corpus.zip\"):\n","  !wget http://www.cs.cornell.edu/~cristian/Politeness_files/Stanford_politeness_corpus.zip\n","\n","if not os.path.exists(\"Stanford_politeness_corpus/wikipedia.annotated.csv\"):\n","  !unzip Stanford_politeness_corpus.zip\n","\n","train_data, test_data = load_data(\"Stanford_politeness_corpus/wikipedia.annotated.csv\")\n","\n","# 데이터셋 만들기 등에 주의해라\n","\n","\n","# 데이터를 어떻게 해석 할 것인지 등등\n","params = {\n","    \"data_column\": \"Request\",\n","    \"label_column\": \"Normalized Score\",\n","#     \"learning_rate\": 2e-5,\n","    \"batch_size\": 16, # 버트 모델이 워낙 커서 배치가 크면 학습이 안된다.\n","                      #(한번에 몇문장 처리할지, 해당 숫자에 맞춰 메모리가 늘어난다.)\n","    \"num_train_epochs\": 3, # 원래 모델이 크고 학습이 잘되서 조금만 해도 학습이 잘나온다.\n","    \"bert_model_hub\": \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\" \n","    # cased, uncased는 소문자 대문자 차이. layer가 작은게 버트중에 작은 것\n","    # url부분 대체하면 해당 모델 사용 가능하다.\n","    # 버트는 해당 예제로만 구현하기 어려울거다. 불친절해서\n","    \n","    # 깃헙에 밑바닥, 원하는 다른데이터 방법도 있다.\n","}\n","\n","tf.logging.set_verbosity(tf.logging.INFO)\n","result, estimator = run_on_dfs(train_data, test_data, **params)\n","print(result)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Data loaded successfully. Train=1961, test=217, total=2178.\n","Some train samples:\n","     Community      Id  ...         TurkId5  Normalized Score\n","460  Wikipedia  621480  ...  A1Y3Z92RE62NPS                 1\n","462  Wikipedia  146267  ...  A3IHLWMZNBLUR4                 1\n","463  Wikipedia   84242  ...   AIPK94CUWL45W                 1\n","464  Wikipedia  487517  ...  A1F4D2PZ7NNWTL                 1\n","466  Wikipedia  629492  ...  A2WZQ92N4809N1                 1\n","\n","[5 rows x 14 columns]\n","Some test samples:\n","   Community      Id  ...         TurkId5  Normalized Score\n","0  Wikipedia  629705  ...  A15DM9BMKZZJQ6                 0\n","1  Wikipedia  244336  ...  A3TFQK7QK8X6LM                 1\n","5  Wikipedia  214411  ...  A1Y3Z92RE62NPS                 1\n","8  Wikipedia  177439  ...  A29B522D0BX6HN                 0\n","9  Wikipedia  341534  ...  A28TXBSZPWMEU9                 0\n","\n","[5 rows x 14 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["I0801 08:59:18.986533 140318135506816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n","W0801 08:59:20.112871 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0801 08:59:20.271485 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","I0801 08:59:20.272496 140318135506816 run_classifier.py:774] Writing example 0 of 1961\n","I0801 08:59:20.278708 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:20.281003 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:20.282832 140318135506816 run_classifier.py:464] tokens: [CLS] Thanks . As an aside , since this did turn out to be fact ##ual , just very hard to source , do you think the community would count ##enan ##ce an un ##block request from B ##la ##ab ##la if he accepted some strict un ##block conditions ( such as packing in the ' systemic bias ' thing , discussing his edit ##s in a less confrontation ##al manner etc ) ? [SEP]\n","I0801 08:59:20.284216 140318135506816 run_classifier.py:465] input_ids: 101 5749 119 1249 1126 4783 117 1290 1142 1225 1885 1149 1106 1129 1864 4746 117 1198 1304 1662 1106 2674 117 1202 1128 1341 1103 1661 1156 5099 25191 2093 1126 8362 27467 4566 1121 139 1742 6639 1742 1191 1119 3134 1199 9382 8362 27467 2975 113 1216 1112 16360 1107 1103 112 27410 15069 112 1645 117 10751 1117 14609 1116 1107 170 1750 14002 1348 4758 3576 114 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.285614 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.286979 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.288266 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:20.290212 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:20.291436 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:20.292678 140318135506816 run_classifier.py:464] tokens: [CLS] Everything about < u ##rl > looks fantastic , but . . going to | 2 instead of | 30 ##em seems like a major step back . Is there a reason for it ? [SEP]\n","I0801 08:59:20.293807 140318135506816 run_classifier.py:465] input_ids: 101 5268 1164 133 190 17670 135 2736 14820 117 1133 119 119 1280 1106 197 123 1939 1104 197 1476 5521 3093 1176 170 1558 2585 1171 119 2181 1175 170 2255 1111 1122 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.295014 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.295971 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.297059 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:20.299353 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:20.300650 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:20.301820 140318135506816 run_classifier.py:464] tokens: [CLS] I wonder if it would ever be worth doing an article on G & S scholarship ? You know , cover the major discoveries , describe the evolution of the field . . . or is that too likely to hit problems ? [SEP]\n","I0801 08:59:20.303004 140318135506816 run_classifier.py:465] input_ids: 101 146 4608 1191 1122 1156 1518 1129 3869 1833 1126 3342 1113 144 111 156 7084 136 1192 1221 117 2267 1103 1558 17707 117 5594 1103 7243 1104 1103 1768 119 119 119 1137 1110 1115 1315 2620 1106 1855 2645 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.304040 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.305171 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.306284 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:20.307966 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:20.309185 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:20.310347 140318135506816 run_classifier.py:464] tokens: [CLS] Thanks for your help on this , it ' s much appreciated . Should I del ##ete my request for check ##user ? [SEP]\n","I0801 08:59:20.311727 140318135506816 run_classifier.py:465] input_ids: 101 5749 1111 1240 1494 1113 1142 117 1122 112 188 1277 12503 119 9743 146 3687 16618 1139 4566 1111 4031 19399 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["Using BERT from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\n","with vocab size=76 and do_lower_case=False.\n"],"name":"stdout"},{"output_type":"stream","text":["I0801 08:59:20.313328 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.314849 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.315971 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:20.317722 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:20.319013 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:20.321036 140318135506816 run_classifier.py:464] tokens: [CLS] Yes please ! B ##uff ##ing up ' ' < u ##rl > ' ' to at least reflect a bit better on current state - of - play - tax ##ono ##mic ##ally would be good : ) Any Re ##lia ##ble Sources call it a < u ##rl > ? [SEP]\n","I0801 08:59:20.322285 140318135506816 run_classifier.py:465] input_ids: 101 2160 4268 106 139 9435 1158 1146 112 112 133 190 17670 135 112 112 1106 1120 1655 7977 170 2113 1618 1113 1954 1352 118 1104 118 1505 118 3641 23038 7257 2716 1156 1129 1363 131 114 6291 11336 4567 2165 22656 1840 1122 170 133 190 17670 135 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.324473 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.325773 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:20.326770 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:21.354981 140318135506816 run_classifier.py:774] Writing example 0 of 217\n","I0801 08:59:21.356204 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:21.356971 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:21.361059 140318135506816 run_classifier.py:464] tokens: [CLS] Where did you learn English ? How come you ' re taking on a third language ? [SEP]\n","I0801 08:59:21.362747 140318135506816 run_classifier.py:465] input_ids: 101 2777 1225 1128 3858 1483 136 1731 1435 1128 112 1231 1781 1113 170 1503 1846 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.365456 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.369037 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.370267 140318135506816 run_classifier.py:468] label: 0 (id = 1)\n","I0801 08:59:21.372491 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:21.374003 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:21.375634 140318135506816 run_classifier.py:464] tokens: [CLS] Thanks very much for your edit to the < u ##rl > article . Would you be interested in ta ##ckling the < u ##rl > of < u ##rl > ? [SEP]\n","I0801 08:59:21.377707 140318135506816 run_classifier.py:465] input_ids: 101 5749 1304 1277 1111 1240 14609 1106 1103 133 190 17670 135 3342 119 5718 1128 1129 3888 1107 27629 27102 1103 133 190 17670 135 1104 133 190 17670 135 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.379250 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.381229 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.382346 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:21.384654 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:21.385524 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:21.387316 140318135506816 run_classifier.py:464] tokens: [CLS] | style = \" vertical - al ##ign : middle ; pad ##ding : 3 ##p ##x ; \" | I ' ve started the Bad ##finger w ##iki am ##d I need help . You seem to know a lot about them , could you please help out ? [SEP]\n","I0801 08:59:21.389234 140318135506816 run_classifier.py:465] input_ids: 101 197 1947 134 107 7391 118 2393 11368 131 2243 132 12921 3408 131 124 1643 1775 132 107 197 146 112 1396 1408 1103 6304 22225 192 12635 1821 1181 146 1444 1494 119 1192 3166 1106 1221 170 1974 1164 1172 117 1180 1128 4268 1494 1149 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.391189 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.395461 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.396625 140318135506816 run_classifier.py:468] label: 1 (id = 0)\n","I0801 08:59:21.399387 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:21.401129 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:21.402483 140318135506816 run_classifier.py:464] tokens: [CLS] These are my numbers : 7 years in Wikipedia , 6 years as an ad ##min , 570 + articles , 4 featured articles , 1 featured list , 21 Good articles , 60 D ##Y ##K ' s - After six years as an ad ##min . I recently made some mistakes and I can understand if I am placed in some type of probation were I am monitored and forbidden to use my tools maybe for a year , but do I really merit the removal of my ad ##mins ##hip ? [SEP]\n","I0801 08:59:21.403889 140318135506816 run_classifier.py:465] input_ids: 101 1636 1132 1139 2849 131 128 1201 1107 18920 117 127 1201 1112 1126 8050 7937 117 28081 116 4237 117 125 2081 4237 117 122 2081 2190 117 1626 2750 4237 117 2539 141 3663 2428 112 188 118 1258 1565 1201 1112 1126 8050 7937 119 146 3055 1189 1199 12572 1105 146 1169 2437 1191 146 1821 1973 1107 1199 2076 1104 23793 1127 146 1821 19232 1105 12031 1106 1329 1139 5537 2654 1111 170 1214 117 1133 1202 146 1541 16008 1103 8116 1104 1139 8050 19296 3157 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.405529 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.407002 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.408620 140318135506816 run_classifier.py:468] label: 0 (id = 1)\n","I0801 08:59:21.411806 140318135506816 run_classifier.py:461] *** Example ***\n","I0801 08:59:21.415309 140318135506816 run_classifier.py:462] guid: None\n","I0801 08:59:21.418872 140318135506816 run_classifier.py:464] tokens: [CLS] I couldn ' t tell you why g ##lam rock was there . Better ? [SEP]\n","I0801 08:59:21.421236 140318135506816 run_classifier.py:465] input_ids: 101 146 1577 112 189 1587 1128 1725 176 7609 2067 1108 1175 119 8529 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.424077 140318135506816 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.427119 140318135506816 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0801 08:59:21.430038 140318135506816 run_classifier.py:468] label: 0 (id = 1)\n","I0801 08:59:21.556416 140318135506816 estimator.py:209] Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9dbe6a5fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","W0801 08:59:21.568390 140318135506816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","I0801 08:59:22.603411 140318135506816 estimator.py:1145] Calling model_fn.\n","I0801 08:59:25.298545 140318135506816 saver.py:1499] Saver not created because there are no variables in the graph to restore\n","W0801 08:59:25.423263 140318135506816 deprecation.py:323] From <ipython-input-3-46a055aefb92>:65: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","W0801 08:59:25.889151 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","W0801 08:59:25.891107 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","W0801 08:59:25.899555 140318135506816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","W0801 08:59:25.920042 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","W0801 08:59:26.119870 140318135506816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0801 08:59:29.555423 140318135506816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","W0801 08:59:35.952176 140318135506816 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","I0801 08:59:36.406354 140318135506816 estimator.py:1147] Done calling model_fn.\n","I0801 08:59:36.408950 140318135506816 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","I0801 08:59:39.555317 140318135506816 monitored_session.py:240] Graph was finalized.\n","I0801 08:59:46.654820 140318135506816 session_manager.py:500] Running local_init_op.\n","I0801 08:59:46.882315 140318135506816 session_manager.py:502] Done running local_init_op.\n","I0801 08:59:55.681823 140318135506816 basic_session_run_hooks.py:606] Saving checkpoints for 0 into output/model.ckpt.\n","I0801 09:00:10.966051 140318135506816 basic_session_run_hooks.py:262] loss = 0.65065295, step = 0\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-12111b662671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_on_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-46a055aefb92>\u001b[0m in \u001b[0;36mrun_on_dfs\u001b[0;34m(train, test, data_column, label_column, max_seq_length, batch_size, learning_rate, num_train_epochs, warmup_proportion, save_summary_steps, save_checkpoint_steps, bert_model_hub, output_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End of epoch %d.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1191\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1253\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         logging.info(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"AhIsEyzdZ5HI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}