{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SequenceTagger_blank.ipynb의 사본","version":"0.3.2","provenance":[{"file_id":"1hTCoI6tliZLdOjWctHnoH1ZbWCV9jfYW","timestamp":1564551115688},{"file_id":"1Y1HqlyLGbkTqbe7VOsDZc6OtrdLWE-_t","timestamp":1564476139071}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ixmf_AX6AeR5","colab_type":"text"},"source":["## 1. CoNLL-2003 Dataset Download"]},{"cell_type":"code","metadata":{"id":"2NZW2Ayi8qMT","colab_type":"code","outputId":"7d451c56-bc3a-4aae-efcd-1f89a4550ddc","executionInfo":{"status":"ok","timestamp":1564554686703,"user_tz":-540,"elapsed":7326,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"source":["!wget -O CoNLL-2003.zip https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n","!mkdir CoNLL-2003\n","!unzip CoNLL-2003.zip -d CoNLL-2003\n","!rm CoNLL-2003.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-07-31 06:31:18--  https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip [following]\n","--2019-07-31 06:31:18--  https://www.dropbox.com/s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline/Alt5zsQ5kguUb-PfweUMC_I_5L2g3btpWv2IXyLyAuufKrzUst5XEty5mjsgTYRprub5pQno05Y6oyZyKd2PpYiVcZVZV4PHmlUWEHG_l-sc_w/file# [following]\n","--2019-07-31 06:31:18--  https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline/Alt5zsQ5kguUb-PfweUMC_I_5L2g3btpWv2IXyLyAuufKrzUst5XEty5mjsgTYRprub5pQno05Y6oyZyKd2PpYiVcZVZV4PHmlUWEHG_l-sc_w/file\n","Resolving uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com (uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com (uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/Als_9RqeukDcr_CyRLkWitqP1kKBWbhjldtaJS0OHl7RFr6tekRUd8fT36muDRebFbzAWJ1GaqvKfbHCGlLbgZndqEBPSGuC_ySQI3RWsHmTQFsaqp8o4yH5YPnV2pIUKw36TJ36IiEaMVp7PvmaAzVxfuus2wH6bi5lzChh3RUW-gpRtC-aQFWIiQ6bsvRABm5dz9fjLZCliXRRtsihWkvzRKnEc-Kv325e5a9CP5eeuoBzxHERI-6tCBbu6Q93IJ75ON8XnpoyuptkQLzS0s-ePLk7mJZdyLdizcwI5JXmVYhz1z2sQzUexyPZxFXfJgoie9Ocr423iJ1MP1TMhyKZ/file [following]\n","--2019-07-31 06:31:19--  https://uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com/cd/0/inline2/Als_9RqeukDcr_CyRLkWitqP1kKBWbhjldtaJS0OHl7RFr6tekRUd8fT36muDRebFbzAWJ1GaqvKfbHCGlLbgZndqEBPSGuC_ySQI3RWsHmTQFsaqp8o4yH5YPnV2pIUKw36TJ36IiEaMVp7PvmaAzVxfuus2wH6bi5lzChh3RUW-gpRtC-aQFWIiQ6bsvRABm5dz9fjLZCliXRRtsihWkvzRKnEc-Kv325e5a9CP5eeuoBzxHERI-6tCBbu6Q93IJ75ON8XnpoyuptkQLzS0s-ePLk7mJZdyLdizcwI5JXmVYhz1z2sQzUexyPZxFXfJgoie9Ocr423iJ1MP1TMhyKZ/file\n","Reusing existing connection to uc1c0584c52e01cdb8f1b767ea2f.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 691339 (675K) [application/zip]\n","Saving to: ‘CoNLL-2003.zip’\n","\n","CoNLL-2003.zip      100%[===================>] 675.14K  2.30MB/s    in 0.3s    \n","\n","2019-07-31 06:31:19 (2.30 MB/s) - ‘CoNLL-2003.zip’ saved [691339/691339]\n","\n","Archive:  CoNLL-2003.zip\n","  inflating: CoNLL-2003/label.vocab  \n","  inflating: CoNLL-2003/sample.inputs  \n","  inflating: CoNLL-2003/sample.labels  \n","  inflating: CoNLL-2003/sample.vocab  \n","  inflating: CoNLL-2003/test.inputs  \n","  inflating: CoNLL-2003/test.labels  \n","  inflating: CoNLL-2003/train.inputs  \n","  inflating: CoNLL-2003/train.labels  \n","  inflating: CoNLL-2003/train.vocab  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_91llDkIA95Z","colab_type":"text"},"source":["## 2. Import Modules"]},{"cell_type":"code","metadata":{"id":"XstaB1Wa6Obr","colab_type":"code","outputId":"3bb2c786-0efa-4d81-f955-1b70999a06be","executionInfo":{"status":"ok","timestamp":1564554693758,"user_tz":-540,"elapsed":3764,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["import json\n","import collections\n","from datetime import datetime\n","import os\n","import logging\n","import tensorflow as tf\n","from nltk.tokenize import word_tokenize\n","import nltk\n","import tqdm\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"W_4JKAp8BKVA","colab_type":"text"},"source":["## 3. Logger Settings"]},{"cell_type":"code","metadata":{"id":"wKMt1HGt_Tel","colab_type":"code","colab":{}},"source":["def init_logger(path:str):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    logger = logging.getLogger()\n","    logger.handlers = []\n","    logger.setLevel(logging.DEBUG)\n","    debug_fh = logging.FileHandler(os.path.join(path, \"debug.log\"))\n","    debug_fh.setLevel(logging.DEBUG)\n","\n","    info_fh = logging.FileHandler(os.path.join(path, \"info.log\"))\n","    info_fh.setLevel(logging.INFO)\n","\n","    ch = logging.StreamHandler()\n","    ch.setLevel(logging.INFO)\n","\n","    info_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')\n","    debug_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s | %(lineno)d:%(funcName)s')\n","\n","    ch.setFormatter(info_formatter)\n","    info_fh.setFormatter(info_formatter)\n","    debug_fh.setFormatter(debug_formatter)\n","\n","    logger.addHandler(ch)\n","    logger.addHandler(debug_fh)\n","    logger.addHandler(info_fh)\n","\n","    return logger\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZedCFnABXYA","colab_type":"text"},"source":["## 4. Hyperparameters Settings"]},{"cell_type":"code","metadata":{"id":"YX0vqhpz-N_H","colab_type":"code","colab":{}},"source":["hparams_dict = {\n","  \"root_dir\": \"out_dirs/KoreaUniv_Data/TEST/\",\n","  \"vocab_size\": 10000,\n","  \"num_epochs\": 10,\n","  \"batch_size\": 16,\n","  \"embedding_dim\": 100,\n","  \"rnn_hidden_dim\": 128,\n","  \"rnn_depth\": 3,\n","  \"dropout_keep_prob\": 1.0\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNQw791j-lDB","colab_type":"code","outputId":"0ccdb13d-5d9c-448a-a288-c30e7a1981bf","executionInfo":{"status":"ok","timestamp":1564554697909,"user_tz":-540,"elapsed":413,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n","root_dir = os.path.join(hparams_dict[\"root_dir\"], \"%s/\" % timestamp)\n","logger = init_logger(root_dir)\n","logger.info(\"Hyper-parameters: %s\" %str(hparams_dict))\n","hparams_dict[\"root_dir\"] = root_dir\n","hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n","\n","data_dir = \"./CoNLL-2003\"\n","dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n","logger = logging.getLogger(__name__)\n","iterator_initializers = []"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2019-07-31 06:31:35,221 | INFO     | Hyper-parameters: {'root_dir': 'out_dirs/KoreaUniv_Data/TEST/', 'vocab_size': 10000, 'num_epochs': 10, 'batch_size': 16, 'embedding_dim': 100, 'rnn_hidden_dim': 128, 'rnn_depth': 3, 'dropout_keep_prob': 1.0}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"J-Pw09XzBl40","colab_type":"text"},"source":["## 5. Make Vocab Table"]},{"cell_type":"code","metadata":{"id":"E7151biR-lcG","colab_type":"code","colab":{}},"source":["def make_vocab_table():\n","    \"\"\"\n","    [A]\n","    Vocabulary(단어집) 파일을 로드합니다.\n","    단어 -> id, id -> 단어 변환 테이블을 생성합니다.\n","\n","    \"\"\"\n","    # Train data set vocab이 전처리 된 것이 들어있음 by nltk\n","    with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n","        vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n","        \n","    # 10000개가 넘으면 그정도만 가져와라. most common기준\n","    if len(vocab) > hparams.vocab_size:\n","        vocab = vocab[:hparams.vocab_size]\n","    \n","    # print(vocab[0:20])\n","    # print(len(vocab))\n","    # 문장 길이를 하나의 매트릭스로 만들기 위해 padding   \n","        \n","    id2word = vocab\n","    word2id = {}\n","    for i, word in enumerate(vocab):\n","        word2id[word] = i\n","\n","    \"\"\"\n","    [B]\n","    Label(태그 모음) 파일을 로드합니다.\n","    태그 -> id, id -> 태그 변환 테이블을 생성합니다.\n","    \"\"\"\n","    \n","    with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n","      # label도 각각 패딩.\n","      labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n","      labels.insert(0, \"PAD\")\n","      id2label = labels\n","      label2id = {}\n","      for i, label in enumerate(labels):\n","          label2id[label] = i\n","    \n","    \n","    return (id2word, word2id), (id2label, label2id)\n","make_vocab_table()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"regd4vxPBnNA","colab_type":"text"},"source":["## 6. Build Graph (Sequence Tagger Model)"]},{"cell_type":"code","metadata":{"id":"hyr0hDR8_p9C","colab_type":"code","colab":{}},"source":["def build_graph(inputs:tf.Tensor, lengths:tf.Tensor, id2word, id2label):\n","      print(\"Building graph for model: sequence tagger\")\n","\n","      \"\"\"\n","      [C]\n","      단어 임베딩 행렬을 생성합니다.\n","      단어 id를 단어 임베딩 텐서로 변환합니다.\n","      \"\"\"\n","      \n","      # Number of possible output categories.\n","      output_dim = len(id2label) \n","      vocab_size = len(id2word) + 1 # +1은 10000개 넘어가면 unknown태그를 주기위해\n","      embeddings = tf.get_variable( # trainable None으로 되있다 -> True 디폴트\n","          \"embeddings\",\n","          shape=[vocab_size, hparams.embedding_dim],\n","          initializer=tf.initializers.variance_scaling(\n","              scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n","      )\n","      \n","      # [batch_size, sequence_length] : inputs. embedding matrix \n","      # 룩어테이블이 매트릭스를 각각의 임베딩 벡터로 확장\n","      embedded = tf.nn.embedding_lookup(embeddings, inputs) # 워드별로 벡터를 만듦.\n","      # shape = [batch_size, sequence_length, embed_dim (100)]\n","      layer_out = embedded\n","      \n","      \"\"\"\n","      [D]\n","      단어 임베딩을 RNN의 입력으로 사용하기 전,\n","      차원 수를 맞춰주고 성능을 향상시키기 위해\n","      projection layer를 생성하여 텐서를 통과시킵니다.\n","      \"\"\"\n","      layer_out = tf.layers.dense(\n","          inputs=layer_out,\n","          units=hparams.rnn_hidden_dim,\n","          activation=tf.nn.relu,\n","          kernel_initializer=tf.initializers.variance_scaling(\n","              scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n","          name=\"input_projection\"\n","      )      \n","      \n","      # batch, sequnece_length, embedding_dim => batch, sequence_length, rnn_hidden_dim\n","\n","      \"\"\"\n","      [E]\n","      양방향 RNN을 생성하고, 여기에 텐서를 통과시킵니다.\n","      이렇게 하여, 단어간 의존 관계가 반영된 단어 자질 텐서를 얻습니다.\n","      \"\"\"\n","      \n","      \n","      with tf.variable_scope(\"bi-RNN\"):\n","          # Build RNN layers\n","          rnn_cell_forward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n","          rnn_cell_backward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n","\n","          # Apply dropout to RNN\n","          if hparams.dropout_keep_prob < 1.0:\n","              rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward, output_keep_prob=dropout_keep_prob_ph)\n","              rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward, output_keep_prob=dropout_keep_prob_ph)\n","\n","          # Stack multiple layers of RNN\n","          # rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * hparams.rnn_depth)\n","          # rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * hparams.rnn_depth)\n","\n","          \n","          # bidirectional rnn에 인자로 던져줌.\n","          (output_forward, output_backward), ((forward_final_cell,forward_final_hidden),(backward_final_cell,backward_final_hidden)) = tf.nn.bidirectional_dynamic_rnn(\n","              rnn_cell_forward, rnn_cell_backward,\n","              inputs=layer_out,\n","              sequence_length=lengths,\n","              dtype=tf.float32\n","          ) # 무조건 rank를 3짜리를 받아야 들어간다.\n","          \n","          # output_forward : [batch, max_sequence_length,rnn_hidden_dim] # 워드 하나에 embedding dim하나 있으므로, \n","          # output_backward : [batch,max_sequence_length, rnn_hiddne_dim]\n","        \n","          # output_foward_hidden : [batch,rnn_hidden_dim] -> 문장의 제일 마지막 단어 rnn_hidden_state\n","          # output_backward_hidden : [batch,rnn_hidden_dim] -> 문장의 제일 첫번째 단어 rnn_hidden_state\n","          # 이 두가지만 concat하면 sentence의 전체적인 representation. \n","          \n","          hiddens = tf.concat([output_forward, output_backward], axis=-1)\n","          # shape = [batch_size, max_sequence_length, rnn_dim*2]\n","\n","      \"\"\"\n","      [F]\n","      마스킹을 적용하여 문장 길이를 통일하기 위해 적용했던 padding을 제거합니다.\n","      \"\"\"\n","\n","      # Donald Trump is the president of the United States. [1,10,100]\n","      # Barack Obama was the president <PAD> <PAD> <PAD> <PAD>  [1,10,100]\n","      \n","      \n","      # <PAD>는 id가 0이 된다.\n","      # 배치가 2라고 가정 / 문장길이 각각 10, 6 -> 패딩필요\n","      # I live in Paris <PAD> <PAD> <PAD> <PAD> <PAD> [1,10,100]\n","      \n","      # [3,10,100] (embedding) -> [3,10,128] (input_projection) -> [3,10,128](rnn forward), [3,10,128](rnn_backward) -> [3,10,256](concat)\n","      # [3,10,256] -> [3,10,10]으로 만든다.\n","      \n","      # padding에 대한 feature를 삭제해야 한다.\n","      \n","      # lengths : [10,6,5]\n","      mask = tf.sequence_mask(lengths) # [[True,True,True,True,True,True,True,True,True,True],\n","                                       #  [True,True,True,True,True,True,False,False,,False,,False]] 와 같은 방식이 된다.\n","        \n","      # hidden : [3,10,256]\n","      \n","      bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, hparams.rnn_hidden_dim * 2]) # True location에 있는 것만 가져오는 작업\n","      layer_out = bi_lstm_out  # shape=[sum of seq length, 2*LSTM hidden layer size]\n","      # True를 가진애들로 reshape하므로 [21,256] 사이즈를 갖는다.\n","      \"\"\"\n","      [G]\n","      단어 자질 텐서를 바탕으로 단어의 태그를 예측합니다.\n","      이를 위해 fully-connected(dense) layer를 생성하고 텐서를 통과시킵니다.\n","      \"\"\"\n","      # [21,256] [256,10]matrix필요. + b[10] -> [21,10]\n","\n","\n","      with tf.variable_scope(\"read-out\"):\n","        prev_layer_size = layer_out.get_shape().as_list()[1]\n","        weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim],\n","                                 initializer=tf.initializers.variance_scaling(\n","                                     scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n","                                 ))\n","        bias = tf.get_variable(\"bias\", shape=[output_dim],\n","                               initializer=tf.initializers.zeros())\n","        predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n","\n","#       tf.layers.dense(\n","#           inputs=\n","#           units=\n","#           # activation=tf.nn.relu 어차피 나중에 필요해서 여기서는 ㄴㄴ\n","#           kernel_initializer=\n","#           bias_initializer= \n","#       )        \n","\n","      return predictions\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSmF2AfNBnsp","colab_type":"text"},"source":["## 7. Load Data (tf.data)"]},{"cell_type":"code","metadata":{"id":"V5TRwTMs-ljq","colab_type":"code","colab":{}},"source":["def load_data(id2word, word2id, id2label, label2id):\n","      \"\"\"\n","      [L]\n","      단어->id 및 태그->id 변환 테이블을 텐서 그래프에 추가합니다.\n","      \"\"\"\n","      \n","      word2id = tf.contrib.lookup.index_table_from_tensor(\n","        mapping=tf.constant(id2word),\n","        num_oov_buckets=1,\n","        name=\"word2id\"\n","      )\n","\n","      label2id = tf.contrib.lookup.index_table_from_tensor(\n","        mapping=tf.constant(id2label),\n","        default_value=label2id[\"O\"],\n","        name=\"label2id\"\n","      )      \n","      \n","      \"\"\"\n","      [M]\n","      입력 데이터 파일을 읽어들여 이를 단어 id로 변환하는 텐서 그래프를 생성합니다.\n","      \"\"\"\n","      \n","      input_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.inputs\"))\n","      batched_input_dataset = input_dataset.batch(hparams.batch_size)\n","      # 배치 사이즈 정의\n","      \n","      input_iterator = batched_input_dataset.make_initializable_iterator()\n","      # epoch이 돌때마다 initialize\n","      \n","      batch_input = input_iterator.get_next()\n","      batch_input.set_shape([hparams.batch_size])\n","      \n","      # 잘라져서 넣어있어서 \n","      words = tf.string_split(batch_input, \" \")\n","      \n","      # split해둔 워드를 각각 id를 받는다.\n","      word_ids = word2id.lookup(words)\n","      \n","      \n","      dense_word_ids = tf.sparse_tensor_to_dense(word_ids)\n","      # shape = [batch_size, time]\n","\n","\n","      line_number = word_ids.indices[:, 0]\n","      line_position = word_ids.indices[:, 1]\n","      lengths = tf.segment_max(data=line_position,\n","                               segment_ids=line_number) + 1\n","      # padding처리하는 부분. 라인에서 length정보를 문장마다 가져온다.\n","      \n","      \n","      \"\"\"\n","      [N]\n","      태그 데이터 파일을 읽어들여 이를 태그 id로 변환하는 텐서 그래프를 생성합니다.\n","      \"\"\"\n","      \n","      label_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.labels\"))\n","      batched_label_dataset = label_dataset.batch(hparams.batch_size)\n","      label_iterator = batched_label_dataset.make_initializable_iterator()\n","      batch_label_str = label_iterator.get_next()\n","      batch_label = tf.string_split(batch_label_str, \" \")\n","      label_ids = label2id.lookup(batch_label)\n","      dense_label_ids = tf.sparse_tensor_to_dense(label_ids)\n","      # shape = [batch_size, time]\n","\n","      mask = tf.sequence_mask(lengths) # 패딩을 없애는 과정.\n","      dense_label_ids = tf.boolean_mask(dense_label_ids, mask) # 패딩을 없애는 과정.\n","\n","      iterator_initializers.append(input_iterator.initializer)\n","      iterator_initializers.append(label_iterator.initializer)\n","      \n","      return dense_word_ids, dense_label_ids, lengths"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XTEfi0RBoJT","colab_type":"text"},"source":["## 8. Train Model (session call)"]},{"cell_type":"code","metadata":{"id":"Yjm5_0Bj-lp1","colab_type":"code","colab":{}},"source":["def train_model():\n","      sess = tf.Session()\n","      with sess.as_default():\n","          global_step = tf.Variable(0, name='global_step', trainable=False)\n","\n","          # vocab을 만드는 부분\n","          (id2word, word2id), (id2label, label2id) = make_vocab_table()\n","          inputs, labels, lengths = load_data(id2word, word2id, id2label, label2id)\n","\n","          with tf.variable_scope(\"build_graph\", reuse=False):\n","              logits = build_graph(inputs, lengths, id2word, id2label)\n","\n","          print(\"ok\")\n","          \"\"\"\n","          [O]\n","          모델을 훈련시키기 위해 필요한 오퍼레이션들을 텐서 그래프에 추가합니다.\n","          여기에는 loss, train, accuracy 계산 등이 포함됩니다.\n","          \"\"\"\n","\n","          loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, # 라벨\n","                                                                   name=\"cross_entropy\")\n","          loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n","          train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n","\n","          eval = tf.nn.in_top_k(logits, labels, 1)\n","          correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n","          accuracy = tf.divide(correct_count, tf.shape(labels)[0])\n","\n","          # Initialize iterators, tables, and variables.\n","          local_iterator_initializers = tf.group(*iterator_initializers)\n","          tf.tables_initializer().run()\n","          tf.global_variables_initializer().run()\n","\n","          saver = tf.train.Saver()\n","\n","          for epochs_completed in range(hparams.num_epochs):\n","              local_iterator_initializers.run()\n","              accuracy_mean, loss_mean, idx_cnt = 0, 0, 0\n","              while True:\n","                  \"\"\"\n","                  [P]\n","                  그래프에 데이터를 입력하여 필요한 계산들을 수행하고,\n","                  Loss에 따라 gradient를 계산하여 파라미터들을 업데이트합니다.\n","                  이러한 과정을 training step이라고 합니다.\n","                  \"\"\"\n","                  try:\n","                    accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n","                        [accuracy, labels, loss_op, global_step, train_op],\n","                        feed_dict={dropout_keep_prob_ph: hparams.dropout_keep_prob}\n","                    )\n","                    accuracy_mean += accuracy_val\n","                    loss_mean += loss_val\n","                    idx_cnt += 1\n","                    if global_step_val % 50 == 0:\n","                        accuracy_mean /= idx_cnt\n","                        loss_mean /= idx_cnt\n","                        logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (global_step_val, loss_mean, accuracy_mean * 100))\n","                        accuracy_mean, loss_mean,idx_cnt = 0, 0, 0\n","                  except tf.errors.OutOfRangeError:\n","                    # End of epoch.\n","                    break\n","\n","\n","\n","              \"\"\"\n","              [Q]\n","              전체 학습 데이터에 대하여 1회 학습을 완료하였습니다.\n","              이를 1 epoch라고 합니다.\n","              딥러닝 모델의 학습은 일반적으로 수십~수백 epoch 동안 진행됩니다.\n","\n","              \"\"\"\n","              logger.info(\"End of epoch %d.\" % (epochs_completed+1))\n","              save_path = saver.save(sess, \"saves/model.ckpt\", global_step=global_step_val)\n","              logger.info(\"Model saved at: %s\" % save_path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-x9dVWZEBpRv","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"8ofY5we0ADFW","colab_type":"code","outputId":"589a4a73-c348-4cf3-e35d-61bee5e7ca0d","executionInfo":{"status":"error","timestamp":1564555165734,"user_tz":-540,"elapsed":5219,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Train the vanilla Bi-directional LSTM model\n","train_model()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2019-07-31 06:31:52,236 | WARNING  | \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","2019-07-31 06:31:52,311 | WARNING  | From <ipython-input-9-882b8025cffa>:25: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n","2019-07-31 06:31:52,330 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:978: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2019-07-31 06:31:52,403 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`normal` is a deprecated alias for `truncated_normal`\n","2019-07-31 06:31:52,404 | WARNING  | From <ipython-input-8-a539595885fb>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Building graph for model: sequence tagger\n"],"name":"stdout"},{"output_type":"stream","text":["2019-07-31 06:31:52,782 | WARNING  | From <ipython-input-8-a539595885fb>:48: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","2019-07-31 06:31:52,785 | WARNING  | From <ipython-input-8-a539595885fb>:64: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","2019-07-31 06:31:52,787 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","2019-07-31 06:31:52,883 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2019-07-31 06:31:52,896 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["ok\n"],"name":"stdout"},{"output_type":"stream","text":["2019-07-31 06:31:59,960 | INFO     | [Step 50] loss: 1.2840, accuracy: 72.56%\n","2019-07-31 06:32:04,806 | INFO     | [Step 100] loss: 0.7828, accuracy: 82.36%\n","2019-07-31 06:32:09,277 | INFO     | [Step 150] loss: 0.7357, accuracy: 80.67%\n","2019-07-31 06:32:13,671 | INFO     | [Step 200] loss: 0.7172, accuracy: 79.55%\n","2019-07-31 06:32:17,870 | INFO     | [Step 250] loss: 0.6377, accuracy: 80.22%\n","2019-07-31 06:32:22,061 | INFO     | [Step 300] loss: 0.5363, accuracy: 84.67%\n","2019-07-31 06:32:24,365 | INFO     | [Step 350] loss: 0.5900, accuracy: 81.33%\n","2019-07-31 06:32:28,062 | INFO     | [Step 400] loss: 0.4391, accuracy: 85.33%\n","2019-07-31 06:32:32,005 | INFO     | [Step 450] loss: 0.4045, accuracy: 86.48%\n","2019-07-31 06:32:36,536 | INFO     | [Step 500] loss: 0.3840, accuracy: 86.98%\n","2019-07-31 06:32:41,239 | INFO     | [Step 550] loss: 0.3492, accuracy: 89.92%\n","2019-07-31 06:32:45,559 | INFO     | [Step 600] loss: 0.3694, accuracy: 86.71%\n","2019-07-31 06:32:50,772 | INFO     | [Step 650] loss: 0.3878, accuracy: 88.27%\n","2019-07-31 06:32:54,937 | INFO     | [Step 700] loss: 0.3036, accuracy: 90.30%\n","2019-07-31 06:32:59,796 | INFO     | [Step 750] loss: 0.3719, accuracy: 88.11%\n","2019-07-31 06:33:05,205 | INFO     | [Step 800] loss: 0.3082, accuracy: 90.30%\n","2019-07-31 06:33:09,626 | INFO     | [Step 850] loss: 0.3066, accuracy: 90.41%\n","2019-07-31 06:33:15,403 | INFO     | [Step 900] loss: 0.2455, accuracy: 92.25%\n","2019-07-31 06:33:19,107 | INFO     | End of epoch 1.\n","2019-07-31 06:33:19,293 | INFO     | Model saved at: saves/model.ckpt-937\n","2019-07-31 06:33:20,657 | INFO     | [Step 950] loss: 0.3083, accuracy: 91.10%\n","2019-07-31 06:33:24,761 | INFO     | [Step 1000] loss: 0.2553, accuracy: 92.68%\n","2019-07-31 06:33:31,311 | INFO     | [Step 1050] loss: 0.2331, accuracy: 93.05%\n","2019-07-31 06:33:35,320 | INFO     | [Step 1100] loss: 0.1635, accuracy: 95.37%\n","2019-07-31 06:33:40,611 | INFO     | [Step 1150] loss: 0.2138, accuracy: 92.98%\n","2019-07-31 06:33:44,004 | INFO     | [Step 1200] loss: 0.2189, accuracy: 93.89%\n","2019-07-31 06:33:48,234 | INFO     | [Step 1250] loss: 0.1700, accuracy: 94.76%\n","2019-07-31 06:33:50,807 | INFO     | [Step 1300] loss: 0.1271, accuracy: 96.66%\n","2019-07-31 06:33:55,285 | INFO     | [Step 1350] loss: 0.1390, accuracy: 96.07%\n","2019-07-31 06:33:58,779 | INFO     | [Step 1400] loss: 0.1434, accuracy: 95.54%\n","2019-07-31 06:34:03,688 | INFO     | [Step 1450] loss: 0.1498, accuracy: 95.34%\n","2019-07-31 06:34:08,183 | INFO     | [Step 1500] loss: 0.1722, accuracy: 94.85%\n","2019-07-31 06:34:13,078 | INFO     | [Step 1550] loss: 0.1516, accuracy: 95.39%\n","2019-07-31 06:34:18,222 | INFO     | [Step 1600] loss: 0.2098, accuracy: 93.88%\n","2019-07-31 06:34:21,949 | INFO     | [Step 1650] loss: 0.1244, accuracy: 96.29%\n","2019-07-31 06:34:27,175 | INFO     | [Step 1700] loss: 0.1666, accuracy: 95.34%\n","2019-07-31 06:34:32,756 | INFO     | [Step 1750] loss: 0.1623, accuracy: 95.03%\n","2019-07-31 06:34:37,153 | INFO     | [Step 1800] loss: 0.0935, accuracy: 97.32%\n","2019-07-31 06:34:42,950 | INFO     | [Step 1850] loss: 0.1091, accuracy: 96.97%\n","2019-07-31 06:34:45,042 | INFO     | End of epoch 2.\n","2019-07-31 06:34:45,182 | INFO     | Model saved at: saves/model.ckpt-1874\n","2019-07-31 06:34:47,795 | INFO     | [Step 1900] loss: 0.1339, accuracy: 96.16%\n","2019-07-31 06:34:51,556 | INFO     | [Step 1950] loss: 0.1010, accuracy: 97.09%\n","2019-07-31 06:34:57,086 | INFO     | [Step 2000] loss: 0.1124, accuracy: 96.67%\n","2019-07-31 06:35:00,813 | INFO     | [Step 2050] loss: 0.0563, accuracy: 98.35%\n","2019-07-31 06:35:06,422 | INFO     | [Step 2100] loss: 0.1040, accuracy: 96.51%\n","2019-07-31 06:35:09,265 | INFO     | [Step 2150] loss: 0.0692, accuracy: 98.17%\n","2019-07-31 06:35:13,339 | INFO     | [Step 2200] loss: 0.0819, accuracy: 97.57%\n","2019-07-31 06:35:16,230 | INFO     | [Step 2250] loss: 0.0524, accuracy: 98.44%\n","2019-07-31 06:35:20,987 | INFO     | [Step 2300] loss: 0.0783, accuracy: 97.81%\n","2019-07-31 06:35:24,484 | INFO     | [Step 2350] loss: 0.0708, accuracy: 98.23%\n","2019-07-31 06:35:29,651 | INFO     | [Step 2400] loss: 0.1088, accuracy: 96.87%\n","2019-07-31 06:35:33,887 | INFO     | [Step 2450] loss: 0.0678, accuracy: 97.93%\n","2019-07-31 06:35:39,032 | INFO     | [Step 2500] loss: 0.1238, accuracy: 96.22%\n","2019-07-31 06:35:43,609 | INFO     | [Step 2550] loss: 0.1041, accuracy: 96.71%\n","2019-07-31 06:35:47,610 | INFO     | [Step 2600] loss: 0.0728, accuracy: 98.04%\n","2019-07-31 06:35:53,180 | INFO     | [Step 2650] loss: 0.1211, accuracy: 96.68%\n","2019-07-31 06:35:58,152 | INFO     | [Step 2700] loss: 0.1080, accuracy: 96.64%\n","2019-07-31 06:36:03,281 | INFO     | [Step 2750] loss: 0.0546, accuracy: 98.37%\n","2019-07-31 06:36:08,655 | INFO     | [Step 2800] loss: 0.0867, accuracy: 97.31%\n","2019-07-31 06:36:09,570 | INFO     | End of epoch 3.\n","2019-07-31 06:36:09,723 | INFO     | Model saved at: saves/model.ckpt-2811\n","2019-07-31 06:36:13,129 | INFO     | [Step 2850] loss: 0.0643, accuracy: 98.05%\n","2019-07-31 06:36:17,538 | INFO     | [Step 2900] loss: 0.0598, accuracy: 98.46%\n","2019-07-31 06:36:22,633 | INFO     | [Step 2950] loss: 0.0745, accuracy: 97.78%\n","2019-07-31 06:36:26,736 | INFO     | [Step 3000] loss: 0.0508, accuracy: 98.32%\n","2019-07-31 06:36:31,684 | INFO     | [Step 3050] loss: 0.0450, accuracy: 98.74%\n","2019-07-31 06:36:35,114 | INFO     | [Step 3100] loss: 0.0400, accuracy: 98.88%\n","2019-07-31 06:36:38,387 | INFO     | [Step 3150] loss: 0.0405, accuracy: 98.82%\n","2019-07-31 06:36:41,484 | INFO     | [Step 3200] loss: 0.0338, accuracy: 99.00%\n","2019-07-31 06:36:46,083 | INFO     | [Step 3250] loss: 0.0599, accuracy: 98.36%\n","2019-07-31 06:36:49,956 | INFO     | [Step 3300] loss: 0.0445, accuracy: 98.78%\n","2019-07-31 06:36:55,058 | INFO     | [Step 3350] loss: 0.0812, accuracy: 97.70%\n","2019-07-31 06:36:58,774 | INFO     | [Step 3400] loss: 0.0641, accuracy: 98.06%\n","2019-07-31 06:37:04,124 | INFO     | [Step 3450] loss: 0.0884, accuracy: 97.03%\n","2019-07-31 06:37:08,648 | INFO     | [Step 3500] loss: 0.0659, accuracy: 97.96%\n","2019-07-31 06:37:13,016 | INFO     | [Step 3550] loss: 0.0693, accuracy: 98.16%\n","2019-07-31 06:37:18,583 | INFO     | [Step 3600] loss: 0.0886, accuracy: 97.58%\n","2019-07-31 06:37:23,213 | INFO     | [Step 3650] loss: 0.0552, accuracy: 98.26%\n","2019-07-31 06:37:28,898 | INFO     | [Step 3700] loss: 0.0634, accuracy: 97.95%\n","2019-07-31 06:37:33,939 | INFO     | End of epoch 4.\n","2019-07-31 06:37:34,083 | INFO     | Model saved at: saves/model.ckpt-3748\n","2019-07-31 06:37:34,338 | INFO     | [Step 3750] loss: 0.0356, accuracy: 99.26%\n","2019-07-31 06:37:38,257 | INFO     | [Step 3800] loss: 0.0410, accuracy: 98.95%\n","2019-07-31 06:37:43,396 | INFO     | [Step 3850] loss: 0.0480, accuracy: 98.50%\n","2019-07-31 06:37:48,090 | INFO     | [Step 3900] loss: 0.0468, accuracy: 98.71%\n","2019-07-31 06:37:52,606 | INFO     | [Step 3950] loss: 0.0394, accuracy: 98.84%\n","2019-07-31 06:37:56,740 | INFO     | [Step 4000] loss: 0.0285, accuracy: 99.14%\n","2019-07-31 06:38:00,972 | INFO     | [Step 4050] loss: 0.0242, accuracy: 99.13%\n","2019-07-31 06:38:03,310 | INFO     | [Step 4100] loss: 0.0208, accuracy: 99.38%\n","2019-07-31 06:38:07,302 | INFO     | [Step 4150] loss: 0.0315, accuracy: 99.04%\n","2019-07-31 06:38:11,368 | INFO     | [Step 4200] loss: 0.0373, accuracy: 98.97%\n","2019-07-31 06:38:15,903 | INFO     | [Step 4250] loss: 0.0322, accuracy: 99.10%\n","2019-07-31 06:38:20,389 | INFO     | [Step 4300] loss: 0.0552, accuracy: 98.51%\n","2019-07-31 06:38:24,807 | INFO     | [Step 4350] loss: 0.0498, accuracy: 98.20%\n","2019-07-31 06:38:29,968 | INFO     | [Step 4400] loss: 0.0828, accuracy: 97.33%\n","2019-07-31 06:38:34,061 | INFO     | [Step 4450] loss: 0.0384, accuracy: 98.71%\n","2019-07-31 06:38:38,997 | INFO     | [Step 4500] loss: 0.0593, accuracy: 98.29%\n","2019-07-31 06:38:44,322 | INFO     | [Step 4550] loss: 0.0772, accuracy: 97.89%\n","2019-07-31 06:38:48,762 | INFO     | [Step 4600] loss: 0.0454, accuracy: 98.68%\n","2019-07-31 06:38:54,580 | INFO     | [Step 4650] loss: 0.0426, accuracy: 98.59%\n","2019-07-31 06:38:58,080 | INFO     | End of epoch 5.\n","2019-07-31 06:38:58,222 | INFO     | Model saved at: saves/model.ckpt-4685\n","2019-07-31 06:38:59,777 | INFO     | [Step 4700] loss: 0.0382, accuracy: 98.86%\n","2019-07-31 06:39:03,367 | INFO     | [Step 4750] loss: 0.0252, accuracy: 99.34%\n","2019-07-31 06:39:08,950 | INFO     | [Step 4800] loss: 0.0360, accuracy: 98.84%\n","2019-07-31 06:39:12,758 | INFO     | [Step 4850] loss: 0.0305, accuracy: 99.04%\n","2019-07-31 06:39:18,291 | INFO     | [Step 4900] loss: 0.0340, accuracy: 98.97%\n","2019-07-31 06:39:21,591 | INFO     | [Step 4950] loss: 0.0136, accuracy: 99.69%\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-45c1ebd97177>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                     accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n\u001b[1;32m     49\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdropout_keep_prob_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     )\n\u001b[1;32m     52\u001b[0m                     \u001b[0maccuracy_mean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"yz85PSZnNS2B","colab_type":"text"},"source":["## Model Load and Predict"]},{"cell_type":"code","metadata":{"id":"NWxSlqZd_iou","colab_type":"code","colab":{}},"source":["def load_and_predict(saved_file:str):\n","    sentence = input(\"Enter a sentence: \")\n","\n","    \"\"\"\n","    [H]\n","    입력 문자열을 단어/문장부호 단위로 쪼개고, 이를 다시 단어 id로 변환합니다.\n","    \"\"\"\n","    sentence = word_tokenize(sentence)\n","    word_ids = []\n","    (id2word, word2id), (id2label, label2id) = make_vocab_table()\n","\n","    for word in sentence:\n","        if word in word2id:\n","            word_ids.append(word2id[word])\n","        else:\n","            word_ids.append(len(word2id))\n","\n","    tf.reset_default_graph()\n","    sess = tf.Session()\n","    with sess.as_default():\n","        \"\"\"\n","        [I]\n","        태깅을 수행하기 위해 텐서 그래프를 생성합니다.\n","        \"\"\"\n","         \n","        dense_word_ids = tf.constant(word_ids)\n","        lengths = tf.constant(len(word_ids))\n","        # Insert batch dimension.\n","        dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n","        lengths = tf.expand_dims(lengths, axis=0)\n","\n","        with tf.variable_scope(\"build_graph\", reuse=tf.AUTO_REUSE):\n","            logits = build_graph(dense_word_ids, lengths, id2word, id2label)\n","        predictions = tf.argmax(logits, axis=1)\n","  \n","        \n","        \"\"\"\n","        [J]\n","        저장된 모델을 로드하고, 데이터를 입력하여 태깅 결과를 얻습니다.\n","        \"\"\"\n","        saver = tf.train.Saver()\n","        saver.restore(sess, saved_file)\n","        pred_val = sess.run(\n","            [predictions]\n","        )[0]\n","\n","        \n","        \n","\n","    \"\"\"\n","    [K]\n","    태깅 결과를 출력합니다.\n","    \"\"\"\n","    \n","    pred_str = [id2label[i] for i in pred_val]\n","    for word, tag in zip(sentence, pred_str):\n","        print(\"%s[%s]\" %(word, tag), end=' ')\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5_B0m2rG-Dq","colab_type":"code","outputId":"ab36776e-867c-4147-f059-ff5ecf3b02e4","executionInfo":{"status":"ok","timestamp":1564555187071,"user_tz":-540,"elapsed":16949,"user":{"displayName":"군고구마","photoUrl":"https://lh5.googleusercontent.com/-I0XUuvaS1h0/AAAAAAAAAAI/AAAAAAAAAC4/1a-GQoFjqY4/s64/photo.jpg","userId":"10196243389166907276"}},"colab":{"base_uri":"https://localhost:8080/","height":205}},"source":["load_and_predict(\"/content/saves/model.ckpt-1874\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Enter a sentence: Donald Trump is the president of The United States\n","['<PAD>', '.', ',', 'the', 'of', 'in', 'to', 'a', '(', ')']\n","10000\n","Building graph for model: sequence tagger\n"],"name":"stdout"},{"output_type":"stream","text":["2019-07-31 06:39:43,901 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","2019-07-31 06:39:43,908 | INFO     | Restoring parameters from /content/saves/model.ckpt-1874\n"],"name":"stderr"},{"output_type":"stream","text":["Donald[B-PER] Trump[I-PER] is[O] the[O] president[O] of[O] The[O] United[B-LOC] States[I-LOC] "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4AP_CxHmwVCs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}