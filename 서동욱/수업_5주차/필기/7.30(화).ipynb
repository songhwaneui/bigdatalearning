{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELMo 다른 선생님 오셔서 코드 같은 것 자세하게 할것\n",
    "오늘은 워드 임베딩이 어느정도 인지 개요정도로 알면 된다.\n",
    "코리안 버트 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "컴퓨터 연산을 위해서 숫자로 바꿔야 한다.\n",
    "\n",
    "1. One - hot encoding\n",
    "=> 단어들 간의 관계성을 고려하여 표현하지 않는다.\n",
    "\n",
    "2. 매우 높은 dimension\n",
    "=> sparse, memory expensive\n",
    "\n",
    "3. Vector space model\n",
    "=> Counter based / Predictive method\n",
    "(LSA, TF-IDF 실습을 하지는 않을 것임) / (Word2Vec) 이걸 더 많이 씀.\n",
    "=> Dense embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec\n",
    "- 모든 NLP문제는 비슷한 의미를 가질 것이라는 가정\n",
    "- Bert : 성능이 요즘 제일. XLnet 관심있으면 찾아봐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Embedding\n",
    "- 하드웨어 발전으로 Document 단위로도 할 수 있게 됨.\n",
    "- sparse 한 vectors -> dense vector. 차원이 너무 높으면 계산량이 많음.\n",
    "- 다른 표현도 있다. Distributionlal Semantic Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "서비스 단에서는 LSA,LDA가 속도가 좋아서 쓰는 곳도 많음.(딥러닝은 아직 느려서)\n",
    "NLP는 word-embedding을 활용한 것이 대부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD를 이용해서 낮은 차원으로 바꿔줌.\n",
    "같은 문맥에서 비슷한 단어가 나온다는 가정.\n",
    "SVM n차원에서 classification을 하는 모델\n",
    "\n",
    "<딥러닝>\n",
    "Brain 자체도 layered 되어있다.\n",
    "Perceptron rule은 linearly separable 인 것만 가정하므로 delta rule이 필요하다.\n",
    "\n",
    "Step function은 미분이 안되서 미분 가능한 함수를 사용한다.\n",
    "데이터를 다 때려박으면서 하면 시간이 오래걸리므로\n",
    "SGD : batch size만큼 나눠서 따로해보자는 취지.\n",
    "=> local minima에 빠질 확률이 적다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2vec\n",
    "1. CBOW < - >  skip gram\n",
    "2. 한계점 : 각각의 dense vector가 표현이 되는데 Hot Potato 같은 게임의 경우 \n",
    "    각각의 단어로 학습이 되서 Game이라는 것이 반영이 안된다.\n",
    "3. 나는 사과를 먹는다. Window size =2  : 앞뒤로 2개씩 본다.\n",
    "    - opossing sentiment polarities 에 대한 문제.\n",
    "    \n",
    "- CBOW : 주변 문맥(vector)이 주여졌을 때, 타겟 워드를 예측 하는 것\n",
    "- Skip gram : 단어가 주어졌을 때 주변 단어를 예측 하는 것이다.\n",
    "    => 그래서 이거 가지고 cat과 dog은 비슷하다고 보는 것이다.\n",
    "    Limit 1. 학습 데이터에서 잘 안나오면 반영이 잘 안된다.\n",
    "          2. Out of Vocabulary : 학습된 단어만 처리하고 처음 보는\n",
    "            단어는 처리를 못하는 문제  \n",
    "          3. 형태소 단위에서는 더 민감한 문제이다.\n",
    "         *4. OoV문제를 character의 조합이니까 그걸가지고 극복하겠다. \n",
    "    형태소 단위는 Oov 문제가 필연적으로 발생한다.\n",
    "    우리 수업에서 character 단위는 하지 않을 것이다.\n",
    "    \n",
    "    - Character Embedding\n",
    "        NER, POS-tagging : character level을 이용해서 학습을 한다.\n",
    "        * convolution, RNN모델을 통해 할 것임을 내일 다룰 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove\n",
    "  Word2Vect은 두개의 layer여서 단순하면서도 효과적여서 각광을 받았었음.\n",
    "\n",
    "1. global count statistics : \n",
    "    TF-IDF 모델처럼 count-based + 관계성을 모델에 모두 반영해 보겠다는 취지.\n",
    "\n",
    "2. co-occurence probabilities를 이용해 vector를 학습 해보겠다.\n",
    "- 적은양으로도 좋은 것을 학습 가능.\n",
    "\n",
    "3. 다른 단어여도 인접한 단어 frog같은 것을 잘 찾아낸다.\n",
    "\n",
    "4. 단어의 비교급을 잘 찾아낸다. / 비교가 잘된다.\n",
    "5. Glove가 더 성능이 높다.\n",
    "\n",
    "FastText\n",
    "\n",
    "1. Word2vec을 좀더 다르게 해서 발표한 모델.\n",
    "대부분의 모델이 형태학적 특성을 반영 못한다는 얘기가 있는데 그걸 해결하려는 취지.\n",
    "\n",
    "N-gram을 이용했음.\n",
    "\n",
    "2. 겹쳐서 학습. 순서도 어느정도 반영 됨.\n",
    "Text Classification. Word representation 사용할 때도 활용한다.\n",
    "\n",
    "3. hashed dictionary 계산량을 편하기 하기 위해 나오는 table\n",
    "\n",
    "4. character단위 에서 OoV문제에 좀더 자유롭게 대처 가능\n",
    "\n",
    "Q FastText와 NER의 차이는??\n",
    "Q Glove는 Word2vec과의 layer 구조적 차이??\n",
    "\n",
    "Contextual Representation\n",
    "1. 다른 bank를 같은 vector로 표현하는 문제.\n",
    "2. 모델학습할때 이 다른 점들을 반영하겠다.\n",
    "language 모델은 내일 배울 것이다.\n",
    "3. Feature-based model은 갖다 붙이면 되고\n",
    "Fine-tuning approach는 task에 맞게 다시 튜닝한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELMo\n",
    "Contextual 한 것을 반영을 하겠다. 여러 단어로 이루어진 것을 합치는 것이 ELMo\n",
    "Transformer 모델이 나중에 갈아 치웠다. biLM, 단어에 따른 다른 문장 나온다.\n",
    "\n",
    "Open GPT\n",
    "Transformer LM. 관련 논문 찾아볼 것. 'Attention is all you need'\n",
    "=> 꼭 읽어보기, 그전에 attention 모델을 먼저 봐야 함.\n",
    "=> 수업에서는 커버하기가 다 불가능함.\n",
    "\n",
    "논문하나 완벽히 이해하는게 더 좋음.\n",
    "\n",
    "Transformer모델은 인코더와 디코더로 되어있는데, (번역을 할 때 인풋에 단어가 들어오면\n",
    "새로운 언어로 변형되는 모델.) 디코더를 조금 바꾼 것이다.\n",
    "\n",
    "GPT는 universial representation이 가능하다. 다방면에서 성능이 좋다.\n",
    "\n",
    "ELMo는 LSTM은 bidirectional, Transformer는 multilayer를 여러개 쌓은 것임.\n",
    "GPT는 fine tuning을 붙여서 더 좋다.\n",
    "\n",
    "BERT\n",
    "Word2Vec 왼쪽 layer 구조 오른쪽은 BERT 논문 볼 것 (Deep해진게 차이점)\n",
    "대부분 모델이 Unidirectional 하다는게 문제인데, 이건 양방향. ELMO랑은 다름.\n",
    "그냥 왼쪽오른쪽 학습된거 갖다 붙인건데, BERT는 그 자체만으로 양방향으로 학습을\n",
    "한다. \n",
    "\n",
    " - MLM 단어를 마스크 처리해놓고 학습을 해본다.\n",
    " - BERT의 장점universial한 representation이다.\n",
    "\n",
    "Pre training, Fine Tuning 두가지가 있는데 Fine-Tuning을 더 살렸다.\n",
    "\n",
    "Transformer Encoder와 decoder가 특징.\n",
    "GPT는 decoder 모델 (다른걸 뱉어주는 것)\n",
    "BERT는 encoder 모델 (들어가서 변환)\n",
    "\n",
    "위치정보를 주기 위해 Positional embedding을 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Korean BERT\n",
    "\n",
    "- 형태소 분석 기반의 언어모델.\n",
    "- 형태소분석을 하지 않은 어절기반의 언어모델.\n",
    "- 자소 단위도 가능하지만 보통 character 단위로 많이 한다. \n",
    "영어는 자소단위가 없다. 한국어의 특성 형태소단위/자소단위 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit.ly에서 url 퍼둔거 관리를 해주는데\n",
    "거기서 url을 줄여주니까 사용하면 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensorboard (로컬 pc)\n",
    "Embeddeing projecter (온라인)\n",
    "\n",
    "Wordvectors\n",
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "지금까지는 단일 언어(한국어)에 관한 것.\n",
    "\n",
    "언어가 2개 이상인 것.\n",
    "\n",
    "서로 다른 두 언어에 대한 data?\n",
    "한국어에 대한 data필요\n",
    "\n",
    "Length-ratio suffle : 서로 다른 언어에서 2개의 문장을 하나로 합쳐서 training해야함.\n",
    "    -> 하나의 doc으로 합치는 과정\n",
    "    -> 그 후 Word2vec사용\n",
    "\n",
    "Dataset : 영어, 한국 자막 데이터, 병렬로 같은 의미로 가진 것이 데이터로 되어 있음.(이래야 환경이 구성이 됨.)\n",
    "    \n",
    "    OPUS 2016\n",
    "    \n",
    "    token 비율로 섞어서 학습을 진행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주재걸 교수님 특강 \n",
    "- 영상인식 / 자연어처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "기계학습 : 태스크가 뭔지 학습데이터가 있다.\n",
    "pair를 만들어서 예제 데이터를 주낟.\n",
    "딥러닝 : 머신러닝의 카테고리다.\n",
    "\n",
    "성공요인:\n",
    "1. 예전에는 데이터의 양이 별로 많지 않았다. labeling 된것도 별로 없어서.\n",
    "2. GPU acceleration\n",
    "\n",
    "CUDA : C++ 그래픽카드에서 거의 할 수 있게 함.\n",
    "뇌의 뉴런이라는 것이 Sigmoid와 비슷하게 작용을 하고 있다.\n",
    "model parameter가 많으면 학습 data도 굉장히 오래걸린다.\n",
    "\n",
    "Test data에 대한 것은 gap이 생겨버린다.\n",
    "L1, L2 Regularization, DropOut.\n",
    "\n",
    "Data Augmentation : 데이터 증대, 왼쪽을 보는 것만을 잘못 overfitting 하면, 간단한 변형을 준다. \n",
    "    상하반전은 하지 않는다. 실제로 일어날법한 데이터만 augmentation을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAM : momentum RMS prop의 차이점은? RMS prop 평균적인 값으로 나눠주자. \n",
    "    다른 방식 : 가중치를 준다. 이때 등비수열을 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch norm을 쓰냐 안쓰냐에 따라 학습이 되냐 안되냐 성능이 나오냐 안나오냐 측면도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine transformation : 정규화도 딥러닝이 스스로 결정하게 하자는 취지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading Comprehension-based Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer based on self-attention \"Attention is all you need\"\n",
    "\n",
    "=> 굳이 LSTM을 쓰지 않아도 되는 딥러닝 layer를 많이 쌓은 모델."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 향후 전망\n",
    "기술 발전 속도는 점점 더뎌지고 있다.\n",
    "모델은 점점 사용하기 쉬워져서 진입장벽은 점점 낮아진다.\n",
    "결국 확보된 데이터의 종류나 그 양과 질 측면이 승부처가 될 것이다.\n",
    "=> 메이저 회사(구글,페이스북,마이크로소프트 등 메이저 회사들이 독식할 가능성이 높다.)\n",
    "문제를 발굴하는 능력 + 기술의 이해를 바탕으로 어떤 식으로 해당 문제를 formulate할 것인지가 중요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
