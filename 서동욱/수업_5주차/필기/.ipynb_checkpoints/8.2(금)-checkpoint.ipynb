{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "문장을 이해하는 데 있어 가장 중요한 것은 단어를 해석 하는 것이다.\n",
    "\n",
    "\n",
    "단어들을 정확히 이해해야함.\n",
    "서로가 대화할 때 그 의미를 정확히 전달하는게 중요. \n",
    ": 중의적 표현이 들어가면 이해가 난해해짐 -> 컴퓨터가 이걸 도와주는 시스템\n",
    "\n",
    "< 중의성 해소 >\n",
    "1. 문맥을 가지고 추론\n",
    "\n",
    "\n",
    "\n",
    "2. 지식을 가지고 추론\n",
    "\n",
    "\n",
    "3. 문맥 정보를 활용한 WSD 추론 과정\n",
    "사전 정보가 없을 때 문맥정보를 가지고 추론을 시키는 것.\n",
    "\n",
    "< WordNet >\n",
    "계층화 시켜서 표현화한게 WordNet의 의미관계 구조\n",
    "\n",
    "WordNet을 이용해 중요성을 해소하는 방법을 배웠음.\n",
    "Lesk는 단어간의 정확한 일치가 되어야 함.\n",
    "사전 정의에 굉장히 의존적임. Count가 되지 않으면 좋은 성능이 나오지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. 사전 정의 기반을 보고 \n",
    "- 생각보다는 성능이 나쁘지 않음.\n",
    "\n",
    "Knowledge Based WSD\n",
    "2. 그래프(관계성)을 보고 추론\n",
    "단어 카운팅에 관한 제약이 없어짐\n",
    "사전적 정의에 의존하지 않음.\n",
    "단점 : context정보를 반영하지 못함.\n",
    "\n",
    "drink v1,v2이라면 , 각각 지정된 length만큼 각각 불러온다.\n",
    "서치기반 알고리즘을 가지고 서브그래프를 만들어준다.\n",
    "\n",
    "가장 의미간 연결성이 많은 노드를 선택한다. Wordnet은 가장 많이 사용 되는\n",
    "것이 위에 올라가서 동일한 경우 위에걸 선택함.\n",
    "\n",
    "< 반복적 그래프 기반 방법 >\n",
    "위에 것은 불필요한 엣지정보까지 너무 많이 가져온다는 생각에서 나옴.\n",
    "작은 것부터 서브그래프를 만들어간다.\n",
    "\n",
    "작은 것들을 먼저 중의성을 해결한다. -> 확률이 좀더 높다.\n",
    "sense count만 가지고 만들면 정확한 그래프를 만든 것이 아니다.\n",
    "\n",
    "=> similarity score를 가지고 만들게 된다.\n",
    "\n",
    "한계점 : 워드넷이 가지고 없는 단어는 풀 수가 없다.\n",
    "sub graph만들 때 count와 유사성만 가지고 워드넷에 있는걸 사용하기 때문에\n",
    "성능이 좋지 않다. (즉 문맥의 핵심단어가 고려가 안된다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[지도학습]\n",
    "- KNN, DNN, SVM\n",
    "- 딥러닝이 더 많은 패턴을 고려해서 성능이 잘나오고 있다.\n",
    "\n",
    "Supervised WSD\n",
    "1. LSTM은 어순정보의 웨이트를 넘겨줌\n",
    "2. Global attention으로 단어간의 의존 관계를 보는 것.\n",
    "\n",
    "Global attention은 한 문장의 모든 단어의 관계성을 보는 것\n",
    "Local attention은 특정 사이즈의 관계성을 보는 것\n",
    "\n",
    "Global attention이 계산량은 많지만 성능은 더 잘나옴.\n",
    "LSTM 자체가 무겁고 global attention까지 보면 속도에 큰 영향을 줘서\n",
    "local을 쓰기도한다.\n",
    "\n",
    "\n",
    "CNN임베딩이 없는 단어를 보완하는 역할\n",
    "\n",
    "pos tag : 형태소분석을 통해 좀더 정확히 파악하려고 달아 줌.\n",
    "    \n",
    "Word embedding이 문맥에 다른 점 반영을 못해서\n",
    "ELMO, BERT등이 나옴\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
