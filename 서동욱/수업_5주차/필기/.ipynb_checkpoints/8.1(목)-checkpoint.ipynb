{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter와 paramter를 계산하는 방법\n",
    "\n",
    "요즘은 paramter가 수십억개 : loss함수가 공간을 형성한다고 볼 수 있다.\n",
    "계곡이 있다면 빠질 수도 있지만 내려갈 수 있다.\n",
    "\n",
    "One to many : image를 설명하는 text\n",
    "many to one : 오늘 함\n",
    "many to many : 일대일 아닌 것들은 기계번역\n",
    "                일대일인 것은 어제 한 것과 같은 개체명 인식\n",
    "        \n",
    "What model to use\n",
    "모델은 실제 개발을 할때는 별로 중요하지 않음.\n",
    "요즘 오픈소스가 많아서 소스 구하기는 쉬움.\n",
    "\n",
    "더 중요한 것은 데이터를 어떻게 확보할 것인가.\n",
    "\n",
    "신문기사 요약 출력? 라벨링이 없으면 전문적인 지식이 좀 필요함.\n",
    "제대로된 성능을 내려면 라벨링 된 데이터가 필요함.\n",
    "\n",
    "Fashion - MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Sentence classification\n",
    "\n",
    "데이터를 가져온 이유. \n",
    "원시적인 데이터가 많이 담겨있음\n",
    "\n",
    "오전은 fashion data는 함수 한줄로 다된다.\n",
    "실제로 프로젝트하면 data가 개떡같이 주어진다.\n",
    "\n",
    "댓글 입력텍스트, 별점이 긍정 부정 기준\n",
    "별점을 그대로 라벨로 사용하면 문제가 생기게 된다.\n",
    "\n",
    "데이터 수집.\n",
    "문장을 준다 -> 얼마나 공손한거 같아? 불손한거 같아?\n",
    "-> 돈을 준다.\n",
    "\n",
    "wiki / stack exchange \n",
    "사이트의 특성에 상관없이 문맥이 공손함을 잘 파악하는지 하기위해\n",
    "교환해서 했음.\n",
    "\n",
    "cross domain 성능이 낮음. 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OoV : 위키피디아 단어 몇만개\n",
    "고빈도 단어만 추려서 인덱스를 붙인다.\n",
    "6천개 단어중 5천개 만 사용할 것임\n",
    "나머지 천 몇개는 Out Of Vocabulary : 하나의 단어로 취급해서 크기를 줄인다.\n",
    "\n",
    "Padding token 문장을 동일한 길이로 붙인다.\n",
    "특수토큰으로 붙여서 할당을 할 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN 문장분류 모델(by 한국인)\n",
    "\n",
    "이미지 말고 텍스트도 분류해 보자\n",
    "단어의 모음은 2차원 행렬이 된다.\n",
    "-> 이미지 입력과 동일한 구조가 된다.\n",
    "\n",
    "이미지와의 차이 : 이미지는 동일한 크기를 사용했지만\n",
    "문장마다 단어 갯수가 다르므로 세로길이가 바뀌게 된다.(우리는 패딩 사용)\n",
    "단어의 특성을 반영하기 위해 커널의 모양도 다르게 한다.\n",
    "\n",
    "50차원의 단어벡터면 50개 중에 몇개 (3,4개)만 보는 것은 의미가 없다.\n",
    "그래서 커널의 폭을 단어벡터의 크기와 동일하게 맞춘다.\n",
    "\n",
    "커널이 한단어는 무조건 통채로 볼 수 있게 한다.\n",
    "커널의 세로가 한번에 몇개의 단어를 보는지가 정해진다.\n",
    "그럼으로써 feature map을 만든다.\n",
    "그리고 cnn 이용 text모델에서는 따로 padding을 해주지 않는다.\n",
    "\n",
    "feature map이 이미지와는 좀 다르게 된다.(벡터)\n",
    "\n",
    "여기서도 풀링을 한다.\n",
    "이미지 크기가 고정되어서 풀링을 했던 것이고, 여기서는 feature map의 길이가 다달라 지므로\n",
    "global max pooling을 사용한다. 풀링의 범위가 정해져있지 않다.\n",
    "전체 값중에 가장 큰 숫자를 가져온다.-> 커널의 갯수와 동일 하게 된다.\n",
    "이를 카테고리 갯수만큼으로 줄인다.\n",
    "\n",
    "보통은 굉장히 빨라서 사용한다.\n",
    "커널마다 패치할 수 있는 패턴이 있다. 그 커널이 매칭이 되는 부분에서\n",
    "숫자가 크게 나온다. => 가장 일치하는 부분이 많다는 뜻이므로 대표성을 띄게 된다.\n",
    "\n",
    "단어임베딩이 보기에는 신기해도 사용안하면 쓸모가 없다.\n",
    "Glove라는 벡터를 사용할 것임.\n",
    "Word2Vec과 비슷한 것이 Glove.\n",
    "\n",
    "-------------------------------------------\n",
    "각 단어마다 50개의 숫자가 주어진다.\n",
    "Glove의 vocab과 우리의 학습 vocab이 안맞을 수도 있다.\n",
    "\n",
    "각단어마다 glove 벡터가 있다.\n",
    "그래서 우리가 가진 단어와 비교를 해볼 것이다.\n",
    "\n",
    "Glove에 much라는 단어의 벡터가 없으면 무작위로 초기화를 해준다.\n",
    "처음에 모든 단어에 대한 임베딩 벡터를 무작위로 초기화 해준다.\n",
    "\n",
    "Glove에 있는 단어는 덮어씌운다.\n",
    "다시 그걸 가지고 모델을 학습 시킬 것이다.\n",
    "\n",
    "지금은 임베딩 벡터가 다 무작위로 된 상태에서 학습이 됬다면\n",
    "지금은 Glove에 있는 것을 가지고 학습을 시킨다.\n",
    "\n",
    "데이터가 2천문장이면 너무 적은 것이다.\n",
    "1. 데이터를 만들어야 한다. (사람을 시켜야 한다)\n",
    "2. 비지도학습 방법이랑 결합을 한다.\n",
    "Transfer learning 다른 model에서 학습 먼저 시키고,\n",
    "이 학습된 정보를 가지고 새로운 모델을 다시 학습을 시키는 것이다. \n",
    "Glove 60억 토큰, 더 큰 말뭉치 비지도학습 말뭉치도 많다.\n",
    "이런것들도 활용을 한다.\n",
    "학습된 정보를 가져온다.\n",
    "-------------------------------------------\n",
    "\n",
    "처음에 학습을 시켰던 목적과\n",
    "이제 쓰려는 목적이 맞지 않으면 성능이 잘 안나온다.\n",
    "\n",
    "Transfer 러닝을 할때는 얼마나 유사성이 있는지가 중요하다.\n",
    "BERT사용법, 코드 주고 마무리\n",
    "텐서플로우 허브. : 기존에 학습된 모델을 재활용하는 것이 중요한 토픽\n",
    "\n",
    "깃헙에서 소스만 다운받으면 그게 바로 모델이었다.\n",
    "모델을 학습시킨 결과도 이제 필요하다. (parameter도 같이 배포)\n",
    "텐서플로우 허브는 텐서플로우가 유지보수를 하고, 사람들이\n",
    "공개하고 업로드 하는 곳임. 즉 전이학습을 위한 플랫폼.\n",
    "\n",
    "문장을 벡터로 변환하는 것에 특화된 것:\n",
    "Universal-sentence-encoder-large 로딩하면 객체로 제공이된다.\n",
    "텐서플로우는 다 그래프 기반.\n",
    "내부 구조가 다 구현이 되어있따.\n",
    "\n",
    "문장을 입력하면 벡터가 나오고, 거기게 dense layer만 추가하면 된다.\n",
    "딥러닝은 backpropagation하는데\n",
    "SVM 추가하면 그걸 안해서 성능이 잘 안나올 것이다.\n",
    "지도학습 모델\n",
    "\n",
    "BERT는 비지도학습 랭기지 모델 : 다음 단어를 예측 하는 것의 변형\n",
    "BERT자체가 어마어마하게 크고 쓰인 것도 많아서\n",
    "\n",
    "마찬가지로 거의 끝까지 대체 가능\n",
    "객체명 인식도 BERT로 대체할 수 있다.\n",
    "\n",
    "그래서 여러군데에서 BERT를 fine tuning하는 방식이 가능하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
