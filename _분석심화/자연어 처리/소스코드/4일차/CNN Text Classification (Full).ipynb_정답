{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN Text Classification (Full).ipynb_정답","version":"0.3.2","provenance":[{"file_id":"1xQifcL1wERKUWy-741_PrRNxmWjXTrt5","timestamp":1564649545017}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TKtiON7SWo-W","colab_type":"text"},"source":["# CNN Text Classification Lab"]},{"cell_type":"markdown","metadata":{"id":"EEbjAYgOW6WB","colab_type":"text"},"source":["## Goal\n","본 실습의 목표는 Convolutional Neural Network을 이용하여 문장을 여러 카테고리 중 하나로 분류하는 모델을 만드는 것입니다. 또한, 미리 학습된 단어 벡터를 모델에 적용하는 방법도 배워볼 것입니다."]},{"cell_type":"markdown","metadata":{"id":"ROSacgMz6V7r","colab_type":"text"},"source":["## Dataset\n","\n","학습 데이터는 Stanford 대학에서 구성한 공손함 데이터를 사용하겠습니다."]},{"cell_type":"code","metadata":{"id":"N1YfHPu-7t1f","colab_type":"code","colab":{}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","import nltk\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import word_tokenize\n","import tensorflow as tf\n","from tensorflow.python.keras.preprocessing import sequence\n","from tensorflow import keras\n","\n","nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2EW2UKy6elS","colab_type":"code","colab":{}},"source":["if not os.path.exists(\"Stanford_politeness_corpus.zip\"):\n","  !wget http://www.cs.cornell.edu/~cristian/Politeness_files/Stanford_politeness_corpus.zip\n","\n","if not os.path.exists(\"Stanford_politeness_corpus/wikipedia.annotated.csv\"):\n","  !unzip Stanford_politeness_corpus.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLxkMzaN9G02","colab_type":"code","colab":{}},"source":["def load_data(data_file):\n","  data = pd.read_csv(data_file)\n","\n","  # Only use the top quartile as polite, and bottom quartile as impolite. Discard the rest.\n","  quantiles = data[\"Normalized Score\"].quantile([0.25, 0.5, 0.75])\n","  print(quantiles)\n","\n","  for i in range(len(data)):\n","    score = data.loc[i, \"Normalized Score\"]\n","    if score <= quantiles[0.25]:\n","      # Bottom quartile (impolite).\n","      data.loc[i, \"Normalized Score\"] = 0\n","    elif score >= quantiles[0.75]:\n","      # Top quartile (polite).\n","      data.loc[i, \"Normalized Score\"] = 1\n","    else:\n","      # Neutral.\n","      data.loc[i, \"Normalized Score\"] = 2\n","\n","  data[\"Normalized Score\"] = data[\"Normalized Score\"].astype(int)\n","\n","  # Discard neutral examples.\n","  data = data[data[\"Normalized Score\"] < 2]\n","  data = data.sample(frac=1).reset_index(drop=True)\n","\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaI1jGtMWOye","colab_type":"code","colab":{}},"source":["data = load_data(\"Stanford_politeness_corpus/wikipedia.annotated.csv\")\n","pd.set_option('display.max_columns', None)\n","\n","print(data.head())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbrRF2ER6oay","colab_type":"text"},"source":["다음으로 할 일은 사전을 구성하는 것입니다.\n","\n","\n","신경망의 입력으로 사용하기 위해서는 문장을 숫자로 바꿔야 하는데, 사전의 역할은 단어를 숫자로, 숫자를 단어로 바꿔주는 것입니다.\n","\n","여기서 빠른 계산을 위해 dictionary 자료 구조를 사용하는 것이 일반적입니다.\n","\n","\n","\n","1.   문장들을 소문자로 바꾸고, tokenization (nltk.tokenize 패키지의 word_tokenize  함수 활용)\n","2.   전체 데이터에서 각 토큰들의 등장 빈도 확인 (collections 패키지의 Counter  클래스 활용)\n","3.   가장 등장 빈도가 높은 단어를 vocab_size 만큼 선택 (Counter의 most_common 함수 활용)\n","4.   각각의 단어에 고유한 숫자 부여. 이때, 0번째 토큰은 \"<PAD>\", 1번째 토큰은 \"<OOV>\" 할당\n","5.   토큰 -> 숫자 변환을 위한 dictionary (word_index 변수에 할당)와, 숫자 -> 토큰 변환을 위한 dictionary (word_inverted_index 변수에 할당) 생성\n","  \n","  \n","\n"]},{"cell_type":"code","metadata":{"id":"RN4C5ewLW9AN","colab_type":"code","colab":{}},"source":["vocab_size = 5000\n","# we assign the first indices in the vocabulary to special tokens that we use\n","# for padding, and for indicating unknown words\n","pad_id = 0\n","oov_id = 1\n","index_offset = 1\n","\n","def make_vocab(sentences):\n","  word_counter = Counter()\n","\n","  for sent in sentences:\n","    tokens = word_tokenize(sent.lower())\n","    word_counter.update(tokens)\n","  \n","  most_common = word_counter.most_common()\n","  print(\"고빈도 단어:\")\n","  for k, v in most_common[:10]:\n","    print(k, \": \", v)\n","  \n","  vocab = {\n","      '<PAD>': pad_id,\n","      '<OOV>': oov_id\n","  }\n","  for i, (word, cnt) in enumerate(most_common, start=index_offset+1):\n","    vocab[word] = i\n","    if len(vocab) >= vocab_size:\n","      break\n","  \n","  return vocab\n","  \n","sentences = data[\"Request\"].tolist()\n","word_index = make_vocab(sentences)\n","word_inverted_index = {v:k for k, v in word_index.items()}\n","\n","print(\"\\n단어 사전:\")\n","for i in range(0, 10):\n","  print(i, word_inverted_index[i])\n","  \n","print(\"\\n단어 사전 크기: \", len(word_index))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vh3O9aHD7HLf","colab_type":"text"},"source":["사전이 잘 구성되었는지 시험해보겠습니다. \n","\n","사전이 잘 구성되고, 각각의 사전이 word_index 변수와 word_inverted_index 변수에 할당되었다면 문장이 숫자로 변환되었다가 다시 원래 문장으로 돌아오는 것을 확인하실 수 있습니다."]},{"cell_type":"code","metadata":{"id":"WdBgH74TcmNk","colab_type":"code","colab":{}},"source":["def index_to_text(indexes):\n","  return ' '.join([word_inverted_index[i] for i in indexes])\n","  \n","def text_to_index(text):\n","  tokens = tokens = word_tokenize(text.lower())\n","  indexes = []\n","  for tok in tokens:\n","    if tok in word_index:\n","      indexes.append(word_index[tok])\n","    else:\n","      indexes.append(oov_id)\n","      \n","  return indexes\n","\n","print(\"원본: \", sentences[0])\n","ids = text_to_index(sentences[0])\n","print(\"문자 -> 숫자: \", ids)\n","print(\"숫자 -> 문자: \", index_to_text(ids))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eb0Zk0YY7MiD","colab_type":"text"},"source":["다음으로, 숫자로 바뀐 문장들을 학습 데이터로 사용할 수 있도록 변형하겠습니다.\n","\n","\n","\n","1.   모든 문장들을 동일한 길이가 되도록 padding 처리하거나 자름 (tensorflow.python.keras.preprocessing.sequence 패키지의 pad_sequence 함수 활용)\n","2.   데이터의 일부(10%)를 테스트 데이터로 분리\n","\n"]},{"cell_type":"code","metadata":{"id":"MQfHyY0GhvBJ","colab_type":"code","colab":{}},"source":["x_variable = [text_to_index(sent) for sent in sentences]\n","\n","sentence_size = 200\n","x_padded = sequence.pad_sequences(x_variable,\n","                                 maxlen=sentence_size,\n","                                 truncating='post',\n","                                 padding='post',\n","                                 value=pad_id)\n","\n","n_test = len(data) // 10\n","test_inputs = x_padded[:n_test]\n","train_inputs = x_padded[n_test:]\n","\n","ys = np.array(data[\"Normalized Score\"].tolist())\n","test_labels = ys[:n_test]\n","train_labels = ys[n_test:]\n","\n","print(\"test_inputs shape: \", test_inputs.shape)\n","print(\"train_inputs shape: \", train_inputs.shape)\n","print(\"test_labels shape: \", test_labels.shape)\n","print(\"train_labels shape: \", train_labels.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ftSUTOB98N_","colab_type":"text"},"source":["이제 모델을 설계할 차례입니다. \n","\n","keras.Sequential을 이용하여 CNN 모델을 구성해봅시다. Sequential 모델을 사용하려면 동일한 크기의 필터만 사용할 수 있습니다.\n","\n","참고 함수: \n","\n","keras.layers.Embedding\n","\n","https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n","\n","keras.layers.Conv1D\n","\n","https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n","\n","keras.layers.GlobalMaxPool1D\n","\n","https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D\n","\n","keras.layers.Dense\n","\n","https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n","\n"]},{"cell_type":"code","metadata":{"id":"dlsrBpQZmUFW","colab_type":"code","colab":{}},"source":["model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, 50),\n","    keras.layers.Conv1D(32, 3, padding=\"same\", activation=tf.nn.relu),\n","    keras.layers.GlobalMaxPool1D(),\n","    keras.layers.Dense(2, activation=tf.nn.softmax)\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI6aCoD9-jNE","colab_type":"text"},"source":["아래는 학습 결과를 시각화해주고, 성능을 측정하는 함수들입니다."]},{"cell_type":"code","metadata":{"id":"hZG1KkAOo_ZY","colab_type":"code","colab":{}},"source":["def plot_loss(history):\n","  plt.figure(figsize=(6,5))\n","  val = plt.plot(history.epoch, history.history['val_loss'],\n","                 '--', label='Test')\n","  plt.plot(history.epoch, history.history['loss'], color=val[0].get_color(),\n","           label='Train')\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(\"Loss\")\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])\n","  \n","def eval_model(model):\n","  test_loss, test_acc = model.evaluate(test_inputs, test_labels)\n","  print('Test accuracy:', test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVGrEUEu-oXv","colab_type":"text"},"source":["만들어진 모델을 학습시켜보겠습니다."]},{"cell_type":"code","metadata":{"id":"-hPsLLFIot0y","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', \n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_inputs,\n","          train_labels,\n","          epochs=10,\n","          validation_data=(test_inputs, test_labels)\n","         )\n","\n","plot_loss(history)\n","eval_model(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCaixZUv-tjb","colab_type":"text"},"source":["## Pretrained word vectors\n","\n","이번에는 만들어진 모델에 미리 학습된 단어 벡터를 적용해보겠습니다.\n","\n","단어 벡터는 GloVe 벡터를 사용할 것입니다.\n","\n","벡터 파일을 다운로드 받고 압축을 풀어보겠습니다.\n","\n","파일이 어떻게 구성되어 있는지 볼까요?"]},{"cell_type":"code","metadata":{"id":"UM6dSiL7rXut","colab_type":"code","outputId":"e23c6724-3e73-4e8a-e99b-e012356a360e","executionInfo":{"status":"ok","timestamp":1564589131125,"user_tz":-540,"elapsed":3612,"user":{"displayName":"이찬희","photoUrl":"","userId":"13767334494174834516"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["if not os.path.exists('glove.6B.zip'):\n","    ! wget http://nlp.stanford.edu/data/glove.6B.zip\n","if not os.path.exists('glove.6B.50d.txt'):\n","    ! unzip glove.6B.zip\n","    \n","! head glove.6B.50d.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n","of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n","to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n","and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n","in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n","a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n","\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n","'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xhv_Sbm4--m9","colab_type":"text"},"source":["GloVe 벡터를 불러와서 임베딩 행렬을 초기화해보겠습니다.\n","\n","\n","\n","1.   GloVe 파일을 읽고, 각 줄에서 단어(1번째 토큰)와 벡터를 이루는 숫자들(2번째 이후 토큰들)을 분리\n","2.   벡터를 이루는 숫자들을 numpy 행렬로 변환 (numpy의 asarray 함수 활용)\n","3.   단어와 벡터를 연결하는 dictionary 자료구조 구성 (단어 -> 벡터)\n","4.   모든 단어들에 대한 임베딩 행렬을 무작위로 생성 (vocab_size X 50 크기의 numpy 행렬)\n","5.   임베딩 행렬에서, GloVe 벡터가 존재하는 단어들만 해당 GloVe 벡터로 대체\n","\n"]},{"cell_type":"code","metadata":{"id":"4OWLKUOFr0tB","colab_type":"code","colab":{}},"source":["def load_glove_embeddings(path):\n","    embeddings = {}\n","    with open(path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.strip().split()\n","            w = values[0]\n","            vectors = np.asarray(values[1:], dtype='float32')\n","            embeddings[w] = vectors\n","\n","    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, 50))\n","    num_loaded = 0\n","    for w, i in word_index.items():\n","        v = embeddings.get(w)\n","        if v is not None and i < vocab_size:\n","            embedding_matrix[i] = v\n","            num_loaded += 1\n","    print('Successfully loaded pretrained embeddings for '\n","          f'{num_loaded}/{vocab_size} words.')\n","    embedding_matrix = embedding_matrix.astype(np.float32)\n","    return embedding_matrix\n","\n","embedding_matrix = load_glove_embeddings('glove.6B.50d.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c70N63qoAKx7","colab_type":"text"},"source":["앞서 사용된 모델에서, Embedding layer의 값을 위에서 생성한 임베딩 행렬로 초기화해봅시다.\n","\n","(keras.initializers.Constant 클래스 활용)\n"]},{"cell_type":"code","metadata":{"id":"cjMv1u72r_qv","colab_type":"code","colab":{}},"source":["glove_init = keras.initializers.Constant(embedding_matrix)\n","\n","model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, 50, embeddings_initializer=glove_init),\n","    keras.layers.Conv1D(32, 3, padding=\"same\", activation=tf.nn.relu),\n","    keras.layers.GlobalMaxPool1D(),\n","    keras.layers.Dense(2, activation=tf.nn.softmax)\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGnd9iJqsk3U","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', \n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_inputs,\n","          train_labels,\n","          epochs=10,\n","          validation_data=(test_inputs, test_labels)\n","         )\n","\n","plot_loss(history)\n","eval_model(model)"],"execution_count":0,"outputs":[]}]}