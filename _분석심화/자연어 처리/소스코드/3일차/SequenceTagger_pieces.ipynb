{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pieces.ipynb의 사본","version":"0.3.2","provenance":[{"file_id":"1ggCMDlT5sD7Bw6WixX8s7dlDvt7vhQW3","timestamp":1564551189017}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vN9ARCXYLWVK","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","\n","with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n","    vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n","if len(vocab) > hparams.vocab_size:\n","    vocab = vocab[:hparams.vocab_size]\n","\n","id2word = vocab\n","word2id = {}\n","for i, word in enumerate(vocab):\n","    word2id[word] = i\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeHrIicTLWb1","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n","  labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n","  labels.insert(0, \"PAD\")\n","  id2label = labels\n","  label2id = {}\n","  for i, label in enumerate(labels):\n","      label2id[label] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1YoAfI_LnRD","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","with tf.variable_scope(\"bi-RNN\"):\n","    # Build RNN layers\n","    rnn_cell_forward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n","    rnn_cell_backward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n","\n","    # Apply dropout to RNN\n","    if hparams.dropout_keep_prob < 1.0:\n","        rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward, output_keep_prob=dropout_keep_prob_ph)\n","        rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward, output_keep_prob=dropout_keep_prob_ph)\n","\n","    # Stack multiple layers of RNN\n","    # rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * hparams.rnn_depth)\n","    # rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * hparams.rnn_depth)\n","\n","    (output_forward, output_backward), _ = tf.nn.bidirectional_dynamic_rnn(\n","        rnn_cell_forward, rnn_cell_backward,\n","        inputs=layer_out,\n","        sequence_length=lengths,\n","        dtype=tf.float32\n","    )\n","    hiddens = tf.concat([output_forward, output_backward], axis=-1)\n","    # shape = [batch_size, time, rnn_dim*2]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-iJHH0hL4mc","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","sentence = word_tokenize(sentence)\n","word_ids = []\n","(id2word, word2id), (id2label, label2id) = make_vocab_table()\n","\n","for word in sentence:\n","    if word in word2id:\n","        word_ids.append(word2id[word])\n","    else:\n","        word_ids.append(len(word2id))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNv1u2WvLnTi","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","with tf.variable_scope(\"read-out\"):\n","  prev_layer_size = layer_out.get_shape().as_list()[1]\n","  weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim],\n","                           initializer=tf.initializers.variance_scaling(\n","                               scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n","                           ))\n","  bias = tf.get_variable(\"bias\", shape=[output_dim],\n","                         initializer=tf.initializers.zeros())\n","  predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n","\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-7jDdFtLnWO","colab_type":"code","colab":{}},"source":["## ok ##\n","loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels,\n","                                                         name=\"cross_entropy\")\n","loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n","train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n","\n","eval = tf.nn.in_top_k(logits, labels, 1)\n","correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n","accuracy = tf.divide(correct_count, tf.shape(labels)[0])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1tDlpzoMN6C","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","\n","dense_word_ids = tf.constant(word_ids)\n","lengths = tf.constant(len(word_ids))\n","# Insert batch dimension.\n","dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n","lengths = tf.expand_dims(lengths, axis=0)\n","\n","with tf.variable_scope(\"build_graph\", reuse=tf.AUTO_REUSE):\n","    logits = build_graph(dense_word_ids, lengths, id2word, id2label)\n","predictions = tf.argmax(logits, axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIxFtxYvMN9E","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","word2id = tf.contrib.lookup.index_table_from_tensor(\n","  mapping=tf.constant(id2word),\n","  num_oov_buckets=1,\n","  name=\"word2id\"\n",")\n","\n","label2id = tf.contrib.lookup.index_table_from_tensor(\n","  mapping=tf.constant(id2label),\n","  default_value=label2id[\"O\"],\n","  name=\"label2id\"\n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-XO06VMMOGG","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","label_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.labels\"))\n","batched_label_dataset = label_dataset.batch(hparams.batch_size)\n","label_iterator = batched_label_dataset.make_initializable_iterator()\n","batch_label_str = label_iterator.get_next()\n","batch_label = tf.string_split(batch_label_str, \" \")\n","label_ids = label2id.lookup(batch_label)\n","dense_label_ids = tf.sparse_tensor_to_dense(label_ids)\n","# shape = [batch_size, time]\n","\n","mask = tf.sequence_mask(lengths)\n","dense_label_ids = tf.boolean_mask(dense_label_ids, mask)\n","\n","iterator_initializers.append(input_iterator.initializer)\n","iterator_initializers.append(label_iterator.initializer)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjt27ZtRMOJe","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","logger.info(\"End of epoch %d.\" % (epochs_completed+1))\n","save_path = saver.save(sess, \"saves/model.ckpt\", global_step=global_step_val)\n","logger.info(\"Model saved at: %s\" % save_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCz8hF3zMOMF","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","input_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.inputs\"))\n","batched_input_dataset = input_dataset.batch(hparams.batch_size)\n","input_iterator = batched_input_dataset.make_initializable_iterator()\n","batch_input = input_iterator.get_next()\n","batch_input.set_shape([hparams.batch_size])\n","words = tf.string_split(batch_input, \" \")\n","word_ids = word2id.lookup(words)\n","dense_word_ids = tf.sparse_tensor_to_dense(word_ids)\n","# shape = [batch_size, time]\n","\n","\n","line_number = word_ids.indices[:, 0]\n","line_position = word_ids.indices[:, 1]\n","lengths = tf.segment_max(data=line_position,\n","                         segment_ids=line_number) + 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbjynpHrMOOt","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","try:\n","  accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n","      [accuracy, labels, loss_op, global_step, train_op],\n","      feed_dict={dropout_keep_prob_ph: hparams.dropout_keep_prob}\n","  )\n","  accuracy_mean += accuracy_val\n","  loss_mean += loss_val\n","  idx_cnt += 1\n","  if global_step_val % 50 == 0:\n","      accuracy_mean /= idx_cnt\n","      loss_mean /= idx_cnt\n","      logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (global_step_val, loss_mean, accuracy_mean * 100))\n","      accuracy_mean, loss_mean,idx_cnt = 0, 0, 0\n","except tf.errors.OutOfRangeError:\n","  # End of epoch.\n","  break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"63vwMSydMs0I","colab_type":"code","colab":{}},"source":["## ok ##\n","\n","saver = tf.train.Saver()\n","saver.restore(sess, saved_file)\n","pred_val = sess.run(\n","    [predictions]\n",")[0]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNpMx60cLnY2","colab_type":"code","colab":{}},"source":["\n","layer_out = tf.layers.dense(\n","    inputs=layer_out,\n","    units=hparams.rnn_hidden_dim,\n","    activation=tf.nn.relu,\n","    kernel_initializer=tf.initializers.variance_scaling(\n","        scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n","    name=\"input_projection\"\n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NLXJ2QLLnbb","colab_type":"code","colab":{}},"source":["## ok ## \n","\n","pred_str = [id2label[i] for i in pred_val]\n","for word, tag in zip(sentence, pred_str):\n","    print(\"%s[%s]\" %(word, tag), end=' ')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wijCTlnLndy","colab_type":"code","colab":{}},"source":["## ok ## \n","  \n","mask = tf.sequence_mask(lengths)\n","bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, hparams.rnn_hidden_dim * 2])\n","layer_out = bi_lstm_out  # shape=[sum of seq length, 2*LSTM hidden layer size]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsTnHPguLngg","colab_type":"code","colab":{}},"source":["## ok ## \n","  \n","  \n","# Number of possible output categories.\n","output_dim = len(id2label)\n","vocab_size = len(id2word) + 1\n","embeddings = tf.get_variable(\n","    \"embeddings\",\n","    shape=[vocab_size, hparams.embedding_dim],\n","    initializer=tf.initializers.variance_scaling(\n","        scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n",")\n","embedded = tf.nn.embedding_lookup(embeddings, inputs)\n","# shape = [batch_size, time, embed_dim]\n","layer_out = embedded\n"],"execution_count":0,"outputs":[]}]}